<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.mapper.reference.html">org.xbib.elasticsearch.index.mapper.reference</a> &gt; ReferenceMappingTests</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">4</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">37.832s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testRefFromID</td>
<td>9.150s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefInDoc</td>
<td>9.182s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefMappings</td>
<td>9.180s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSearch</td>
<td>10.320s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[09:25:25,972][INFO ][test                     ][Test worker] settings cluster name
[09:25:25,972][INFO ][test                     ][Test worker] starting nodes
[09:25:25,972][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[09:25:25,972][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[09:25:25,975][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[09:25:25,975][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [211gb], usable_space [210.8gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[09:25:25,975][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[09:25:25,976][INFO ][org.elasticsearch.node.Node][Test worker] node name [dUhhjH_] derived from node ID [dUhhjH_UT9yE4-qO6_7BwA]; set [node.name] to override
[09:25:25,976][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[53621], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_92/25.92-b15]
[09:25:25,976][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[09:25:25,976][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[09:25:25,976][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[09:25:25,976][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[09:25:25,977][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[09:25:25,978][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[09:25:25,978][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[09:25:25,978][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[09:25:25,980][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en4
        inet 10.1.1.42 netmask:255.255.0.0 broadcast:10.1.255.255 scope:site
        inet6 fe80::6a5b:35ff:febc:4672 prefixlen:64 scope:link
        hardware 68:5B:35:BC:46:72
        UP MULTICAST mtu:1500 index:10

[09:25:25,980][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[09:25:25,980][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[09:25:25,981][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[09:25:25,981][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[09:25:25,981][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[09:25:25,981][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[09:25:25,981][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[09:25:25,981][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[09:25:25,982][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[09:25:25,982][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[09:25:25,982][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[09:25:25,983][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[09:25:25,983][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[09:25:25,983][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[09:25:25,983][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[09:25:25,983][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:25,983][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:26,009][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[09:25:26,014][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[09:25:26,564][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[09:25:26,567][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[09:25:26,568][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[09:25:26,568][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[09:25:26,568][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[11]}, bound_addresses {local[11]}
[09:25:26,569][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[09:25:26,569][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [initial_join]: execute
[09:25:26,569][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[09:25:29,573][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[dUhhjH_][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]}], id[77], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[09:25:29,573][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[dUhhjH_][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[09:25:29,574][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[09:25:29,574][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[09:25:29,574][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] new_master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[09:25:29,574][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [1]
[09:25:29,575][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 1
[09:25:29,575][INFO ][org.elasticsearch.node.Node][Test worker] started
[09:25:29,575][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: 2a9hsCZ4RfKOkmiizlHFcw)
[09:25:29,576][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[09:25:29,576][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[09:25:29,576][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [2]
[09:25:29,577][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 2
[09:25:29,578][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[09:25:29,578][INFO ][test                     ][Test worker] nodes are started
[09:25:29,578][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: kvtSLTALSlKhmkjSgtYmew)
[09:25:29,579][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[09:25:29,580][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[09:25:29,580][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating Index [[test/P12EvAW6Ro-CjacxGBK-KQ]], shards [5]/[1] - reason [create index]
[09:25:29,581][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:30,092][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:30,092][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:30,094][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[09:25:30,094][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test/P12EvAW6Ro-CjacxGBK-KQ] closing index service (reason [cleaning up after validating index on master])
[09:25:30,094][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:30,094][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:30,094][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:30,094][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test/P12EvAW6Ro-CjacxGBK-KQ] closed... (reason [cleaning up after validating index on master])
[09:25:30,094][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[09:25:30,094][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [3]
[09:25:30,094][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 3
[09:25:30,094][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [[test/P12EvAW6Ro-CjacxGBK-KQ]] creating index
[09:25:30,094][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating Index [[test/P12EvAW6Ro-CjacxGBK-KQ]], shards [5]/[1] - reason [create index]
[09:25:30,095][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:30,570][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:30,570][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][2] creating shard
[09:25:30,571][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/P12EvAW6Ro-CjacxGBK-KQ/2, shard=[test][2]}]
[09:25:30,571][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [test][2]
[09:25:30,571][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:30,571][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:30,572][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:30,572][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][1] creating shard
[09:25:30,572][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] starting recovery from store ...
[09:25:30,572][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/P12EvAW6Ro-CjacxGBK-KQ/1, shard=[test][1]}]
[09:25:30,573][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [test][1]
[09:25:30,573][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:30,573][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:30,574][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#3]] wipe translog location - creating new translog
[09:25:30,574][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:30,574][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][3] creating shard
[09:25:30,574][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] starting recovery from store ...
[09:25:30,574][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/P12EvAW6Ro-CjacxGBK-KQ/3, shard=[test][3]}]
[09:25:30,574][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [test][3]
[09:25:30,575][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:30,575][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:30,575][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:30,575][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#1]] wipe translog location - creating new translog
[09:25:30,575][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:30,575][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][0] creating shard
[09:25:30,575][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] starting recovery from store ...
[09:25:30,576][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:30,576][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/P12EvAW6Ro-CjacxGBK-KQ/0, shard=[test][0]}]
[09:25:30,576][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [test][0]
[09:25:30,576][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:30,576][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:30,577][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#2]] wipe translog location - creating new translog
[09:25:30,577][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:30,577][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:30,577][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] recovery completed from [shard_store], took [7ms]
[09:25:30,578][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:30,577][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] starting recovery from store ...
[09:25:30,578][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [test][2] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery]]
[09:25:30,578][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:30,578][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [test][2] received shard started for [shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery]]
[09:25:30,578][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] recovery completed from [shard_store], took [5ms]
[09:25:30,578][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [test][1] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]]
[09:25:30,578][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [test][1] received shard started for [shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]]
[09:25:30,579][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [998ms] done applying updated cluster_state (version: 3, uuid: pkMWBxiESv-6GatLLglawQ)
[09:25:30,579][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]]]: execute
[09:25:30,579][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#4]] wipe translog location - creating new translog
[09:25:30,579][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=SCCY1eRbRW-jqVgQvJz7Pg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:30.092Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery]])
[09:25:30,579][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=qVL2hykWR8ug0A_s29_1hA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:30.092Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]])
[09:25:30,580][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:30,580][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:30,580][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] recovery completed from [shard_store], took [6ms]
[09:25:30,580][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery]]
[09:25:30,580][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery]]
[09:25:30,580][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]]]
[09:25:30,581][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [4]
[09:25:30,581][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 4
[09:25:30,582][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:30,582][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:30,582][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] recovery completed from [shard_store], took [6ms]
[09:25:30,582][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery]]
[09:25:30,582][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:30,583][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery]]
[09:25:30,583][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,583][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,583][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][4] creating shard
[09:25:30,583][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/P12EvAW6Ro-CjacxGBK-KQ/4, shard=[test][4]}]
[09:25:30,583][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [test][4]
[09:25:30,583][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:30,584][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:30,584][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:30,584][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] starting recovery from store ...
[09:25:30,584][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,584][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,585][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#3]] wipe translog location - creating new translog
[09:25:30,585][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [SCCY1eRbRW-jqVgQvJz7Pg], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [qVL2hykWR8ug0A_s29_1hA], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 4, uuid: R13UlsoUTH-JkRikwtm6Bw)
[09:25:30,586][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:30,586][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=_LOZCPHvQQOgE_EbmTOW-A], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:30.092Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery]])
[09:25:30,586][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=__Nmks_RTQm4PCuoXeK6rw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:30.092Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery]])
[09:25:30,586][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:30,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:30,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [5]
[09:25:30,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 5
[09:25:30,588][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:30,589][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:30,589][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] recovery completed from [shard_store], took [5ms]
[09:25:30,589][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery]]
[09:25:30,589][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,589][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery]]
[09:25:30,589][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:30,589][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:30,590][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [_LOZCPHvQQOgE_EbmTOW-A], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [__Nmks_RTQm4PCuoXeK6rw], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: ncLlUJ24RqqxEczVVFoPiw)
[09:25:30,591][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:30,591][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=5ZnecQj2RDC3UmcSX2g8RA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:30.092Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery]])
[09:25:30,591][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:30,592][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [6]
[09:25:30,592][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 6
[09:25:30,592][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:30,593][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [5ZnecQj2RDC3UmcSX2g8RA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: F5aZRN42TI2oqlBdlvzFrQ)
[09:25:30,595][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[09:25:31,085][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:31,087][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [test/P12EvAW6Ro-CjacxGBK-KQ] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:31,088][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[09:25:31,088][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [7]
[09:25:31,088][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 7
[09:25:31,088][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [[test/P12EvAW6Ro-CjacxGBK-KQ]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:31,091][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [495ms] done applying updated cluster_state (version: 7, uuid: YL67jGqUS4SexTxq-b-_Jg)
[09:25:31,113][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[09:25:31,113][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[09:25:31,114][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating Index [[authorities/ufbzum8XRkmvsFjwij9ipg]], shards [5]/[1] - reason [create index]
[09:25:31,114][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:31,591][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:31,592][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:31,593][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[09:25:31,593][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities/ufbzum8XRkmvsFjwij9ipg] closing index service (reason [cleaning up after validating index on master])
[09:25:31,593][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:31,593][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:31,593][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:31,593][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities/ufbzum8XRkmvsFjwij9ipg] closed... (reason [cleaning up after validating index on master])
[09:25:31,593][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[09:25:31,593][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [8]
[09:25:31,594][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 8
[09:25:31,594][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [[authorities/ufbzum8XRkmvsFjwij9ipg]] creating index
[09:25:31,594][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating Index [[authorities/ufbzum8XRkmvsFjwij9ipg]], shards [5]/[1] - reason [create index]
[09:25:31,594][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:32,074][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:32,074][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][2] creating shard
[09:25:32,074][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/ufbzum8XRkmvsFjwij9ipg/2, shard=[authorities][2]}]
[09:25:32,075][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[09:25:32,075][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:32,075][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:32,077][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:32,077][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][1] creating shard
[09:25:32,077][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] starting recovery from store ...
[09:25:32,077][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/ufbzum8XRkmvsFjwij9ipg/1, shard=[authorities][1]}]
[09:25:32,077][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[09:25:32,077][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:32,078][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:32,078][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#1]] wipe translog location - creating new translog
[09:25:32,078][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:32,078][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][3] creating shard
[09:25:32,078][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] starting recovery from store ...
[09:25:32,079][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/ufbzum8XRkmvsFjwij9ipg/3, shard=[authorities][3]}]
[09:25:32,079][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[09:25:32,079][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:32,079][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:32,079][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:32,080][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#2]] wipe translog location - creating new translog
[09:25:32,080][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:32,080][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][0] creating shard
[09:25:32,080][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] starting recovery from store ...
[09:25:32,080][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:32,080][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/ufbzum8XRkmvsFjwij9ipg/0, shard=[authorities][0]}]
[09:25:32,080][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[09:25:32,081][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:32,081][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] recovery completed from [shard_store], took [6ms]
[09:25:32,081][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery]]
[09:25:32,081][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery]]
[09:25:32,081][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:32,081][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#4]] wipe translog location - creating new translog
[09:25:32,081][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:32,082][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:32,082][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:32,082][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] starting recovery from store ...
[09:25:32,083][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:32,084][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#2]] recovery completed from [shard_store], took [6ms]
[09:25:32,084][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#2]] [authorities][1] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]]
[09:25:32,084][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#3]] wipe translog location - creating new translog
[09:25:32,084][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#2]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]]
[09:25:32,084][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [970ms] done applying updated cluster_state (version: 8, uuid: guXLSDjzR_2dMtpi-vb41w)
[09:25:32,084][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]]]: execute
[09:25:32,084][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=nz4abqqySkSNeh1I3r0xUQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:31.592Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery]])
[09:25:32,084][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=m2Fo12ocSxSiTFSr6UnsjA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:31.592Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]])
[09:25:32,084][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:32,085][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:32,085][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#4]] recovery completed from [shard_store], took [6ms]
[09:25:32,085][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery]]
[09:25:32,085][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery]]
[09:25:32,087][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]]]
[09:25:32,087][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [9]
[09:25:32,087][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:32,087][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:32,087][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 9
[09:25:32,087][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery]]
[09:25:32,087][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery]]
[09:25:32,088][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:32,088][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:32,088][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,088][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,088][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][4] creating shard
[09:25:32,089][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/ufbzum8XRkmvsFjwij9ipg/4, shard=[authorities][4]}]
[09:25:32,089][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[09:25:32,089][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:32,089][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:32,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:32,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] starting recovery from store ...
[09:25:32,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,091][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[dUhhjH_][generic][T#1]] wipe translog location - creating new translog
[09:25:32,092][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [nz4abqqySkSNeh1I3r0xUQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [m2Fo12ocSxSiTFSr6UnsjA], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 9, uuid: ABgxfMZ0T4S7ct6gWCknlA)
[09:25:32,092][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:32,092][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=CZtKounJRXCwXyjeaTGreA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:31.592Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery]])
[09:25:32,092][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=NQbQFxi9QryOGPbwKBzLlg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:31.592Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery]])
[09:25:32,092][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[dUhhjH_][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:32,093][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:32,093][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [10]
[09:25:32,093][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 10
[09:25:32,094][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:32,094][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:32,094][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][generic][T#1]] recovery completed from [shard_store], took [5ms]
[09:25:32,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [dUhhjH_UT9yE4-qO6_7BwA] for shard entry [shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [after new shard recovery]]
[09:25:32,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [after new shard recovery]]
[09:25:32,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:32,094][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:32,095][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [CZtKounJRXCwXyjeaTGreA], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [NQbQFxi9QryOGPbwKBzLlg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 10, uuid: AsQaA4EvS4uWC8-jEsdC0Q)
[09:25:32,096][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [after new shard recovery]]]: execute
[09:25:32,096][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[dUhhjH_UT9yE4-qO6_7BwA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=zlEFRnTeR5OuUQtfYsvFbg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:31.592Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[09:25:32,097][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [after new shard recovery]]]
[09:25:32,097][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [11]
[09:25:32,097][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 11
[09:25:32,098][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:32,099][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [master {dUhhjH_}{dUhhjH_UT9yE4-qO6_7BwA}{F959FXXAQV-Jf0Dg6TvNug}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [zlEFRnTeR5OuUQtfYsvFbg], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: QZ5ySbomRAiYMseH0RL4Aw)
[09:25:32,100][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[09:25:32,584][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:32,587][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [authorities/ufbzum8XRkmvsFjwij9ipg] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:32,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[09:25:32,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] publishing cluster state version [12]
[09:25:32,587][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] set local cluster state to version 12
[09:25:32,587][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] [[authorities/ufbzum8XRkmvsFjwij9ipg]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:32,590][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[dUhhjH_][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [489ms] done applying updated cluster_state (version: 12, uuid: tQE4CcibT7qhUsWgPWS0Og)
[09:25:33,113][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[09:25:33,115][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _source = null
[09:25:33,115][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = someType
[09:25:33,115][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = null
[09:25:33,115][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _uid = someType#1
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _version = -1
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings someField = 1234
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = a
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = b
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = c
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _all = 1234
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _source
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _uid
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _version
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = someField
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[09:25:33,116][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _all
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _source = null
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = someType
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = null
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _uid = someType#1
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _version = -1
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings someField = 1234
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = a
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = b
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = c
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _all = 1234
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _source
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _uid
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _version
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = someField
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[09:25:33,118][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _all
[09:25:33,118][INFO ][test                     ][Test worker] stopping nodes
[09:25:33,118][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[09:25:33,119][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities] closing ... (reason [shutdown])
[09:25:33,119][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/ufbzum8XRkmvsFjwij9ipg] closing index service (reason [shutdown])
[09:25:33,119][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[09:25:33,119][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test] closing ... (reason [shutdown])
[09:25:33,120][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,120][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/P12EvAW6Ro-CjacxGBK-KQ] closing index service (reason [shutdown])
[09:25:33,120][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,120][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:33,120][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:33,120][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[09:25:33,120][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:33,121][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,121][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,121][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:33,121][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:33,121][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:33,122][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:33,122][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:33,122][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[09:25:33,122][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[09:25:33,122][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:33,122][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,123][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:33,123][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[09:25:33,123][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[09:25:33,123][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:33,123][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:33,123][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:33,124][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:33,124][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:33,124][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:33,124][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[09:25:33,124][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:33,125][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,125][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,125][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[09:25:33,125][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:33,125][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:33,125][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:33,125][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:33,126][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:33,126][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:33,126][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:33,126][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[09:25:33,126][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:33,126][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[09:25:33,126][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:33,126][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[09:25:33,126][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,126][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[09:25:33,127][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,127][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,127][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,127][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:33,127][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:33,127][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:33,128][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:33,128][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:33,128][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[09:25:33,128][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[09:25:33,128][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,128][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,135][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:33,135][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:33,136][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:33,136][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:33,136][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:33,136][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:33,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:33,137][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:33,137][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[09:25:33,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:33,137][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:33,137][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:33,137][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[09:25:33,137][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[09:25:33,137][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[09:25:33,137][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:33,137][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:33,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:33,138][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/P12EvAW6Ro-CjacxGBK-KQ] closed... (reason [shutdown])
[09:25:33,138][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:33,138][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:33,138][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:33,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:33,139][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:33,139][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[09:25:33,139][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:33,139][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[09:25:33,139][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:33,139][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/ufbzum8XRkmvsFjwij9ipg] closed... (reason [shutdown])
[09:25:33,139][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[09:25:33,139][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[09:25:33,140][INFO ][org.elasticsearch.node.Node][Test worker] closed
[09:25:33,150][INFO ][test                     ][Test worker] data files wiped
[09:25:35,151][INFO ][test                     ][Test worker] settings cluster name
[09:25:35,151][INFO ][test                     ][Test worker] starting nodes
[09:25:35,151][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[09:25:35,152][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[09:25:35,155][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[09:25:35,155][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [211gb], usable_space [210.8gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[09:25:35,155][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[09:25:35,156][INFO ][org.elasticsearch.node.Node][Test worker] node name [UQD86Pq] derived from node ID [UQD86PqWR5iTBEvHzJHumw]; set [node.name] to override
[09:25:35,156][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[53621], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_92/25.92-b15]
[09:25:35,156][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[09:25:35,156][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[09:25:35,156][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[09:25:35,156][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[09:25:35,157][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[09:25:35,158][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[09:25:35,160][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en4
        inet 10.1.1.42 netmask:255.255.0.0 broadcast:10.1.255.255 scope:site
        inet6 fe80::6a5b:35ff:febc:4672 prefixlen:64 scope:link
        hardware 68:5B:35:BC:46:72
        UP MULTICAST mtu:1500 index:10

[09:25:35,160][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[09:25:35,160][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[09:25:35,160][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[09:25:35,160][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[09:25:35,160][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[09:25:35,160][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[09:25:35,160][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[09:25:35,161][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[09:25:35,162][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[09:25:35,162][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[09:25:35,162][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[09:25:35,162][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[09:25:35,162][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[09:25:35,163][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[09:25:35,163][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[09:25:35,163][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:35,163][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:35,188][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[09:25:35,193][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[09:25:35,746][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[09:25:35,749][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[09:25:35,750][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[09:25:35,750][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[09:25:35,750][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[12]}, bound_addresses {local[12]}
[09:25:35,751][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[09:25:35,751][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [initial_join]: execute
[09:25:35,752][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[09:25:38,755][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[UQD86Pq][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]}], id[84], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[09:25:38,755][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[UQD86Pq][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[09:25:38,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[09:25:38,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[09:25:38,756][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] new_master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[09:25:38,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [1]
[09:25:38,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 1
[09:25:38,757][INFO ][org.elasticsearch.node.Node][Test worker] started
[09:25:38,757][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: pyGrD_jMQLq6-noY-EEWuQ)
[09:25:38,758][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[09:25:38,758][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[09:25:38,758][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [2]
[09:25:38,758][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 2
[09:25:38,760][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[09:25:38,760][INFO ][test                     ][Test worker] nodes are started
[09:25:38,760][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: Bjm1ij6yTASM8e6V3PedcA)
[09:25:38,760][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[09:25:38,761][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[09:25:38,761][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating Index [[test/Qf26_sgNSQS0TTgWoykLcw]], shards [5]/[1] - reason [create index]
[09:25:38,761][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:39,228][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:39,229][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:39,230][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[09:25:39,230][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test/Qf26_sgNSQS0TTgWoykLcw] closing index service (reason [cleaning up after validating index on master])
[09:25:39,230][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:39,230][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:39,230][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:39,230][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test/Qf26_sgNSQS0TTgWoykLcw] closed... (reason [cleaning up after validating index on master])
[09:25:39,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[09:25:39,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [3]
[09:25:39,230][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 3
[09:25:39,230][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [[test/Qf26_sgNSQS0TTgWoykLcw]] creating index
[09:25:39,231][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating Index [[test/Qf26_sgNSQS0TTgWoykLcw]], shards [5]/[1] - reason [create index]
[09:25:39,231][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:39,728][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:39,728][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][2] creating shard
[09:25:39,729][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Qf26_sgNSQS0TTgWoykLcw/2, shard=[test][2]}]
[09:25:39,729][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [test][2]
[09:25:39,730][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:39,730][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:39,731][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:39,731][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][1] creating shard
[09:25:39,731][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] starting recovery from store ...
[09:25:39,731][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Qf26_sgNSQS0TTgWoykLcw/1, shard=[test][1]}]
[09:25:39,731][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [test][1]
[09:25:39,732][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:39,732][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:39,733][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#3]] wipe translog location - creating new translog
[09:25:39,733][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:39,733][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][3] creating shard
[09:25:39,733][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] starting recovery from store ...
[09:25:39,733][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Qf26_sgNSQS0TTgWoykLcw/3, shard=[test][3]}]
[09:25:39,733][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [test][3]
[09:25:39,734][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:39,734][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:39,734][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:39,734][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#1]] wipe translog location - creating new translog
[09:25:39,734][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:39,734][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][0] creating shard
[09:25:39,735][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] starting recovery from store ...
[09:25:39,735][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Qf26_sgNSQS0TTgWoykLcw/0, shard=[test][0]}]
[09:25:39,735][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [test][0]
[09:25:39,735][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:39,735][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:39,736][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:39,736][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#2]] wipe translog location - creating new translog
[09:25:39,736][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:39,736][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] recovery completed from [shard_store], took [7ms]
[09:25:39,736][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [test][2] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery]]
[09:25:39,736][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [test][2] received shard started for [shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery]]
[09:25:39,737][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:39,737][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:39,737][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:39,737][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] recovery completed from [shard_store], took [5ms]
[09:25:39,737][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] starting recovery from store ...
[09:25:39,737][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [test][1] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]]
[09:25:39,737][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [test][1] received shard started for [shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]]
[09:25:39,738][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#4]] wipe translog location - creating new translog
[09:25:39,738][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [977ms] done applying updated cluster_state (version: 3, uuid: O4d0_5whTimsKtXEQIEIcw)
[09:25:39,738][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]]]: execute
[09:25:39,738][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=gJa-lmK5SF6rJ_NtfQ5cOQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:39.229Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery]])
[09:25:39,738][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=F2JCNI2cQti94I_kD-CJ9w], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:39.229Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]])
[09:25:39,739][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:39,739][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:39,739][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] recovery completed from [shard_store], took [5ms]
[09:25:39,739][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery]]
[09:25:39,739][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery]]
[09:25:39,739][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]]]
[09:25:39,739][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [4]
[09:25:39,740][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 4
[09:25:39,741][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:39,741][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:39,741][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] recovery completed from [shard_store], took [6ms]
[09:25:39,741][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery]]
[09:25:39,741][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:39,741][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,741][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery]]
[09:25:39,741][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,741][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][4] creating shard
[09:25:39,742][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Qf26_sgNSQS0TTgWoykLcw/4, shard=[test][4]}]
[09:25:39,742][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [test][4]
[09:25:39,743][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:39,743][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:39,743][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:39,743][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] starting recovery from store ...
[09:25:39,743][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,743][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,744][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [gJa-lmK5SF6rJ_NtfQ5cOQ], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [F2JCNI2cQti94I_kD-CJ9w], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 4, uuid: auxgPGp1Tk2RyP-8zzf73w)
[09:25:39,744][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:39,744][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#3]] wipe translog location - creating new translog
[09:25:39,745][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=wMVZOpY6QbKY2Biidiq73Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:39.229Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery]])
[09:25:39,745][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ojdFsvB0Stm6eqIK-4dX2Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:39.229Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery]])
[09:25:39,745][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:39,745][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:39,745][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [5]
[09:25:39,746][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 5
[09:25:39,748][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:39,748][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:39,748][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:39,748][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery]]
[09:25:39,748][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,748][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery]]
[09:25:39,748][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:39,748][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:39,749][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [wMVZOpY6QbKY2Biidiq73Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ojdFsvB0Stm6eqIK-4dX2Q], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: tcZ3HAvZREOQrOvjOFjJ1w)
[09:25:39,750][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:39,750][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=6tyyvvMgRa2TDahkezhhrw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:39.229Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery]])
[09:25:39,750][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:39,750][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [6]
[09:25:39,750][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 6
[09:25:39,751][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:39,752][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [6tyyvvMgRa2TDahkezhhrw], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: tr8Ma7SNRAGVmOxcrVKqGw)
[09:25:39,753][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[09:25:40,256][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:40,257][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [test/Qf26_sgNSQS0TTgWoykLcw] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:40,258][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[09:25:40,258][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [7]
[09:25:40,258][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 7
[09:25:40,258][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [[test/Qf26_sgNSQS0TTgWoykLcw]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:40,260][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [506ms] done applying updated cluster_state (version: 7, uuid: wFhzO-MSQRO1i05c2oMCUA)
[09:25:40,281][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[09:25:40,282][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[09:25:40,282][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating Index [[authorities/3RiJx0HdRxm9YG9D_7gz4w]], shards [5]/[1] - reason [create index]
[09:25:40,282][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:40,754][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:40,754][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:40,755][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[09:25:40,755][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities/3RiJx0HdRxm9YG9D_7gz4w] closing index service (reason [cleaning up after validating index on master])
[09:25:40,755][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:40,755][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:40,755][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:40,756][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities/3RiJx0HdRxm9YG9D_7gz4w] closed... (reason [cleaning up after validating index on master])
[09:25:40,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[09:25:40,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [8]
[09:25:40,756][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 8
[09:25:40,756][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [[authorities/3RiJx0HdRxm9YG9D_7gz4w]] creating index
[09:25:40,756][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating Index [[authorities/3RiJx0HdRxm9YG9D_7gz4w]], shards [5]/[1] - reason [create index]
[09:25:40,756][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:41,221][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:41,222][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][2] creating shard
[09:25:41,222][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3RiJx0HdRxm9YG9D_7gz4w/2, shard=[authorities][2]}]
[09:25:41,222][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[09:25:41,223][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:41,223][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:41,224][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:41,224][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][1] creating shard
[09:25:41,224][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] starting recovery from store ...
[09:25:41,224][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3RiJx0HdRxm9YG9D_7gz4w/1, shard=[authorities][1]}]
[09:25:41,224][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[09:25:41,225][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:41,225][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:41,225][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#1]] wipe translog location - creating new translog
[09:25:41,226][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:41,226][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][3] creating shard
[09:25:41,226][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] starting recovery from store ...
[09:25:41,226][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3RiJx0HdRxm9YG9D_7gz4w/3, shard=[authorities][3]}]
[09:25:41,226][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[09:25:41,226][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:41,226][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:41,227][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:41,227][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#2]] wipe translog location - creating new translog
[09:25:41,227][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:41,227][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][0] creating shard
[09:25:41,227][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] starting recovery from store ...
[09:25:41,228][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3RiJx0HdRxm9YG9D_7gz4w/0, shard=[authorities][0]}]
[09:25:41,228][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[09:25:41,228][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:41,228][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:41,228][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:41,229][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#4]] wipe translog location - creating new translog
[09:25:41,229][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:41,229][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] recovery completed from [shard_store], took [7ms]
[09:25:41,229][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery]]
[09:25:41,229][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery]]
[09:25:41,229][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:41,229][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:41,229][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] starting recovery from store ...
[09:25:41,230][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:41,230][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#2]] recovery completed from [shard_store], took [5ms]
[09:25:41,230][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#2]] [authorities][1] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]]
[09:25:41,230][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#2]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]]
[09:25:41,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [948ms] done applying updated cluster_state (version: 8, uuid: vB7t5Ru6RJ6rIIuuiKeW2w)
[09:25:41,231][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]]]: execute
[09:25:41,231][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#3]] wipe translog location - creating new translog
[09:25:41,231][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=LoPBbvpmQjiz-oalx0k4Qw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:40.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery]])
[09:25:41,231][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=gHdrUqgwTXyWrvP8r1Nugw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:40.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]])
[09:25:41,231][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:41,231][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#4]] recovery completed from [shard_store], took [5ms]
[09:25:41,232][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery]]
[09:25:41,232][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery]]
[09:25:41,232][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:41,233][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]]]
[09:25:41,233][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [9]
[09:25:41,233][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 9
[09:25:41,234][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:41,234][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:41,234][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery]]
[09:25:41,234][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery]]
[09:25:41,234][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:41,234][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:41,235][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,235][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,235][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][4] creating shard
[09:25:41,235][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/3RiJx0HdRxm9YG9D_7gz4w/4, shard=[authorities][4]}]
[09:25:41,235][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[09:25:41,236][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:41,237][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:41,238][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:41,239][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] starting recovery from store ...
[09:25:41,239][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,239][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,240][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [LoPBbvpmQjiz-oalx0k4Qw], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [gHdrUqgwTXyWrvP8r1Nugw], primary term [0], message [after new shard recovery]]]: took [9ms] done applying updated cluster_state (version: 9, uuid: Zfw6yK4xQ2Om0tYbKv6lVw)
[09:25:41,240][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[UQD86Pq][generic][T#1]] wipe translog location - creating new translog
[09:25:41,240][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:41,240][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=xBKYOZ7-S6KFn8It1trQfQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:40.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery]])
[09:25:41,240][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=DgbBKBKiQEGGp_9puugjhQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:40.754Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery]])
[09:25:41,241][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[UQD86Pq][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:41,242][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:41,242][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [10]
[09:25:41,242][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 10
[09:25:41,244][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:41,244][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][generic][T#1]] recovery completed from [shard_store], took [8ms]
[09:25:41,244][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery]]
[09:25:41,244][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery]]
[09:25:41,244][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:41,244][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [UQD86PqWR5iTBEvHzJHumw] for shard entry [shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,244][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:41,245][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:41,246][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [xBKYOZ7-S6KFn8It1trQfQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [DgbBKBKiQEGGp_9puugjhQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 10, uuid: KKKVzpViR6Of-O-eLoNj8g)
[09:25:41,246][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:41,246][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[UQD86PqWR5iTBEvHzJHumw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=7vuNRtQsQvujjoQwBdPHPQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:40.754Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery]])
[09:25:41,247][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:41,247][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [11]
[09:25:41,248][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 11
[09:25:41,248][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:41,249][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [7vuNRtQsQvujjoQwBdPHPQ], primary term [0], message [master {UQD86Pq}{UQD86PqWR5iTBEvHzJHumw}{7VtizdrhRw2UIbyri40zng}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: UqTR3IAJRUi7Zd1LFmtRvA)
[09:25:41,251][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[09:25:41,774][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:41,776][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [authorities/3RiJx0HdRxm9YG9D_7gz4w] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:41,776][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[09:25:41,776][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] publishing cluster state version [12]
[09:25:41,777][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] set local cluster state to version 12
[09:25:41,777][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] [[authorities/3RiJx0HdRxm9YG9D_7gz4w]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:41,779][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[UQD86Pq][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [528ms] done applying updated cluster_state (version: 12, uuid: zdelGRN_QEiedCV8QUGjVg)
[09:25:42,274][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[09:25:42,276][INFO ][test                     ][Test worker] stopping nodes
[09:25:42,276][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[09:25:42,277][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/Qf26_sgNSQS0TTgWoykLcw] closing index service (reason [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,277][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,277][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/3RiJx0HdRxm9YG9D_7gz4w] closing index service (reason [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:42,277][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[09:25:42,277][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:42,278][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,278][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:42,278][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,278][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:42,278][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:42,278][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:42,279][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:42,279][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[09:25:42,279][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[09:25:42,279][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:42,279][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[09:25:42,279][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[09:25:42,279][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,279][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:42,279][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:42,279][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:42,279][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:42,280][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:42,280][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[09:25:42,280][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:42,280][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,280][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:42,280][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:42,280][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:42,280][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,280][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,281][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:42,281][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:42,281][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:42,281][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:42,281][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:42,282][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:42,282][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[09:25:42,282][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:42,282][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[09:25:42,282][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:42,282][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[09:25:42,282][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[09:25:42,282][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,282][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,282][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,282][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,282][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:42,282][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:42,282][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:42,283][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:42,283][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:42,283][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[09:25:42,283][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[09:25:42,283][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,283][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,289][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:42,289][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:42,289][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:42,289][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:42,289][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:42,290][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:42,290][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:42,290][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:42,290][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:42,290][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:42,290][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[09:25:42,290][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[09:25:42,290][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:42,291][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[09:25:42,291][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[09:25:42,291][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:42,291][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:42,291][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:42,291][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:42,291][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:42,291][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/Qf26_sgNSQS0TTgWoykLcw] closed... (reason [shutdown])
[09:25:42,291][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:42,292][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:42,292][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:42,292][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[09:25:42,292][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:42,292][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[09:25:42,292][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:42,292][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/3RiJx0HdRxm9YG9D_7gz4w] closed... (reason [shutdown])
[09:25:42,292][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[09:25:42,292][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[09:25:42,293][INFO ][org.elasticsearch.node.Node][Test worker] closed
[09:25:42,301][INFO ][test                     ][Test worker] data files wiped
[09:25:44,301][INFO ][test                     ][Test worker] settings cluster name
[09:25:44,301][INFO ][test                     ][Test worker] starting nodes
[09:25:44,301][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[09:25:44,302][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[09:25:44,305][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[09:25:44,305][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [211gb], usable_space [210.8gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[09:25:44,305][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[09:25:44,305][INFO ][org.elasticsearch.node.Node][Test worker] node name [KeYJVTW] derived from node ID [KeYJVTW4Q-OW1BzhhuLj1Q]; set [node.name] to override
[09:25:44,305][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[53621], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_92/25.92-b15]
[09:25:44,305][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[09:25:44,305][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[09:25:44,305][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[09:25:44,305][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[09:25:44,306][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[09:25:44,307][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[09:25:44,307][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[09:25:44,310][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en4
        inet 10.1.1.42 netmask:255.255.0.0 broadcast:10.1.255.255 scope:site
        inet6 fe80::6a5b:35ff:febc:4672 prefixlen:64 scope:link
        hardware 68:5B:35:BC:46:72
        UP MULTICAST mtu:1500 index:10

[09:25:44,310][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[09:25:44,310][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[09:25:44,310][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[09:25:44,310][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[09:25:44,310][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[09:25:44,310][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[09:25:44,310][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[09:25:44,311][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[09:25:44,312][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[09:25:44,312][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[09:25:44,312][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[09:25:44,312][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[09:25:44,312][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[09:25:44,312][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[09:25:44,312][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[09:25:44,313][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:44,313][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:44,341][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[09:25:44,348][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[09:25:44,923][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[09:25:44,929][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[09:25:44,930][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[09:25:44,930][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[09:25:44,930][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[13]}, bound_addresses {local[13]}
[09:25:44,931][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[09:25:44,931][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [initial_join]: execute
[09:25:44,931][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[09:25:47,934][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[KeYJVTW][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]}], id[91], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[09:25:47,934][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[KeYJVTW][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[09:25:47,934][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[09:25:47,935][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[09:25:47,935][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] new_master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[09:25:47,935][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [1]
[09:25:47,935][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 1
[09:25:47,935][INFO ][org.elasticsearch.node.Node][Test worker] started
[09:25:47,935][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: 1zGqZ3DKSfCryIt4KjvZdA)
[09:25:47,936][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[09:25:47,936][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[09:25:47,936][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [2]
[09:25:47,936][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 2
[09:25:47,938][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[09:25:47,938][INFO ][test                     ][Test worker] nodes are started
[09:25:47,938][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [1ms] done applying updated cluster_state (version: 2, uuid: vDz7TpQRQHOafDm0fjnk2Q)
[09:25:47,938][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[09:25:47,939][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[09:25:47,939][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating Index [[test/6SPKwnadQOiQzv6SZu0S_Q]], shards [5]/[1] - reason [create index]
[09:25:47,939][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:48,428][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:48,428][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:48,429][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[09:25:48,429][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test/6SPKwnadQOiQzv6SZu0S_Q] closing index service (reason [cleaning up after validating index on master])
[09:25:48,429][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:48,429][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:48,429][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:48,429][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test/6SPKwnadQOiQzv6SZu0S_Q] closed... (reason [cleaning up after validating index on master])
[09:25:48,430][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[09:25:48,430][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [3]
[09:25:48,430][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 3
[09:25:48,430][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [[test/6SPKwnadQOiQzv6SZu0S_Q]] creating index
[09:25:48,430][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating Index [[test/6SPKwnadQOiQzv6SZu0S_Q]], shards [5]/[1] - reason [create index]
[09:25:48,430][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:48,916][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:48,916][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][2] creating shard
[09:25:48,917][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/6SPKwnadQOiQzv6SZu0S_Q/2, shard=[test][2]}]
[09:25:48,917][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [test][2]
[09:25:48,918][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:48,918][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:48,919][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:48,919][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][1] creating shard
[09:25:48,919][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] starting recovery from store ...
[09:25:48,919][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/6SPKwnadQOiQzv6SZu0S_Q/1, shard=[test][1]}]
[09:25:48,919][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [test][1]
[09:25:48,920][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:48,920][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:48,920][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#3]] wipe translog location - creating new translog
[09:25:48,921][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:48,921][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][3] creating shard
[09:25:48,921][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] starting recovery from store ...
[09:25:48,921][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/6SPKwnadQOiQzv6SZu0S_Q/3, shard=[test][3]}]
[09:25:48,921][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [test][3]
[09:25:48,921][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:48,922][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:48,922][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#1]] wipe translog location - creating new translog
[09:25:48,922][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:48,922][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:48,922][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:48,922][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][0] creating shard
[09:25:48,922][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] starting recovery from store ...
[09:25:48,923][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/6SPKwnadQOiQzv6SZu0S_Q/0, shard=[test][0]}]
[09:25:48,923][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [test][0]
[09:25:48,923][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:48,923][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:48,924][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:48,924][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] recovery completed from [shard_store], took [7ms]
[09:25:48,924][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#2]] wipe translog location - creating new translog
[09:25:48,924][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [test][2] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery]]
[09:25:48,924][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [test][2] received shard started for [shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery]]
[09:25:48,924][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:48,925][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] starting recovery from store ...
[09:25:48,925][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:48,925][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:48,925][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] recovery completed from [shard_store], took [5ms]
[09:25:48,925][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [test][1] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]]
[09:25:48,925][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [test][1] received shard started for [shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]]
[09:25:48,925][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [986ms] done applying updated cluster_state (version: 3, uuid: 1wm_KOWuRFaMBabG4nM_pA)
[09:25:48,926][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]]]: execute
[09:25:48,926][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=Or1uoR-sT4G9vHJl0YGu0Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:48.428Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery]])
[09:25:48,926][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=elzGsSsUR1mYtDb-lklXPQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:48.428Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]])
[09:25:48,926][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#4]] wipe translog location - creating new translog
[09:25:48,927][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:48,927][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]]]
[09:25:48,927][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [4]
[09:25:48,927][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 4
[09:25:48,927][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:48,927][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] recovery completed from [shard_store], took [6ms]
[09:25:48,927][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery]]
[09:25:48,927][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery]]
[09:25:48,929][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:48,929][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:48,929][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] recovery completed from [shard_store], took [6ms]
[09:25:48,929][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery]]
[09:25:48,929][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery]]
[09:25:48,930][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:48,930][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,930][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,930][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][4] creating shard
[09:25:48,930][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/6SPKwnadQOiQzv6SZu0S_Q/4, shard=[test][4]}]
[09:25:48,930][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [test][4]
[09:25:48,931][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:48,931][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:48,932][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:48,932][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] starting recovery from store ...
[09:25:48,932][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,932][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,933][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#3]] wipe translog location - creating new translog
[09:25:48,933][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [Or1uoR-sT4G9vHJl0YGu0Q], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [elzGsSsUR1mYtDb-lklXPQ], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: Q9YgKfPxSAaftwpZPCpRIg)
[09:25:48,933][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:48,933][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=0b_PJeplR3Gz18sdblY78Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:48.428Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery]])
[09:25:48,933][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=xs7YWP4FSti0nzwPqwxtDg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:48.428Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery]])
[09:25:48,934][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:48,934][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:48,934][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [5]
[09:25:48,934][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 5
[09:25:48,935][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:48,936][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:48,936][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] recovery completed from [shard_store], took [5ms]
[09:25:48,936][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,936][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [after new shard recovery]]
[09:25:48,936][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:48,936][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [after new shard recovery]]
[09:25:48,936][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:48,937][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [0b_PJeplR3Gz18sdblY78Q], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [xs7YWP4FSti0nzwPqwxtDg], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 5, uuid: 9eNiT906RnO1sc3XP5sHsQ)
[09:25:48,937][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [after new shard recovery]]]: execute
[09:25:48,937][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=GYE-4uS0S1efu575-mz7kQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:48.428Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[09:25:48,938][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [after new shard recovery]]]
[09:25:48,938][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [6]
[09:25:48,938][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 6
[09:25:48,938][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:48,939][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][4]], allocation id [GYE-4uS0S1efu575-mz7kQ], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: 1vfrcKVoR3WuJRyqIOKzbg)
[09:25:48,941][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[09:25:49,422][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:49,424][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [test/6SPKwnadQOiQzv6SZu0S_Q] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:49,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[09:25:49,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [7]
[09:25:49,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 7
[09:25:49,425][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [[test/6SPKwnadQOiQzv6SZu0S_Q]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:49,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [486ms] done applying updated cluster_state (version: 7, uuid: 8a9Y2GqsTWyLmxhWjutJcA)
[09:25:49,449][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[09:25:49,449][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[09:25:49,450][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating Index [[authorities/QtXaC0v3TyaTVMA-pTZIoA]], shards [5]/[1] - reason [create index]
[09:25:49,450][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:49,927][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:49,928][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:49,929][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[09:25:49,929][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities/QtXaC0v3TyaTVMA-pTZIoA] closing index service (reason [cleaning up after validating index on master])
[09:25:49,929][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:49,929][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:49,929][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:49,929][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities/QtXaC0v3TyaTVMA-pTZIoA] closed... (reason [cleaning up after validating index on master])
[09:25:49,929][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[09:25:49,929][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [8]
[09:25:49,929][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 8
[09:25:49,930][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [[authorities/QtXaC0v3TyaTVMA-pTZIoA]] creating index
[09:25:49,930][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating Index [[authorities/QtXaC0v3TyaTVMA-pTZIoA]], shards [5]/[1] - reason [create index]
[09:25:49,930][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:50,428][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:50,428][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][2] creating shard
[09:25:50,428][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/QtXaC0v3TyaTVMA-pTZIoA/2, shard=[authorities][2]}]
[09:25:50,428][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[09:25:50,429][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:50,429][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:50,430][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:50,430][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][1] creating shard
[09:25:50,430][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] starting recovery from store ...
[09:25:50,431][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/QtXaC0v3TyaTVMA-pTZIoA/1, shard=[authorities][1]}]
[09:25:50,431][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[09:25:50,431][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:50,431][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:50,431][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#1]] wipe translog location - creating new translog
[09:25:50,432][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:50,432][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][3] creating shard
[09:25:50,432][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] starting recovery from store ...
[09:25:50,432][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/QtXaC0v3TyaTVMA-pTZIoA/3, shard=[authorities][3]}]
[09:25:50,432][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:50,432][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[09:25:50,433][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:50,433][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#2]] wipe translog location - creating new translog
[09:25:50,433][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:50,433][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:50,433][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][0] creating shard
[09:25:50,433][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] starting recovery from store ...
[09:25:50,433][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:50,434][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/QtXaC0v3TyaTVMA-pTZIoA/0, shard=[authorities][0]}]
[09:25:50,434][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[09:25:50,434][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:50,434][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:50,434][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#4]] wipe translog location - creating new translog
[09:25:50,434][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:50,435][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] recovery completed from [shard_store], took [6ms]
[09:25:50,435][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery]]
[09:25:50,435][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery]]
[09:25:50,435][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:50,435][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] starting recovery from store ...
[09:25:50,435][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:50,435][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#2]] recovery completed from [shard_store], took [4ms]
[09:25:50,435][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#2]] [authorities][1] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]]
[09:25:50,435][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:50,435][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#2]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]]
[09:25:50,436][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#3]] wipe translog location - creating new translog
[09:25:50,437][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [987ms] done applying updated cluster_state (version: 8, uuid: KyvXfhQNQ8-kMedULCpBXA)
[09:25:50,437][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]]]: execute
[09:25:50,437][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=UbGhuVE5Tz6nFFtMiIoHzQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:49.928Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery]])
[09:25:50,437][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=Oh8XYW1oS26NAm7SRyOcUg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:49.928Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]])
[09:25:50,437][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:50,438][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:50,438][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#4]] recovery completed from [shard_store], took [5ms]
[09:25:50,438][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery]]
[09:25:50,438][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery]]
[09:25:50,439][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]]]
[09:25:50,439][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [9]
[09:25:50,439][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 9
[09:25:50,440][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:50,440][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:50,440][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery]]
[09:25:50,440][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery]]
[09:25:50,440][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:50,440][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:50,440][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,440][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,440][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][4] creating shard
[09:25:50,441][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/QtXaC0v3TyaTVMA-pTZIoA/4, shard=[authorities][4]}]
[09:25:50,441][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[09:25:50,442][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:50,442][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:50,443][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:50,443][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] starting recovery from store ...
[09:25:50,443][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,443][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,444][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[KeYJVTW][generic][T#1]] wipe translog location - creating new translog
[09:25:50,444][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [UbGhuVE5Tz6nFFtMiIoHzQ], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [Oh8XYW1oS26NAm7SRyOcUg], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 9, uuid: jDpDYByiR2eGj-nG_5MzZw)
[09:25:50,445][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:50,445][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=0-1-XNVdTEaW6SSH_Iy74g], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:49.928Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery]])
[09:25:50,445][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=vl3w0X5iSMWt6kXEYCYDgQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:49.928Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery]])
[09:25:50,445][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[KeYJVTW][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:50,446][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:50,446][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [10]
[09:25:50,446][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 10
[09:25:50,447][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:50,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:50,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][generic][T#1]] recovery completed from [shard_store], took [6ms]
[09:25:50,448][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,448][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:50,448][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [KeYJVTW4Q-OW1BzhhuLj1Q] for shard entry [shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [after new shard recovery]]
[09:25:50,448][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [after new shard recovery]]
[09:25:50,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:50,449][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [0-1-XNVdTEaW6SSH_Iy74g], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [vl3w0X5iSMWt6kXEYCYDgQ], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 10, uuid: g0remu9rQtyHVK5lJaokUw)
[09:25:50,449][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [after new shard recovery]]]: execute
[09:25:50,449][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[KeYJVTW4Q-OW1BzhhuLj1Q], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=9XWoZ3ViQBms_iraHqBysw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:49.928Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[09:25:50,451][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [after new shard recovery]]]
[09:25:50,451][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [11]
[09:25:50,451][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 11
[09:25:50,451][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:50,452][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [master {KeYJVTW}{KeYJVTW4Q-OW1BzhhuLj1Q}{opD6xCijRzWM9k4IhxkyMg}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [9XWoZ3ViQBms_iraHqBysw], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: jVDdxCFBQPGBl5hDJ4bCtQ)
[09:25:50,454][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[09:25:50,942][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:50,944][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [authorities/QtXaC0v3TyaTVMA-pTZIoA] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:50,944][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[09:25:50,944][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] publishing cluster state version [12]
[09:25:50,944][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] set local cluster state to version 12
[09:25:50,945][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] [[authorities/QtXaC0v3TyaTVMA-pTZIoA]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:50,947][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[KeYJVTW][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [492ms] done applying updated cluster_state (version: 12, uuid: OI27PTMFSxiazy_v76eDfQ)
[09:25:51,451][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _source = null
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = docs
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = null
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _uid = docs#1
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _version = -1
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title = A title
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = A creator
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = A contributor
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc authorID = 1
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = John Doe
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = John Doe
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A title
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A creator
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A contributor
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = 1
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _source
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _uid
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _version
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = authorID
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[09:25:51,453][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[09:25:51,453][INFO ][test                     ][Test worker] stopping nodes
[09:25:51,453][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[09:25:51,454][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[09:25:51,454][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/6SPKwnadQOiQzv6SZu0S_Q] closing index service (reason [shutdown])
[09:25:51,455][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[09:25:51,455][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[09:25:51,455][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,455][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,455][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/QtXaC0v3TyaTVMA-pTZIoA] closing index service (reason [shutdown])
[09:25:51,455][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:51,455][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:51,455][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[09:25:51,455][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:51,455][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:51,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:51,456][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:51,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:51,457][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:51,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[09:25:51,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:51,457][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,457][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[09:25:51,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:51,457][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:51,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:51,458][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:51,458][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:51,458][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:51,458][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:51,458][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:51,458][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[09:25:51,458][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:51,458][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[09:25:51,458][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[09:25:51,458][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[09:25:51,458][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,459][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:51,459][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:51,459][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:51,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:51,460][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:51,460][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[09:25:51,460][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[09:25:51,460][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:51,460][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,460][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[09:25:51,460][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,460][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[09:25:51,460][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:51,460][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:51,460][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,460][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:51,460][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,461][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:51,461][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:51,461][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[09:25:51,461][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[09:25:51,461][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,461][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,468][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:25:51,468][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:25:51,468][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:25:51,469][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:51,469][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:25:51,469][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:51,469][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:25:51,469][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[09:25:51,469][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:51,469][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:51,469][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[09:25:51,469][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:25:51,470][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/6SPKwnadQOiQzv6SZu0S_Q] closed... (reason [shutdown])
[09:25:51,470][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:51,470][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:51,470][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[09:25:51,470][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[09:25:51,470][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:25:51,470][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:25:51,470][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:25:51,470][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:25:51,470][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:25:51,471][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:25:51,471][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:25:51,471][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[09:25:51,471][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:51,471][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[09:25:51,471][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:25:51,471][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/QtXaC0v3TyaTVMA-pTZIoA] closed... (reason [shutdown])
[09:25:51,471][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[09:25:51,471][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[09:25:51,472][INFO ][org.elasticsearch.node.Node][Test worker] closed
[09:25:51,482][INFO ][test                     ][Test worker] data files wiped
[09:25:53,483][INFO ][test                     ][Test worker] settings cluster name
[09:25:53,483][INFO ][test                     ][Test worker] starting nodes
[09:25:53,483][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[09:25:53,484][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[09:25:53,486][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[09:25:53,486][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [211gb], usable_space [210.8gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[09:25:53,486][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[09:25:53,486][INFO ][org.elasticsearch.node.Node][Test worker] node name [_px3OSh] derived from node ID [_px3OShDSSygS6avbeoKuQ]; set [node.name] to override
[09:25:53,486][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[53621], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_92/25.92-b15]
[09:25:53,486][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[09:25:53,487][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[09:25:53,487][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[09:25:53,487][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[09:25:53,487][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[09:25:53,487][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[09:25:53,487][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[09:25:53,487][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[09:25:53,488][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[09:25:53,488][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[09:25:53,490][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en4
        inet 10.1.1.42 netmask:255.255.0.0 broadcast:10.1.255.255 scope:site
        inet6 fe80::6a5b:35ff:febc:4672 prefixlen:64 scope:link
        hardware 68:5B:35:BC:46:72
        UP MULTICAST mtu:1500 index:10

[09:25:53,491][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[09:25:53,491][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[09:25:53,491][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[09:25:53,491][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[09:25:53,491][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[09:25:53,491][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[09:25:53,491][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[09:25:53,491][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[09:25:53,493][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[09:25:53,493][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[09:25:53,493][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[09:25:53,493][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[09:25:53,494][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[09:25:53,494][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[09:25:53,494][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[09:25:53,494][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:53,494][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[09:25:53,519][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[09:25:53,523][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[09:25:54,069][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[09:25:54,072][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[09:25:54,073][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[09:25:54,073][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[09:25:54,074][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[14]}, bound_addresses {local[14]}
[09:25:54,074][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[09:25:54,074][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [initial_join]: execute
[09:25:54,075][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[09:25:57,077][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[_px3OSh][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]}], id[98], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[09:25:57,077][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[_px3OSh][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[09:25:57,078][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[09:25:57,078][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[09:25:57,078][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] new_master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[09:25:57,078][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [1]
[09:25:57,078][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 1
[09:25:57,078][INFO ][org.elasticsearch.node.Node][Test worker] started
[09:25:57,079][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [0s] done applying updated cluster_state (version: 1, uuid: CTplA2vlRBKWcr1U_sSGvQ)
[09:25:57,079][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[09:25:57,079][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[09:25:57,079][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [2]
[09:25:57,079][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 2
[09:25:57,081][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[09:25:57,081][INFO ][test                     ][Test worker] nodes are started
[09:25:57,081][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [1ms] done applying updated cluster_state (version: 2, uuid: vxGIMWXKSdG0h2x4ZmeWZQ)
[09:25:57,081][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[09:25:57,082][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[09:25:57,082][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[test/tRlD5zegRzWH-wdh9wU45g]], shards [5]/[1] - reason [create index]
[09:25:57,082][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:57,584][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:57,585][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:57,586][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[09:25:57,586][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test/tRlD5zegRzWH-wdh9wU45g] closing index service (reason [cleaning up after validating index on master])
[09:25:57,586][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:57,586][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:57,586][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:57,586][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test/tRlD5zegRzWH-wdh9wU45g] closed... (reason [cleaning up after validating index on master])
[09:25:57,586][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[09:25:57,586][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [3]
[09:25:57,586][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 3
[09:25:57,586][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[test/tRlD5zegRzWH-wdh9wU45g]] creating index
[09:25:57,587][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[test/tRlD5zegRzWH-wdh9wU45g]], shards [5]/[1] - reason [create index]
[09:25:57,587][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:58,077][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:58,077][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][2] creating shard
[09:25:58,078][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tRlD5zegRzWH-wdh9wU45g/2, shard=[test][2]}]
[09:25:58,078][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [test][2]
[09:25:58,080][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:58,080][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:58,081][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:58,081][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][1] creating shard
[09:25:58,081][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] starting recovery from store ...
[09:25:58,081][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tRlD5zegRzWH-wdh9wU45g/1, shard=[test][1]}]
[09:25:58,081][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [test][1]
[09:25:58,081][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:58,082][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:58,082][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:58,082][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#3]] wipe translog location - creating new translog
[09:25:58,082][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][3] creating shard
[09:25:58,082][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] starting recovery from store ...
[09:25:58,082][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tRlD5zegRzWH-wdh9wU45g/3, shard=[test][3]}]
[09:25:58,082][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [test][3]
[09:25:58,083][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:58,083][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:58,083][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#1]] wipe translog location - creating new translog
[09:25:58,083][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:58,084][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:58,084][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:58,084][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][0] creating shard
[09:25:58,084][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] starting recovery from store ...
[09:25:58,084][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tRlD5zegRzWH-wdh9wU45g/0, shard=[test][0]}]
[09:25:58,084][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [test][0]
[09:25:58,085][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:58,085][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:58,085][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:58,085][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] recovery completed from [shard_store], took [7ms]
[09:25:58,085][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [test][2] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery]]
[09:25:58,085][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [test][2] received shard started for [shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery]]
[09:25:58,086][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#2]] wipe translog location - creating new translog
[09:25:58,086][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:58,086][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] recovery completed from [shard_store], took [4ms]
[09:25:58,086][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [test][1] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]]
[09:25:58,086][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:58,086][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [test][1] received shard started for [shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]]
[09:25:58,086][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] starting recovery from store ...
[09:25:58,086][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:58,087][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 3, uuid: lHSpX_LoSsKC52Ub6_ES8w)
[09:25:58,087][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]]]: execute
[09:25:58,087][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=6EypT5ChSYSKZ3-QRAe1ZA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:57.585Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery]])
[09:25:58,087][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#4]] wipe translog location - creating new translog
[09:25:58,087][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=_5jQrEcJT5aIjGktvk5VGg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:57.585Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]])
[09:25:58,088][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:58,088][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:58,088][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] recovery completed from [shard_store], took [5ms]
[09:25:58,088][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery]]
[09:25:58,088][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery]]
[09:25:58,088][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]]]
[09:25:58,088][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [4]
[09:25:58,089][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 4
[09:25:58,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:58,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:58,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] recovery completed from [shard_store], took [5ms]
[09:25:58,090][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:58,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery]]
[09:25:58,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery]]
[09:25:58,090][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,090][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][4] creating shard
[09:25:58,091][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/tRlD5zegRzWH-wdh9wU45g/4, shard=[test][4]}]
[09:25:58,091][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [test][4]
[09:25:58,092][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:58,092][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:58,092][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:58,092][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] starting recovery from store ...
[09:25:58,092][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,093][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,094][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][2]], allocation id [6EypT5ChSYSKZ3-QRAe1ZA], primary term [0], message [after new shard recovery], shard id [[test][1]], allocation id [_5jQrEcJT5aIjGktvk5VGg], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 4, uuid: ievVefVKQUuQ3kE0fkq1oQ)
[09:25:58,094][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#3]] wipe translog location - creating new translog
[09:25:58,094][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:58,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=qXGnhkFSS6-Au3AABOdlHQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:57.585Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery]])
[09:25:58,094][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=Dedd8dr6QE2fwNqNfOmoTQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:57.585Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery]])
[09:25:58,095][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:58,095][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [5]
[09:25:58,095][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 5
[09:25:58,095][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:58,097][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:58,097][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:58,097][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:58,097][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery]]
[09:25:58,097][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery]]
[09:25:58,097][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,097][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:58,097][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:58,098][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [qXGnhkFSS6-Au3AABOdlHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [Dedd8dr6QE2fwNqNfOmoTQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: rtCwcCUeRS6n9PrHaPRiLQ)
[09:25:58,098][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:58,098][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=OG-Fu7V9SB6youjWTRdM1A], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:57.585Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery]])
[09:25:58,099][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:58,099][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [6]
[09:25:58,099][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 6
[09:25:58,100][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:58,100][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [OG-Fu7V9SB6youjWTRdM1A], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: Q4dAuW4iRAykNYtuWWnHdw)
[09:25:58,102][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[09:25:58,631][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:58,633][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [test/tRlD5zegRzWH-wdh9wU45g] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:58,633][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[09:25:58,633][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [7]
[09:25:58,633][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 7
[09:25:58,633][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[test/tRlD5zegRzWH-wdh9wU45g]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:25:58,636][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [533ms] done applying updated cluster_state (version: 7, uuid: ClTxJr_CRv-x1K9CCT9dAw)
[09:25:58,655][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[09:25:58,655][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[09:25:58,655][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[authorities/UsNMZsHKSyWhaHeBQ98bFg]], shards [5]/[1] - reason [create index]
[09:25:58,656][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:59,179][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:59,179][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[09:25:59,180][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[09:25:59,180][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities/UsNMZsHKSyWhaHeBQ98bFg] closing index service (reason [cleaning up after validating index on master])
[09:25:59,180][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:59,181][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:25:59,181][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:25:59,181][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities/UsNMZsHKSyWhaHeBQ98bFg] closed... (reason [cleaning up after validating index on master])
[09:25:59,181][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[09:25:59,181][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [8]
[09:25:59,181][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 8
[09:25:59,181][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[authorities/UsNMZsHKSyWhaHeBQ98bFg]] creating index
[09:25:59,181][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[authorities/UsNMZsHKSyWhaHeBQ98bFg]], shards [5]/[1] - reason [create index]
[09:25:59,182][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:25:59,660][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:25:59,661][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][2] creating shard
[09:25:59,661][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UsNMZsHKSyWhaHeBQ98bFg/2, shard=[authorities][2]}]
[09:25:59,661][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[09:25:59,662][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:59,662][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:59,663][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:59,663][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][1] creating shard
[09:25:59,663][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] starting recovery from store ...
[09:25:59,663][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UsNMZsHKSyWhaHeBQ98bFg/1, shard=[authorities][1]}]
[09:25:59,664][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[09:25:59,664][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:59,664][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:59,664][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#1]] wipe translog location - creating new translog
[09:25:59,665][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:59,665][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][3] creating shard
[09:25:59,665][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] starting recovery from store ...
[09:25:59,665][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:59,666][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UsNMZsHKSyWhaHeBQ98bFg/3, shard=[authorities][3]}]
[09:25:59,666][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[09:25:59,666][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:59,666][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:59,667][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#2]] wipe translog location - creating new translog
[09:25:59,667][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:59,667][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][0] creating shard
[09:25:59,667][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] starting recovery from store ...
[09:25:59,667][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:59,667][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] recovery completed from [shard_store], took [6ms]
[09:25:59,667][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UsNMZsHKSyWhaHeBQ98bFg/0, shard=[authorities][0]}]
[09:25:59,667][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[09:25:59,667][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery]]
[09:25:59,667][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery]]
[09:25:59,668][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#2]] no translog ID present in the current generation - creating one
[09:25:59,668][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:59,668][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:59,669][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#4]] wipe translog location - creating new translog
[09:25:59,669][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:59,669][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] starting recovery from store ...
[09:25:59,669][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#4]] no translog ID present in the current generation - creating one
[09:25:59,670][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:59,670][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] recovery completed from [shard_store], took [6ms]
[09:25:59,670][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [authorities][1] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]]
[09:25:59,670][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]]
[09:25:59,671][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#3]] wipe translog location - creating new translog
[09:25:59,671][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 8, uuid: FYJkocYySWas9AeOiuUyrA)
[09:25:59,671][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]]]: execute
[09:25:59,671][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=AEFpNLwuSQaeidutvCCxSg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:59.179Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery]])
[09:25:59,671][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=xvJoTxa4ScSjII6oErPzvw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:59.179Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]])
[09:25:59,672][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#3]] no translog ID present in the current generation - creating one
[09:25:59,672][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:59,672][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] recovery completed from [shard_store], took [6ms]
[09:25:59,672][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery]]
[09:25:59,672][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery]]
[09:25:59,673][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]]]
[09:25:59,673][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [9]
[09:25:59,673][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 9
[09:25:59,674][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:59,674][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] recovery completed from [shard_store], took [6ms]
[09:25:59,674][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:59,674][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery]]
[09:25:59,674][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery]]
[09:25:59,674][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:59,674][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,674][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,674][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][4] creating shard
[09:25:59,674][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/UsNMZsHKSyWhaHeBQ98bFg/4, shard=[authorities][4]}]
[09:25:59,674][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[09:25:59,675][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:25:59,675][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:25:59,676][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:25:59,676][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] starting recovery from store ...
[09:25:59,676][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,676][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,677][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#1]] wipe translog location - creating new translog
[09:25:59,678][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [AEFpNLwuSQaeidutvCCxSg], primary term [0], message [after new shard recovery], shard id [[authorities][1]], allocation id [xvJoTxa4ScSjII6oErPzvw], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 9, uuid: tUtQGCboSu-DyoJAUtfnww)
[09:25:59,678][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:25:59,678][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=uz033yVUQCGQDRwc43D11Q], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:59.179Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery]])
[09:25:59,678][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=PGF9vvXbT5uOyKVESB7XQA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:59.179Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery]])
[09:25:59,678][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#1]] no translog ID present in the current generation - creating one
[09:25:59,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:25:59,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [10]
[09:25:59,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 10
[09:25:59,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:25:59,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:59,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] recovery completed from [shard_store], took [7ms]
[09:25:59,681][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [after new shard recovery]]
[09:25:59,681][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,681][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:25:59,681][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [after new shard recovery]]
[09:25:59,681][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:59,683][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [uz033yVUQCGQDRwc43D11Q], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [PGF9vvXbT5uOyKVESB7XQA], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 10, uuid: VME7SNW9QWiJlh940PTkig)
[09:25:59,683][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [after new shard recovery]]]: execute
[09:25:59,683][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=CVAvmKleS4q8pEj15DKyhw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:25:59.179Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]])
[09:25:59,684][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [after new shard recovery]]]
[09:25:59,684][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [11]
[09:25:59,684][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 11
[09:25:59,684][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:25:59,685][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][4]], allocation id [CVAvmKleS4q8pEj15DKyhw], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: AAmS1g0RSSW_O61r8JmMKw)
[09:25:59,686][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[09:26:00,178][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:26:00,179][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [authorities/UsNMZsHKSyWhaHeBQ98bFg] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:26:00,180][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[09:26:00,180][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [12]
[09:26:00,180][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 12
[09:26:00,180][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[authorities/UsNMZsHKSyWhaHeBQ98bFg]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:26:00,183][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [496ms] done applying updated cluster_state (version: 12, uuid: m463KTpwR3iw4opoUeCRkA)
[09:26:00,210][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete index 'books'
[09:26:00,210][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: execute
[09:26:00,211][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[books/zVs0Mt-IQqea0tcSyv0vlA]], shards [5]/[1] - reason [create index]
[09:26:00,211][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:26:00,696][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:26:00,698][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books] creating index, cause [api], templates [], shards [5]/[1], mappings [test]
[09:26:00,699][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books] closing ... (reason [cleaning up after validating index on master])
[09:26:00,699][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books/zVs0Mt-IQqea0tcSyv0vlA] closing index service (reason [cleaning up after validating index on master])
[09:26:00,699][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:26:00,699][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] full cache clear, reason [close]
[09:26:00,699][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[09:26:00,699][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books/zVs0Mt-IQqea0tcSyv0vlA] closed... (reason [cleaning up after validating index on master])
[09:26:00,699][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [13], source [create-index [books], cause [api]]
[09:26:00,699][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [13]
[09:26:00,699][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 13
[09:26:00,699][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[books/zVs0Mt-IQqea0tcSyv0vlA]] creating index
[09:26:00,699][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating Index [[books/zVs0Mt-IQqea0tcSyv0vlA]], shards [5]/[1] - reason [create index]
[09:26:00,700][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[09:26:01,185][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:26:01,185][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[books/zVs0Mt-IQqea0tcSyv0vlA]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}}}}}]
[09:26:01,186][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][2] creating shard
[09:26:01,186][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/zVs0Mt-IQqea0tcSyv0vlA/2, shard=[books][2]}]
[09:26:01,186][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [books][2]
[09:26:01,188][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:26:01,188][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:26:01,191][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:26:01,191][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][1] creating shard
[09:26:01,191][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] starting recovery from store ...
[09:26:01,191][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/zVs0Mt-IQqea0tcSyv0vlA/1, shard=[books][1]}]
[09:26:01,191][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [books][1]
[09:26:01,194][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:26:01,194][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:26:01,194][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#2]] wipe translog location - creating new translog
[09:26:01,195][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:26:01,195][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][3] creating shard
[09:26:01,195][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] starting recovery from store ...
[09:26:01,195][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/zVs0Mt-IQqea0tcSyv0vlA/3, shard=[books][3]}]
[09:26:01,195][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [books][3]
[09:26:01,195][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#2]] no translog ID present in the current generation - creating one
[09:26:01,198][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:26:01,198][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:26:01,198][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#4]] wipe translog location - creating new translog
[09:26:01,201][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:26:01,201][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] recovery completed from [shard_store], took [14ms]
[09:26:01,201][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [books][2] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery]]
[09:26:01,201][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [books][2] received shard started for [shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery]]
[09:26:01,201][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:26:01,201][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#4]] no translog ID present in the current generation - creating one
[09:26:01,201][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][0] creating shard
[09:26:01,201][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] starting recovery from store ...
[09:26:01,201][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/zVs0Mt-IQqea0tcSyv0vlA/0, shard=[books][0]}]
[09:26:01,201][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [books][0]
[09:26:01,202][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:26:01,202][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#3]] wipe translog location - creating new translog
[09:26:01,202][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:26:01,203][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#3]] no translog ID present in the current generation - creating one
[09:26:01,203][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:26:01,203][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] starting recovery from store ...
[09:26:01,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:26:01,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#4]] recovery completed from [shard_store], took [13ms]
[09:26:01,204][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [books][1] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]]
[09:26:01,204][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#4]] [books][1] received shard started for [shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]]
[09:26:01,205][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: took [994ms] done applying updated cluster_state (version: 13, uuid: 7ZD0HiOyTv6X-uvzSykRlQ)
[09:26:01,205][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#1]] wipe translog location - creating new translog
[09:26:01,205][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery], shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]]]: execute
[09:26:01,205][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][2] starting shard [books][2], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=I4xZHHULR7uNUHPrZ7FeXA], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:26:00.698Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery]])
[09:26:01,206][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][1] starting shard [books][1], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=EB0xLHgsQNKw-sRQkNV5Jg], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:26:00.698Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]])
[09:26:01,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:26:01,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#3]] recovery completed from [shard_store], took [10ms]
[09:26:01,206][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#1]] no translog ID present in the current generation - creating one
[09:26:01,206][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [books][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery]]
[09:26:01,206][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#3]] [books][3] received shard started for [shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery]]
[09:26:01,208][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [14], source [shard-started[shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery], shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]]]
[09:26:01,208][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [14]
[09:26:01,208][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 14
[09:26:01,208][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:26:01,208][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#1]] recovery completed from [shard_store], took [7ms]
[09:26:01,208][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [books][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery]]
[09:26:01,208][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery]]
[09:26:01,209][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:26:01,209][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:26:01,209][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][3] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,209][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][3] received shard started for [shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,209][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][4] creating shard
[09:26:01,209][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/zVs0Mt-IQqea0tcSyv0vlA/4, shard=[books][4]}]
[09:26:01,209][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] creating shard_id [books][4]
[09:26:01,210][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[09:26:01,211][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]
[09:26:01,211][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[09:26:01,211][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] starting recovery from store ...
[09:26:01,211][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][0] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,211][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,212][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[_px3OSh][generic][T#2]] wipe translog location - creating new translog
[09:26:01,213][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][2]], allocation id [I4xZHHULR7uNUHPrZ7FeXA], primary term [0], message [after new shard recovery], shard id [[books][1]], allocation id [EB0xLHgsQNKw-sRQkNV5Jg], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 14, uuid: 3RPl-lHWSFeD_wqaYjOIzA)
[09:26:01,213][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:26:01,213][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][3] starting shard [books][3], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=BKPlbPqRR7CE0AvHItBXsw], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:26:00.698Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery]])
[09:26:01,213][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][0] starting shard [books][0], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=kxryHQkXSxWnugJ1RY1SHQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:26:00.698Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery]])
[09:26:01,214][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[_px3OSh][generic][T#2]] no translog ID present in the current generation - creating one
[09:26:01,215][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [15], source [shard-started[shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:26:01,215][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [15]
[09:26:01,215][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 15
[09:26:01,215][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[09:26:01,215][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][generic][T#2]] recovery completed from [shard_store], took [6ms]
[09:26:01,215][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:26:01,215][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [books][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery]]
[09:26:01,216][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][generic][T#2]] [books][4] received shard started for [shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery]]
[09:26:01,216][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][4] sending [internal:cluster/shard/started] to [_px3OShDSSygS6avbeoKuQ] for shard entry [shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,216][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][4] received shard started for [shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[09:26:01,216][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:26:01,217][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [BKPlbPqRR7CE0AvHItBXsw], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [kxryHQkXSxWnugJ1RY1SHQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 15, uuid: RGj1YvMtS0yXvtfvPZqOrg)
[09:26:01,217][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[09:26:01,217][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books][4] starting shard [books][4], node[_px3OShDSSygS6avbeoKuQ], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=fbJxtvOHSx-sZVug3SUMuQ], unassigned_info[[reason=INDEX_CREATED], at[2017-01-03T08:26:00.698Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery]])
[09:26:01,218][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [16], source [shard-started[shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[09:26:01,218][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [16]
[09:26:01,218][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 16
[09:26:01,218][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[09:26:01,220][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [fbJxtvOHSx-sZVug3SUMuQ], primary term [0], message [master {_px3OSh}{_px3OShDSSygS6avbeoKuQ}{widti_z1T0KmyPJIm7b9iA}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 16, uuid: GkOJTK3HSuSI-l7dA-klbw)
[09:26:01,222][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[09:26:01,721][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] using dynamic[true]
[09:26:01,724][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [books/zVs0Mt-IQqea0tcSyv0vlA] update_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:26:01,724][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] cluster state updated, version [17], source [put-mapping[test]]
[09:26:01,724][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] publishing cluster state version [17]
[09:26:01,724][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] set local cluster state to version 17
[09:26:01,724][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] [[books/zVs0Mt-IQqea0tcSyv0vlA]] updating mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[09:26:01,728][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[_px3OSh][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [506ms] done applying updated cluster_state (version: 17, uuid: NLcI25BpTpyJ0e8tezBjzA)
[09:26:01,758][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] mappings={properties={authorID={type=ref, ref_index=authorities, ref_type=persons, ref_fields=[author], copy_to=[dc.creator, bib.contributor]}, bib={properties={contributor={type=text}}}, dc={properties={creator={type=text}}}, title={type=text, fields={keyword={type=keyword, ignore_above=256}}}}}
[09:26:01,761][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unref hits = 1
[09:26:01,761][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[09:26:01,764][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] ref hits = 1
[09:26:01,764][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[09:26:01,766][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 unref hits = 1
[09:26:01,766][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[09:26:01,767][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 ref hits = 1
[09:26:01,767][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[09:26:01,767][INFO ][test                     ][Test worker] stopping nodes
[09:26:01,767][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[09:26:01,768][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books] closing ... (reason [shutdown])
[09:26:01,769][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books/zVs0Mt-IQqea0tcSyv0vlA] closing index service (reason [shutdown])
[09:26:01,769][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[09:26:01,769][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,769][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test] closing ... (reason [shutdown])
[09:26:01,769][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,769][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:26:01,769][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:26:01,769][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:26:01,769][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test/tRlD5zegRzWH-wdh9wU45g] closing index service (reason [shutdown])
[09:26:01,770][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closing... (reason: [shutdown])
[09:26:01,769][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[09:26:01,770][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,770][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,770][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[09:26:01,770][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[09:26:01,770][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:26:01,770][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/UsNMZsHKSyWhaHeBQ98bFg] closing index service (reason [shutdown])
[09:26:01,771][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[09:26:01,771][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[09:26:01,771][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:26:01,771][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[09:26:01,771][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[09:26:01,771][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,771][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:26:01,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:26:01,771][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:26:01,771][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[09:26:01,771][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:26:01,771][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closed (reason: [shutdown])
[09:26:01,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closing... (reason: [shutdown])
[09:26:01,772][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:26:01,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,772][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:26:01,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[09:26:01,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[09:26:01,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[09:26:01,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:26:01,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[09:26:01,772][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:26:01,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[09:26:01,773][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,773][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[09:26:01,773][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:26:01,773][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:26:01,773][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[09:26:01,773][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:26:01,773][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[09:26:01,773][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:26:01,773][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closed (reason: [shutdown])
[09:26:01,773][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closing... (reason: [shutdown])
[09:26:01,774][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[09:26:01,774][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:26:01,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[09:26:01,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[09:26:01,774][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:26:01,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[09:26:01,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[09:26:01,774][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,774][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[09:26:01,774][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:26:01,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[09:26:01,774][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:26:01,774][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[09:26:01,775][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closed (reason: [shutdown])
[09:26:01,775][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closing... (reason: [shutdown])
[09:26:01,775][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[09:26:01,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[09:26:01,775][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[09:26:01,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:26:01,775][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:26:01,775][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[09:26:01,775][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[09:26:01,775][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,776][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[09:26:01,776][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,776][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[09:26:01,776][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closed (reason: [shutdown])
[09:26:01,776][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closing... (reason: [shutdown])
[09:26:01,776][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,776][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,783][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:26:01,783][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:26:01,783][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:26:01,783][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:26:01,783][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:26:01,784][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:26:01,784][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:26:01,784][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[09:26:01,784][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[09:26:01,784][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[09:26:01,784][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:26:01,784][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[09:26:01,784][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[09:26:01,785][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[09:26:01,785][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[09:26:01,785][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[09:26:01,785][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[09:26:01,785][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[09:26:01,785][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[09:26:01,785][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[09:26:01,785][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[09:26:01,785][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[09:26:01,785][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[09:26:01,785][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[09:26:01,786][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[09:26:01,786][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:26:01,786][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[09:26:01,786][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closed (reason: [shutdown])
[09:26:01,786][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[09:26:01,786][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[09:26:01,786][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[09:26:01,786][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[09:26:01,786][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[09:26:01,786][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[09:26:01,786][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:26:01,787][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#3]] full cache clear, reason [close]
[09:26:01,787][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[09:26:01,787][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[09:26:01,787][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[09:26:01,787][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/UsNMZsHKSyWhaHeBQ98bFg] closed... (reason [shutdown])
[09:26:01,787][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test/tRlD5zegRzWH-wdh9wU45g] closed... (reason [shutdown])
[09:26:01,787][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books/zVs0Mt-IQqea0tcSyv0vlA] closed... (reason [shutdown])
[09:26:01,787][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[09:26:01,787][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[09:26:01,788][INFO ][org.elasticsearch.node.Node][Test worker] closed
[09:26:01,802][INFO ][test                     ][Test worker] data files wiped
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 03.01.2017 09:26:11</p>
</div>
</div>
</body>
</html>
