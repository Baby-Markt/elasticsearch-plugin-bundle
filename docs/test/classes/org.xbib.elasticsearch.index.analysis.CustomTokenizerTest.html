<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.analysis.CustomTokenizerTest</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.analysis.CustomTokenizerTest</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.analysis.html">org.xbib.elasticsearch.index.analysis</a> &gt; CustomTokenizerTest</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">1</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">12.088s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testCustomTokenizerRemoval</td>
<td>12.088s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[23:10:55,460][INFO ][test                     ][Test worker] settings={cluster.name=null, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:10:55,469][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:10:55,533][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:10:55,536][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:10:55,537][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:10:55,538][INFO ][org.elasticsearch.node.Node][Test worker] node name [7EddQ-C] derived from node ID [7EddQ-C9THSMygqlWcgOnw]; set [node.name] to override
[23:10:55,540][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:10:55,540][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:10:55,547][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:10:55,549][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:10:55,550][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:10:55,572][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:10:55,578][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:10:55,579][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:10:55,588][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:10:55,588][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:10:55,589][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:10:55,589][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:10:55,589][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:10:55,590][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:10:55,590][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:10:55,590][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:10:55,591][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:10:55,591][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:10:55,591][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:10:55,696][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:10:55,911][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:10:55,954][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:10:55,963][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:10:56,007][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:10:56,020][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:10:56,021][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:10:56,023][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:10:56,024][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:10:56,141][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:10:57,051][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:10:57,054][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:10:57,067][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:10:57,075][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:10:57,086][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:10:57,089][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:10:57,090][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:10:57,094][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:10:57,101][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:10:57,552][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:10:57,819][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:10:58,205][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:10:58,290][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:10:58,301][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:10:58,301][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:10:58,314][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[1]}, bound_addresses {local[1]}
[23:10:58,333][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:10:58,334][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:10:58,337][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: took [2ms] no change in cluster_state
[23:11:01,358][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]}], id[7], master [null],cluster_state_version [-1], cluster_name[elasticsearch]}
[23:11:01,361][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:11:01,365][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:11:01,381][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:11:01,382][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] new_master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:11:01,382][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:11:01,384][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:11:01,388][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:11:01,388][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [23ms] done applying updated cluster_state (version: 1, uuid: _AGOG6I9TnmtSpueB1f5hQ)
[23:11:01,401][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:11:01,403][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:11:01,404][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:11:01,404][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:11:01,421][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:11:01,421][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [19ms] done applying updated cluster_state (version: 2, uuid: ZBLEVKfMQ2qOMowGeb3mww)
[23:11:01,421][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [demo], cause [api]]: execute
[23:11:01,464][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[demo/BmIpml7TQNKxqU4-kd5w0w]], shards [5]/[1] - reason [create index]
[23:11:01,482][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:02,591][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:02,754][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo] creating index, cause [api], templates [], shards [5]/[1], mappings [demo]
[23:11:02,786][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo] closing ... (reason [cleaning up after validating index on master])
[23:11:02,786][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/BmIpml7TQNKxqU4-kd5w0w] closing index service (reason [cleaning up after validating index on master])
[23:11:02,787][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:02,787][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:11:02,788][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:02,789][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/BmIpml7TQNKxqU4-kd5w0w] closed... (reason [cleaning up after validating index on master])
[23:11:02,791][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [demo], cause [api]]
[23:11:02,791][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:11:02,792][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:11:02,792][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] creating index
[23:11:02,796][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[demo/BmIpml7TQNKxqU4-kd5w0w]], shards [5]/[1] - reason [create index]
[23:11:02,801][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:03,650][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:03,651][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] adding mapping [demo], source [{&quot;demo&quot;:{&quot;date_detection&quot;:false,&quot;properties&quot;:{&quot;text&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;my_analyzer&quot;}}}}]
[23:11:03,654][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating shard
[23:11:03,664][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/BmIpml7TQNKxqU4-kd5w0w/1, shard=[demo][1]}]
[23:11:03,664][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][1]
[23:11:03,670][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:03,750][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:03,761][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:03,762][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating shard
[23:11:03,763][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/BmIpml7TQNKxqU4-kd5w0w/2, shard=[demo][2]}]
[23:11:03,764][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][2]
[23:11:03,766][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:03,766][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:03,768][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:03,767][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] starting recovery from store ...
[23:11:03,768][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating shard
[23:11:03,768][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] starting recovery from store ...
[23:11:03,768][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/BmIpml7TQNKxqU4-kd5w0w/3, shard=[demo][3]}]
[23:11:03,768][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][3]
[23:11:03,769][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:03,770][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:03,771][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:03,772][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating shard
[23:11:03,772][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:03,773][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/BmIpml7TQNKxqU4-kd5w0w/0, shard=[demo][0]}]
[23:11:03,773][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][0]
[23:11:03,775][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:03,776][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:03,778][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:03,778][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] starting recovery from store ...
[23:11:03,784][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [demo], cause [api]]: took [2.3s] done applying updated cluster_state (version: 3, uuid: sw4jSSpdQam9VotzuflK-A)
[23:11:03,862][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] wipe translog location - creating new translog
[23:11:03,862][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#3]] wipe translog location - creating new translog
[23:11:03,863][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#4]] wipe translog location - creating new translog
[23:11:03,863][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#1]] wipe translog location - creating new translog
[23:11:03,878][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#2]] no translog ID present in the current generation - creating one
[23:11:03,879][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#1]] no translog ID present in the current generation - creating one
[23:11:03,879][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#4]] no translog ID present in the current generation - creating one
[23:11:03,880][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#3]] no translog ID present in the current generation - creating one
[23:11:03,903][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:03,903][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:03,903][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:03,903][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:03,904][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [135ms]
[23:11:03,904][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] recovery completed from [shard_store], took [131ms]
[23:11:03,904][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] recovery completed from [shard_store], took [244ms]
[23:11:03,904][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] recovery completed from [shard_store], took [140ms]
[23:11:03,904][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]]
[23:11:03,904][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery]]
[23:11:03,904][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery]]
[23:11:03,904][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery]]
[23:11:03,905][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] received shard started for [shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery]]
[23:11:03,905][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] received shard started for [shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery]]
[23:11:03,905][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] received shard started for [shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery]]
[23:11:03,905][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] received shard started for [shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]]
[23:11:03,906][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery], shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]]]: execute
[23:11:03,925][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] starting shard [demo][3], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=MLVWe7OYQIC0SUuKNxgqww], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:02.757Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery]])
[23:11:03,925][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] starting shard [demo][1], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=mlQenfSHT0SLly186B5Huw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:02.757Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery]])
[23:11:03,926][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] starting shard [demo][0], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=hdzLZAvaTPSrlfiuZVh7sw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:02.757Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]])
[23:11:03,933][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery], shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]]]
[23:11:03,933][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:11:03,934][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:11:03,935][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating shard
[23:11:03,935][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/BmIpml7TQNKxqU4-kd5w0w/4, shard=[demo][4]}]
[23:11:03,936][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][4]
[23:11:03,936][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:03,937][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:03,938][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:03,938][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:03,938][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:03,939][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:03,939][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] received shard started for [shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:03,939][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:03,939][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] wipe translog location - creating new translog
[23:11:03,940][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:03,941][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#2]] no translog ID present in the current generation - creating one
[23:11:03,943][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [MLVWe7OYQIC0SUuKNxgqww], primary term [0], message [after new shard recovery], shard id [[demo][1]], allocation id [mlQenfSHT0SLly186B5Huw], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [hdzLZAvaTPSrlfiuZVh7sw], primary term [0], message [after new shard recovery]]]: took [37ms] done applying updated cluster_state (version: 4, uuid: j-Ew-3S9QrSoTVFIByhLsw)
[23:11:03,944][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:03,944][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] starting shard [demo][2], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=A1ViRa_UQkKGsQSBZwwXQQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:02.757Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery]])
[23:11:03,947][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:03,947][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [12ms]
[23:11:03,948][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery]]
[23:11:03,948][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:03,948][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][4] received shard started for [shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery]]
[23:11:03,948][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:11:03,948][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:11:03,949][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:03,949][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] received shard started for [shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:03,949][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:03,951][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [A1ViRa_UQkKGsQSBZwwXQQ], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [6ms] done applying updated cluster_state (version: 5, uuid: QvjC6JrTSEiPcR-OUzR1rQ)
[23:11:03,951][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery], shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:03,952][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] starting shard [demo][4], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=kas9C89CTy6TzI8ovpKbVw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:02.757Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery]])
[23:11:03,954][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery], shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:03,954][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:11:03,954][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:11:03,955][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:03,957][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [after new shard recovery], shard id [[demo][4]], allocation id [kas9C89CTy6TzI8ovpKbVw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{t-hBmWjzQfaJCw1eSBZcJg}{local}{local[1]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 6, uuid: PiW9G2aVTt2vBoHe-RzVDQ)
[23:11:04,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [delete-index [[demo/BmIpml7TQNKxqU4-kd5w0w]]]: execute
[23:11:04,165][DEBUG][org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] deleting index
[23:11:04,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [7], source [delete-index [[demo/BmIpml7TQNKxqU4-kd5w0w]]]
[23:11:04,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:11:04,167][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:11:04,167][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] cleaning index, no longer part of the metadata
[23:11:04,167][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo] closing ... (reason [index no longer part of the metadata])
[23:11:04,167][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/BmIpml7TQNKxqU4-kd5w0w] closing index service (reason [index no longer part of the metadata])
[23:11:04,167][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [0] closing... (reason: [index no longer part of the metadata])
[23:11:04,168][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [STARTED]-&gt;[CLOSED], reason [index no longer part of the metadata]
[23:11:04,168][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close now acquiring writeLock
[23:11:04,168][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close acquired writeLock
[23:11:04,168][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] translog closed
[23:11:04,170][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] engine closed [api]
[23:11:04,174][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store reference count on close: 0
[23:11:04,174][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [0] closed (reason: [index no longer part of the metadata])
[23:11:04,175][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [1] closing... (reason: [index no longer part of the metadata])
[23:11:04,175][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [STARTED]-&gt;[CLOSED], reason [index no longer part of the metadata]
[23:11:04,175][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close now acquiring writeLock
[23:11:04,175][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close acquired writeLock
[23:11:04,176][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] translog closed
[23:11:04,178][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] engine closed [api]
[23:11:04,180][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store reference count on close: 0
[23:11:04,180][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [1] closed (reason: [index no longer part of the metadata])
[23:11:04,180][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [2] closing... (reason: [index no longer part of the metadata])
[23:11:04,181][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [STARTED]-&gt;[CLOSED], reason [index no longer part of the metadata]
[23:11:04,181][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close now acquiring writeLock
[23:11:04,181][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close acquired writeLock
[23:11:04,182][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] translog closed
[23:11:04,182][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] engine closed [api]
[23:11:04,184][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store reference count on close: 0
[23:11:04,184][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [2] closed (reason: [index no longer part of the metadata])
[23:11:04,184][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [3] closing... (reason: [index no longer part of the metadata])
[23:11:04,184][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [STARTED]-&gt;[CLOSED], reason [index no longer part of the metadata]
[23:11:04,184][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close now acquiring writeLock
[23:11:04,184][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close acquired writeLock
[23:11:04,185][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] translog closed
[23:11:04,187][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] engine closed [api]
[23:11:04,189][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store reference count on close: 0
[23:11:04,189][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [3] closed (reason: [index no longer part of the metadata])
[23:11:04,189][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [4] closing... (reason: [index no longer part of the metadata])
[23:11:04,189][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [STARTED]-&gt;[CLOSED], reason [index no longer part of the metadata]
[23:11:04,189][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close now acquiring writeLock
[23:11:04,190][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] close acquired writeLock
[23:11:04,190][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] translog closed
[23:11:04,192][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] engine closed [api]
[23:11:04,195][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store reference count on close: 0
[23:11:04,195][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [4] closed (reason: [index no longer part of the metadata])
[23:11:04,195][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:04,195][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:11:04,196][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:04,198][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/BmIpml7TQNKxqU4-kd5w0w] closed... (reason [index no longer part of the metadata])
[23:11:04,198][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/BmIpml7TQNKxqU4-kd5w0w] deleting index store reason [index no longer part of the metadata]
[23:11:04,200][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][generic][T#3]] [demo/BmIpml7TQNKxqU4-kd5w0w] processing pending deletes
[23:11:04,202][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:11:04,202][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [delete-index [[demo/BmIpml7TQNKxqU4-kd5w0w]]]: took [36ms] done applying updated cluster_state (version: 7, uuid: 6IsMIkQySqWOfSPMcQBggQ)
[23:11:04,204][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:11:04,205][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:11:04,214][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:11:04,214][INFO ][test                     ][Test worker] settings={cluster.name=null, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:11:04,217][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:11:04,230][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:11:04,231][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:11:04,231][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:11:04,233][INFO ][org.elasticsearch.node.Node][Test worker] node name [7EddQ-C] derived from node ID [7EddQ-C9THSMygqlWcgOnw]; set [node.name] to override
[23:11:04,233][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:11:04,233][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:11:04,234][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:11:04,234][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:11:04,234][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no plugins loaded
[23:11:04,236][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:11:04,236][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:11:04,237][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:11:04,237][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:11:04,237][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:11:04,238][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:11:04,238][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:11:04,238][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:11:04,239][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:11:04,239][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:11:04,239][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:11:04,239][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:11:04,240][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:11:04,240][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:11:04,241][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:11:04,246][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:11:04,247][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:11:04,247][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:11:04,247][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:11:04,248][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:11:04,248][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:11:04,249][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:11:04,250][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:11:04,251][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:11:04,255][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:11:04,256][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:11:04,256][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:11:04,257][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:11:04,258][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:11:04,258][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:11:04,258][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:11:04,259][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:11:04,259][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:11:04,306][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:11:04,330][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:11:04,358][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 1ms to load state
[23:11:04,361][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:11:04,362][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:11:04,364][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[2]}, bound_addresses {local[2]}
[23:11:04,365][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:11:04,366][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:11:04,367][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:11:07,371][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]}], id[14], master [null],cluster_state_version [-1], cluster_name[elasticsearch]}
[23:11:07,371][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:11:07,372][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:11:07,372][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:11:07,372][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] new_master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:11:07,372][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:11:07,373][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:11:07,373][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:11:07,391][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: k1gGHLIiQUeleQq-qki_Og)
[23:11:07,400][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:11:07,401][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:11:07,401][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:11:07,402][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:11:07,403][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] cleaning index, no longer part of the metadata
[23:11:07,409][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:11:07,409][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [9ms] done applying updated cluster_state (version: 2, uuid: U2OlKiDLSbOqoybqmLwAlw)
[23:11:07,410][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [demo], cause [api]]: execute
[23:11:07,411][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[demo/Utbg6-yIQTOUEnPXnGcH9Q]], shards [5]/[1] - reason [create index]
[23:11:07,413][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:07,416][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:07,418][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo] creating index, cause [api], templates [], shards [5]/[1], mappings []
[23:11:07,423][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo] closing ... (reason [cleaning up after validating index on master])
[23:11:07,423][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closing index service (reason [cleaning up after validating index on master])
[23:11:07,424][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:07,424][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:11:07,425][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:07,425][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closed... (reason [cleaning up after validating index on master])
[23:11:07,426][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [demo], cause [api]]
[23:11:07,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:11:07,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:11:07,428][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]] creating index
[23:11:07,429][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[demo/Utbg6-yIQTOUEnPXnGcH9Q]], shards [5]/[1] - reason [create index]
[23:11:07,431][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:07,432][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:07,432][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating shard
[23:11:07,434][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1, shard=[demo][1]}]
[23:11:07,434][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][1]
[23:11:07,437][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:07,438][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:07,441][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:07,441][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating shard
[23:11:07,442][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2, shard=[demo][2]}]
[23:11:07,441][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] starting recovery from store ...
[23:11:07,442][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][2]
[23:11:07,444][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:07,445][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:07,446][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#3]] wipe translog location - creating new translog
[23:11:07,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:07,448][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating shard
[23:11:07,448][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] starting recovery from store ...
[23:11:07,449][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#3]] no translog ID present in the current generation - creating one
[23:11:07,450][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3, shard=[demo][3]}]
[23:11:07,450][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][3]
[23:11:07,452][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#1]] wipe translog location - creating new translog
[23:11:07,452][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:07,453][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:07,455][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#1]] no translog ID present in the current generation - creating one
[23:11:07,456][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:07,456][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating shard
[23:11:07,456][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:07,457][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0, shard=[demo][0]}]
[23:11:07,457][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][0]
[23:11:07,459][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:07,460][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] wipe translog location - creating new translog
[23:11:07,460][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:07,462][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:07,462][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:07,463][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] recovery completed from [shard_store], took [28ms]
[23:11:07,463][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] recovery completed from [shard_store], took [21ms]
[23:11:07,463][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery]]
[23:11:07,463][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:07,463][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]]
[23:11:07,463][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] received shard started for [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery]]
[23:11:07,464][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] received shard started for [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]]
[23:11:07,464][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#2]] no translog ID present in the current generation - creating one
[23:11:07,464][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] starting recovery from store ...
[23:11:07,466][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [demo], cause [api]]: took [56ms] done applying updated cluster_state (version: 3, uuid: lOwP5-vRSgatl-uX16q1aA)
[23:11:07,467][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]]]: execute
[23:11:07,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] starting shard [demo][1], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=GaYq7DR6SJCJfaKxOZrS9Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:07.418Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery]])
[23:11:07,468][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] starting shard [demo][2], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=WT_IoIrYTp-g8LeJCVMtzA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:07.418Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]])
[23:11:07,468][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#4]] wipe translog location - creating new translog
[23:11:07,470][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:07,470][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [21ms]
[23:11:07,471][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery]]
[23:11:07,471][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#4]] no translog ID present in the current generation - creating one
[23:11:07,471][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] received shard started for [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery]]
[23:11:07,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]]]
[23:11:07,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:11:07,472][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:11:07,477][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:07,477][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating shard
[23:11:07,477][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] recovery completed from [shard_store], took [20ms]
[23:11:07,478][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery]]
[23:11:07,478][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] received shard started for [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery]]
[23:11:07,479][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4, shard=[demo][4]}]
[23:11:07,479][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][4]
[23:11:07,481][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:07,482][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:07,484][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:07,485][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] starting recovery from store ...
[23:11:07,485][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:07,486][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:07,486][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:07,487][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] received shard started for [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:07,487][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:07,487][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] received shard started for [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:07,490][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#3]] wipe translog location - creating new translog
[23:11:07,492][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after new shard recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after new shard recovery]]]: took [24ms] done applying updated cluster_state (version: 4, uuid: -eAJnpfMShayFPyS4rx5WA)
[23:11:07,493][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery], shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:07,493][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] starting shard [demo][3], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=VdBwviCJT9mJPtYfI5fgWg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:07.418Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery]])
[23:11:07,494][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] starting shard [demo][0], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=qIkUCqsWRJWRrN22NrIqZA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:07.418Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery]])
[23:11:07,496][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#3]] no translog ID present in the current generation - creating one
[23:11:07,497][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery], shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:07,497][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:11:07,498][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:11:07,503][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:07,503][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:07,504][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] recovery completed from [shard_store], took [25ms]
[23:11:07,505][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]]
[23:11:07,505][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:07,505][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][4] received shard started for [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]]
[23:11:07,507][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after new shard recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after new shard recovery], shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{LOzfB9TfQBuqZtCFTcts3Q}{local}{local[2]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [14ms] done applying updated cluster_state (version: 5, uuid: eS3CgYsETamzLimNI-4P3w)
[23:11:07,508][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]]]: execute
[23:11:07,508][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] starting shard [demo][4], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=vKwmcV-nTB2KT4DKGjkE_w], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:07.418Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]])
[23:11:07,511][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]]]
[23:11:07,512][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:11:07,512][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:11:07,513][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:07,516][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 6, uuid: aRgx9OwUT3Kh2027zBCuHg)
[23:11:07,516][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:11:07,519][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo] closing ... (reason [shutdown])
[23:11:07,520][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closing index service (reason [shutdown])
[23:11:07,520][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:11:07,520][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:11:07,520][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:11:07,521][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:11:07,521][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:11:07,522][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:11:07,523][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:11:07,523][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:11:07,523][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:11:07,523][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:11:07,523][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:11:07,523][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:11:07,523][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:11:07,523][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:11:07,524][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:11:07,524][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:11:07,524][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:11:07,524][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:11:07,525][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:11:07,525][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:11:07,525][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:11:07,525][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:11:07,525][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:11:07,526][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:11:07,528][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:11:07,528][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:11:07,528][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:11:07,529][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:11:07,529][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:11:07,529][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:11:07,529][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:11:07,529][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:11:07,530][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:11:07,531][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:11:07,532][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:11:07,532][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:11:07,532][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:11:07,532][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:11:07,532][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:11:07,532][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:11:07,532][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:11:07,533][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:11:07,533][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:11:07,533][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:11:07,534][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:11:07,534][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:11:07,534][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:11:07,535][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:11:07,535][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closed... (reason [shutdown])
[23:11:07,536][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:11:07,536][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:11:07,540][INFO ][org.elasticsearch.node.Node][Test worker] closed
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 05.02.2017 23:15:52</p>
</div>
</div>
</body>
</html>
