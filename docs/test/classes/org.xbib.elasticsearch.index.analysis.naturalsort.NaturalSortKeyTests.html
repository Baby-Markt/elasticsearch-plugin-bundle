<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.analysis.naturalsort.NaturalSortKeyTests</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.analysis.naturalsort.NaturalSortKeyTests</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.analysis.naturalsort.html">org.xbib.elasticsearch.index.analysis.naturalsort</a> &gt; NaturalSortKeyTests</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">3</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">1m51.29s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testComplex</td>
<td>36.394s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testDewey</td>
<td>38.501s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSort</td>
<td>36.392s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[23:11:50,177][INFO ][test                     ][Test worker] settings cluster name
[23:11:50,177][INFO ][test                     ][Test worker] starting nodes
[23:11:50,177][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:11:50,178][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:11:50,187][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:11:50,187][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:11:50,187][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:11:50,195][INFO ][org.elasticsearch.node.Node][Test worker] node name [7EddQ-C] derived from node ID [7EddQ-C9THSMygqlWcgOnw]; set [node.name] to override
[23:11:50,196][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:11:50,196][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:11:50,196][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:11:50,196][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:11:50,196][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:11:50,197][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:11:50,198][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:11:50,199][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:11:50,200][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:11:50,203][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:11:50,204][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:11:50,204][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:11:50,204][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:11:50,205][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:11:50,205][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:11:50,205][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:11:50,205][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:11:50,206][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:11:50,209][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:11:50,209][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:11:50,209][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:11:50,210][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:11:50,211][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:11:50,211][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:11:50,212][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:11:50,212][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:11:50,212][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:11:50,242][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:11:50,253][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:11:50,760][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:11:50,782][DEBUG][org.elasticsearch.common.util.IndexFolderUpgrader][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q] no upgrade needed - already upgraded
[23:11:50,986][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 1ms to load state
[23:11:50,987][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:11:50,987][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:11:50,988][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[3]}, bound_addresses {local[3]}
[23:11:50,989][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:11:50,989][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:11:50,990][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:11:53,992][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}], id[21], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:11:53,992][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[7EddQ-C][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:11:53,993][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:11:53,993][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:11:53,993][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] new_master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:11:53,993][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:11:53,994][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:11:53,995][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:11:53,995][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: A85l3p7FS5CY4lH7gSJphg)
[23:11:53,998][DEBUG][org.elasticsearch.indices.IndicesQueryCache][elasticsearch[7EddQ-C][generic][T#4]] using [node] query cache with size [364mb] max filter count [10000]
[23:11:53,999][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][generic][T#4]] creating Index [[demo/Utbg6-yIQTOUEnPXnGcH9Q]], shards [5]/[1] - reason [metadata verification]
[23:11:54,000][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][generic][T#4]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:54,587][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][generic][T#4]] using dynamic[true]
[23:11:54,588][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][generic][T#4]] clearing all bitsets because [close]
[23:11:54,588][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[7EddQ-C][generic][T#4]] full cache clear, reason [close]
[23:11:54,588][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][generic][T#4]] clearing all bitsets because [close]
[23:11:54,589][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:11:54,593][DEBUG][org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] skipping rebalance due to in-flight shard/store fetches
[23:11:54,593][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:11:54,594][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:11:54,594][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:11:54,595][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/BmIpml7TQNKxqU4-kd5w0w]] cleaning index, no longer part of the metadata
[23:11:54,596][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#2]] [demo][0] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0]
[23:11:54,596][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#1]] [demo][4] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4]
[23:11:54,596][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#3]] [demo][3] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3]
[23:11:54,597][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#5]] [demo][1] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1]
[23:11:54,597][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#4]] [demo][2] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2]
[23:11:54,600][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] recovered [1] indices into cluster_state
[23:11:54,600][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [11ms] done applying updated cluster_state (version: 2, uuid: 8fhU6U6lRU2e1SDbuo3_7Q)
[23:11:54,628][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#1]] [demo][4] shard state info found: [version [-1], primary [true], allocation [[id=vKwmcV-nTB2KT4DKGjkE_w]]]
[23:11:54,628][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#2]] [demo][0] shard state info found: [version [-1], primary [true], allocation [[id=qIkUCqsWRJWRrN22NrIqZA]]]
[23:11:54,628][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#4]] [demo][2] shard state info found: [version [-1], primary [true], allocation [[id=WT_IoIrYTp-g8LeJCVMtzA]]]
[23:11:54,628][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#3]] [demo][3] shard state info found: [version [-1], primary [true], allocation [[id=VdBwviCJT9mJPtYfI5fgWg]]]
[23:11:54,628][DEBUG][org.elasticsearch.gateway.TransportNodesListGatewayStartedShards][elasticsearch[7EddQ-C][fetch_shard_started][T#5]] [demo][1] shard state info found: [version [-1], primary [true], allocation [[id=GaYq7DR6SJCJfaKxOZrS9Q]]]
[23:11:54,630][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [cluster_reroute(async_shard_fetch)]: execute
[23:11:54,632][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][2]: found 1 allocation candidates of [demo][2], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] based on allocation ids: [[WT_IoIrYTp-g8LeJCVMtzA]]
[23:11:54,633][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][2]: allocating [[demo][2], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]]] to [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}] on primary allocation
[23:11:54,633][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][0]: found 1 allocation candidates of [demo][0], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] based on allocation ids: [[qIkUCqsWRJWRrN22NrIqZA]]
[23:11:54,634][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][0]: allocating [[demo][0], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]]] to [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}] on primary allocation
[23:11:54,634][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][1]: found 1 allocation candidates of [demo][1], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] based on allocation ids: [[GaYq7DR6SJCJfaKxOZrS9Q]]
[23:11:54,634][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][1]: allocating [[demo][1], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]]] to [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}] on primary allocation
[23:11:54,634][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][3]: found 1 allocation candidates of [demo][3], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] based on allocation ids: [[VdBwviCJT9mJPtYfI5fgWg]]
[23:11:54,634][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][3]: allocating [[demo][3], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]]] to [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}] on primary allocation
[23:11:54,635][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][4]: found 1 allocation candidates of [demo][4], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] based on allocation ids: [[vKwmcV-nTB2KT4DKGjkE_w]]
[23:11:54,635][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][4]: throttling allocation [[demo][4], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]]] to [[org.elasticsearch.gateway.PrimaryShardAllocator$DecidedNode@45b4e7d]] on primary allocation
[23:11:54,636][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [3], source [cluster_reroute(async_shard_fetch)]
[23:11:54,636][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:11:54,636][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:11:54,636][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]] creating index
[23:11:54,637][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[demo/Utbg6-yIQTOUEnPXnGcH9Q]], shards [5]/[1] - reason [create index]
[23:11:54,637][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:55,200][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:55,200][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating shard
[23:11:55,201][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1]
[23:11:55,201][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] creating using an existing path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/1, shard=[demo][1]}]
[23:11:55,201][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][1]
[23:11:55,202][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:55,202][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:55,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:55,204][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating shard
[23:11:55,204][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] starting recovery from store ...
[23:11:55,205][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2]
[23:11:55,205][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] creating using an existing path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/2, shard=[demo][2]}]
[23:11:55,205][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][2]
[23:11:55,205][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:55,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:55,206][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:55,207][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] starting recovery from store ...
[23:11:55,207][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating shard
[23:11:55,207][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3]
[23:11:55,207][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] creating using an existing path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/3, shard=[demo][3]}]
[23:11:55,208][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][3]
[23:11:55,208][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:55,208][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:55,209][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:55,209][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating shard
[23:11:55,210][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:55,210][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0]
[23:11:55,210][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] creating using an existing path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/0, shard=[demo][0]}]
[23:11:55,210][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][0]
[23:11:55,211][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:55,212][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:55,213][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:55,213][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] starting recovery from store ...
[23:11:55,214][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [cluster_reroute(async_shard_fetch)]: took [584ms] done applying updated cluster_state (version: 3, uuid: 2H2apcY3TFKMfWnEdncq4A)
[23:11:55,215][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#3]] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 1}
[23:11:55,230][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#4]] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 1}
[23:11:55,241][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 1}
[23:11:55,241][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#1]] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 1}
[23:11:55,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:55,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:55,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:55,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:55,254][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [46ms]
[23:11:55,254][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] recovery completed from [shard_store], took [52ms]
[23:11:55,254][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] recovery completed from [shard_store], took [49ms]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] recovery completed from [shard_store], took [43ms]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][3] received shard started for [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [demo][1] received shard started for [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [demo][2] received shard started for [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [demo][0] received shard started for [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery]]
[23:11:55,254][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery], shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]]]: execute
[23:11:55,255][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][3] starting shard [demo][3], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[existing recovery], s[INITIALIZING], a[id=VdBwviCJT9mJPtYfI5fgWg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] (shard started task: [shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery]])
[23:11:55,255][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][2] starting shard [demo][2], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[existing recovery], s[INITIALIZING], a[id=WT_IoIrYTp-g8LeJCVMtzA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] (shard started task: [shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery]])
[23:11:55,255][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][1] starting shard [demo][1], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[existing recovery], s[INITIALIZING], a[id=GaYq7DR6SJCJfaKxOZrS9Q], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] (shard started task: [shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]])
[23:11:55,255][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][4]: found 1 allocation candidates of [demo][4], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[deciders_throttled]] based on allocation ids: [[vKwmcV-nTB2KT4DKGjkE_w]]
[23:11:55,256][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[demo/Utbg6-yIQTOUEnPXnGcH9Q]][4]: allocating [[demo][4], node[null], [P], recovery_source[existing recovery], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[deciders_throttled]]] to [{7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]}] on primary allocation
[23:11:55,257][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery], shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]]]
[23:11:55,257][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:11:55,257][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:11:55,257][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating shard
[23:11:55,258][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] loaded data path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4], state path [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4]
[23:11:55,258][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] creating using an existing path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Utbg6-yIQTOUEnPXnGcH9Q/4, shard=[demo][4]}]
[23:11:55,258][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [demo][4]
[23:11:55,259][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:55,259][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:55,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:55,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:55,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:55,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:55,261][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:55,261][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:55,261][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] received shard started for [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:55,262][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][3]], allocation id [VdBwviCJT9mJPtYfI5fgWg], primary term [0], message [after existing recovery], shard id [[demo][2]], allocation id [WT_IoIrYTp-g8LeJCVMtzA], primary term [0], message [after existing recovery], shard id [[demo][1]], allocation id [GaYq7DR6SJCJfaKxOZrS9Q], primary term [0], message [after existing recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: SGcGJAOYTmiPPojzGvQTkg)
[23:11:55,262][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:55,263][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][0] starting shard [demo][0], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[existing recovery], s[INITIALIZING], a[id=qIkUCqsWRJWRrN22NrIqZA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[fetching_shard_data]] (shard started task: [shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery]])
[23:11:55,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:55,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:11:55,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:11:55,269][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 1}
[23:11:55,280][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:55,280][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:55,281][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [22ms]
[23:11:55,281][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]]
[23:11:55,281][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [demo][4] received shard started for [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]]
[23:11:55,286][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [after existing recovery], shard id [[demo][0]], allocation id [qIkUCqsWRJWRrN22NrIqZA], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [23ms] done applying updated cluster_state (version: 5, uuid: 6in7WesLQNao5zrF-JuisQ)
[23:11:55,286][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]]]: execute
[23:11:55,286][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [demo][4] starting shard [demo][4], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[existing recovery], s[INITIALIZING], a[id=vKwmcV-nTB2KT4DKGjkE_w], unassigned_info[[reason=CLUSTER_RECOVERED], at[2017-02-05T22:11:54.589Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]])
[23:11:55,288][INFO ][org.elasticsearch.cluster.routing.allocation.AllocationService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[demo][4]] ...]).
[23:11:55,288][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]]]
[23:11:55,288][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:11:55,288][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:11:55,288][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:55,290][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[demo][4]], allocation id [vKwmcV-nTB2KT4DKGjkE_w], primary term [0], message [after existing recovery]]]: took [3ms] done applying updated cluster_state (version: 6, uuid: NJW9fAMGTG6RW6RpauzV9g)
[23:11:55,290][INFO ][test                     ][Test worker] nodes are started
[23:11:55,290][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[23:11:55,291][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[test/iPOA8rApScytmgpRjDYSGw]], shards [5]/[1] - reason [create index]
[23:11:55,291][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:55,826][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:55,832][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[23:11:55,834][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:11:55,834][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test/iPOA8rApScytmgpRjDYSGw] closing index service (reason [cleaning up after validating index on master])
[23:11:55,834][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:55,835][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:11:55,835][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:11:55,835][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test/iPOA8rApScytmgpRjDYSGw] closed... (reason [cleaning up after validating index on master])
[23:11:55,835][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [7], source [create-index [test], cause [api]]
[23:11:55,835][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:11:55,835][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:11:55,835][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[test/iPOA8rApScytmgpRjDYSGw]] creating index
[23:11:55,836][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating Index [[test/iPOA8rApScytmgpRjDYSGw]], shards [5]/[1] - reason [create index]
[23:11:55,836][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:11:56,386][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] using dynamic[true]
[23:11:56,387][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [[test/iPOA8rApScytmgpRjDYSGw]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;notation&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[23:11:56,390][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][1] creating shard
[23:11:56,390][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iPOA8rApScytmgpRjDYSGw/1, shard=[test][1]}]
[23:11:56,390][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:11:56,391][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:56,391][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:56,393][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:56,393][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][2] creating shard
[23:11:56,393][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] starting recovery from store ...
[23:11:56,393][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iPOA8rApScytmgpRjDYSGw/2, shard=[test][2]}]
[23:11:56,393][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:11:56,393][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:56,394][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:56,394][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#1]] wipe translog location - creating new translog
[23:11:56,395][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:56,395][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][3] creating shard
[23:11:56,395][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] starting recovery from store ...
[23:11:56,395][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#1]] no translog ID present in the current generation - creating one
[23:11:56,396][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iPOA8rApScytmgpRjDYSGw/3, shard=[test][3]}]
[23:11:56,396][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:11:56,397][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:56,398][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:56,398][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#3]] wipe translog location - creating new translog
[23:11:56,399][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:56,399][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][0] creating shard
[23:11:56,399][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] starting recovery from store ...
[23:11:56,399][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#3]] no translog ID present in the current generation - creating one
[23:11:56,400][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iPOA8rApScytmgpRjDYSGw/0, shard=[test][0]}]
[23:11:56,400][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:11:56,401][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:56,401][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] recovery completed from [shard_store], took [11ms]
[23:11:56,402][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [test][1] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery]]
[23:11:56,402][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [test][1] received shard started for [shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery]]
[23:11:56,402][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:56,402][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#4]] wipe translog location - creating new translog
[23:11:56,402][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:56,403][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:56,403][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#4]] no translog ID present in the current generation - creating one
[23:11:56,403][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] starting recovery from store ...
[23:11:56,404][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:56,404][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#3]] recovery completed from [shard_store], took [10ms]
[23:11:56,404][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [test][2] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]]
[23:11:56,404][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#3]] [test][2] received shard started for [shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]]
[23:11:56,406][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#2]] wipe translog location - creating new translog
[23:11:56,406][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [1.1s] done applying updated cluster_state (version: 7, uuid: hq41EC2lQxSZ_jLjoRDN9g)
[23:11:56,407][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]]]: execute
[23:11:56,407][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=YfJIkknPSnCtsKGZ5sfBow], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:55.832Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery]])
[23:11:56,407][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=Z26zIDpVQhue_3SO9wrQFw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:55.832Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]])
[23:11:56,407][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#2]] no translog ID present in the current generation - creating one
[23:11:56,408][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:56,408][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#4]] recovery completed from [shard_store], took [12ms]
[23:11:56,408][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [test][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery]]
[23:11:56,408][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#4]] [test][3] received shard started for [shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery]]
[23:11:56,409][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [8], source [shard-started[shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]]]
[23:11:56,410][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:56,410][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [8]
[23:11:56,410][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#2]] recovery completed from [shard_store], took [10ms]
[23:11:56,410][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [test][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery]]
[23:11:56,410][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 8
[23:11:56,410][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#2]] [test][0] received shard started for [shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery]]
[23:11:56,410][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][4] creating shard
[23:11:56,411][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/iPOA8rApScytmgpRjDYSGw/4, shard=[test][4]}]
[23:11:56,411][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:11:56,412][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:11:56,412][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]
[23:11:56,414][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:11:56,414][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] starting recovery from store ...
[23:11:56,414][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:56,415][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:56,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,416][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[7EddQ-C][generic][T#1]] wipe translog location - creating new translog
[23:11:56,417][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [YfJIkknPSnCtsKGZ5sfBow], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [Z26zIDpVQhue_3SO9wrQFw], primary term [0], message [after new shard recovery]]]: took [10ms] done applying updated cluster_state (version: 8, uuid: 8Gqu0wO9SG-aQ5bvP-P_2A)
[23:11:56,417][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[7EddQ-C][generic][T#1]] no translog ID present in the current generation - creating one
[23:11:56,417][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:56,417][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=7VofD2llTFCqtcVwM5Ho_A], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:55.832Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery]])
[23:11:56,417][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=9QMVB7kASQ-DwFewSjNdEg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:55.832Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery]])
[23:11:56,419][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:56,419][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [9]
[23:11:56,419][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 9
[23:11:56,419][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:11:56,419][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:11:56,420][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [test][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery]]
[23:11:56,420][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][generic][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery]]
[23:11:56,420][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [7EddQ-C9THSMygqlWcgOnw] for shard entry [shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,420][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:11:56,420][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:56,420][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:56,422][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [7VofD2llTFCqtcVwM5Ho_A], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [9QMVB7kASQ-DwFewSjNdEg], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 9, uuid: ayuTnPtzQHKc41nWISOZ3w)
[23:11:56,422][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:11:56,422][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[7EddQ-C9THSMygqlWcgOnw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=aL8j2FfiTzONMA1AkTBVJw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:11:55.832Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery]])
[23:11:56,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:11:56,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] publishing cluster state version [10]
[23:11:56,424][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] set local cluster state to version 10
[23:11:56,425][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:11:56,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[7EddQ-C][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [aL8j2FfiTzONMA1AkTBVJw], primary term [0], message [master {7EddQ-C}{7EddQ-C9THSMygqlWcgOnw}{WwUSFufUTTCjHdy6t0oCAQ}{local}{local[3]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 10, uuid: Z2RTEcBXTiu7FIus9nraTg)
[23:12:24,007][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[7EddQ-C][management][T#2]] low disk watermark [85%] exceeded on [7EddQ-C9THSMygqlWcgOnw][7EddQ-C][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 134gb[14.3%], replicas will not be assigned to this node
[23:12:26,547][DEBUG][org.elasticsearch.action.search.TransportSearchAction][elasticsearch[7EddQ-C][search][T#1]] [demo][0], node[7EddQ-C9THSMygqlWcgOnw], [P], s[STARTED], a[id=qIkUCqsWRJWRrN22NrIqZA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={
  &quot;stored_fields&quot; : &quot;notation&quot;,
  &quot;sort&quot; : [
    {
      &quot;notation.sort&quot; : {
        &quot;order&quot; : &quot;asc&quot;
      }
    }
  ],
  &quot;ext&quot; : { }
}}] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [7EddQ-C][local[3]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.query.QueryShardException: No mapping found for [notation.sort] in order to sort on
	at org.elasticsearch.search.sort.FieldSortBuilder.build(FieldSortBuilder.java:262) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.sort.SortBuilder.buildSort(SortBuilder.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:553) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:300) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:297) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
[23:12:26,547][DEBUG][org.elasticsearch.action.search.TransportSearchAction][elasticsearch[7EddQ-C][search][T#3]] [demo][1], node[7EddQ-C9THSMygqlWcgOnw], [P], s[STARTED], a[id=GaYq7DR6SJCJfaKxOZrS9Q]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={
  &quot;stored_fields&quot; : &quot;notation&quot;,
  &quot;sort&quot; : [
    {
      &quot;notation.sort&quot; : {
        &quot;order&quot; : &quot;asc&quot;
      }
    }
  ],
  &quot;ext&quot; : { }
}}] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [7EddQ-C][local[3]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.query.QueryShardException: No mapping found for [notation.sort] in order to sort on
	at org.elasticsearch.search.sort.FieldSortBuilder.build(FieldSortBuilder.java:262) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.sort.SortBuilder.buildSort(SortBuilder.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:553) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:300) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:297) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
[23:12:26,547][DEBUG][org.elasticsearch.action.search.TransportSearchAction][elasticsearch[7EddQ-C][search][T#9]] [demo][4], node[7EddQ-C9THSMygqlWcgOnw], [P], s[STARTED], a[id=vKwmcV-nTB2KT4DKGjkE_w]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={
  &quot;stored_fields&quot; : &quot;notation&quot;,
  &quot;sort&quot; : [
    {
      &quot;notation.sort&quot; : {
        &quot;order&quot; : &quot;asc&quot;
      }
    }
  ],
  &quot;ext&quot; : { }
}}] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [7EddQ-C][local[3]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.query.QueryShardException: No mapping found for [notation.sort] in order to sort on
	at org.elasticsearch.search.sort.FieldSortBuilder.build(FieldSortBuilder.java:262) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.sort.SortBuilder.buildSort(SortBuilder.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:553) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:300) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:297) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
[23:12:26,547][DEBUG][org.elasticsearch.action.search.TransportSearchAction][elasticsearch[7EddQ-C][search][T#5]] [demo][2], node[7EddQ-C9THSMygqlWcgOnw], [P], s[STARTED], a[id=WT_IoIrYTp-g8LeJCVMtzA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={
  &quot;stored_fields&quot; : &quot;notation&quot;,
  &quot;sort&quot; : [
    {
      &quot;notation.sort&quot; : {
        &quot;order&quot; : &quot;asc&quot;
      }
    }
  ],
  &quot;ext&quot; : { }
}}] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [7EddQ-C][local[3]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.query.QueryShardException: No mapping found for [notation.sort] in order to sort on
	at org.elasticsearch.search.sort.FieldSortBuilder.build(FieldSortBuilder.java:262) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.sort.SortBuilder.buildSort(SortBuilder.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:553) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:300) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:297) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
[23:12:26,547][DEBUG][org.elasticsearch.action.search.TransportSearchAction][elasticsearch[7EddQ-C][search][T#7]] [demo][3], node[7EddQ-C9THSMygqlWcgOnw], [P], s[STARTED], a[id=VdBwviCJT9mJPtYfI5fgWg]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, source={
  &quot;stored_fields&quot; : &quot;notation&quot;,
  &quot;sort&quot; : [
    {
      &quot;notation.sort&quot; : {
        &quot;order&quot; : &quot;asc&quot;
      }
    }
  ],
  &quot;ext&quot; : { }
}}] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [7EddQ-C][local[3]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.query.QueryShardException: No mapping found for [notation.sort] in order to sort on
	at org.elasticsearch.search.sort.FieldSortBuilder.build(FieldSortBuilder.java:262) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.sort.SortBuilder.buildSort(SortBuilder.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createContext(SearchService.java:553) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:300) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.action.search.SearchTransportService$6.messageReceived(SearchTransportService.java:297) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
[23:12:26,606][INFO ][test                     ][Test worker] stopping nodes
[23:12:26,606][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:12:26,607][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo] closing ... (reason [shutdown])
[23:12:26,607][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closing index service (reason [shutdown])
[23:12:26,607][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:12:26,607][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test] closing ... (reason [shutdown])
[23:12:26,607][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,607][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,607][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/iPOA8rApScytmgpRjDYSGw] closing index service (reason [shutdown])
[23:12:26,607][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:12:26,608][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[23:12:26,608][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:12:26,608][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,608][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:12:26,608][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,609][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:12:26,609][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:12:26,609][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:12:26,609][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:12:26,609][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,609][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,609][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:12:26,609][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:12:26,610][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:12:26,611][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:12:26,611][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:12:26,611][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:12:26,611][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:12:26,611][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,611][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,611][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:12:26,611][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:12:26,612][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:12:26,613][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:12:26,613][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:12:26,613][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:12:26,613][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:12:26,613][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,613][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,613][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:12:26,614][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:12:26,614][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:12:26,615][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:12:26,615][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:12:26,615][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:12:26,615][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:12:26,616][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,616][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,616][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:12:26,616][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:12:26,616][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:12:26,617][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:12:26,618][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:12:26,618][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:12:26,618][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:12:26,618][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:12:26,618][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:12:26,618][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [demo/Utbg6-yIQTOUEnPXnGcH9Q] closed... (reason [shutdown])
[23:12:26,622][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:12:26,622][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:12:26,622][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:12:26,624][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:12:26,624][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:12:26,624][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[23:12:26,625][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[23:12:26,625][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,625][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,629][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:12:26,629][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:12:26,629][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:12:26,630][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:12:26,631][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:12:26,631][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[23:12:26,631][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[23:12:26,631][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,631][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,638][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:12:26,638][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:12:26,638][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:12:26,639][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:12:26,639][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:12:26,640][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[23:12:26,640][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[23:12:26,640][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,640][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,645][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:12:26,645][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:12:26,646][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:12:26,646][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:12:26,646][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:12:26,646][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[23:12:26,646][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[23:12:26,647][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:12:26,647][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:12:26,653][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:12:26,653][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:12:26,653][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:12:26,654][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:12:26,654][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:12:26,654][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[23:12:26,654][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:12:26,654][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[23:12:26,654][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:12:26,654][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/iPOA8rApScytmgpRjDYSGw] closed... (reason [shutdown])
[23:12:26,654][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:12:26,654][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:12:26,656][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:12:26,678][INFO ][test                     ][Test worker] data files wiped
[23:12:28,679][INFO ][test                     ][Test worker] settings cluster name
[23:12:28,679][INFO ][test                     ][Test worker] starting nodes
[23:12:28,679][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:12:28,680][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:12:28,686][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:12:28,686][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:12:28,686][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:12:28,687][INFO ][org.elasticsearch.node.Node][Test worker] node name [SOT4M-R] derived from node ID [SOT4M-RpSrCUm-o3uXWFvA]; set [node.name] to override
[23:12:28,687][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:12:28,687][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:12:28,687][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:12:28,688][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:12:28,688][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:12:28,688][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:12:28,688][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:12:28,688][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:12:28,688][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:12:28,688][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:12:28,689][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:12:28,689][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:12:28,691][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:12:28,692][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:12:28,692][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:12:28,692][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:12:28,692][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:12:28,692][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:12:28,693][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:12:28,693][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:12:28,693][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:12:28,695][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:12:28,695][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:12:28,695][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:12:28,696][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:12:28,696][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:12:28,696][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:12:28,696][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:12:28,696][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:12:28,696][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:12:28,724][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:12:28,736][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:12:28,882][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:12:28,886][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:12:28,887][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:12:28,887][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:12:28,888][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[4]}, bound_addresses {local[4]}
[23:12:28,888][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:12:28,889][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:12:28,889][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:12:31,894][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[SOT4M-R][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]}], id[28], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:12:31,894][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[SOT4M-R][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:12:31,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:12:31,895][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:12:31,895][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] new_master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:12:31,896][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:12:31,897][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:12:31,898][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:12:31,898][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [3ms] done applying updated cluster_state (version: 1, uuid: 0guwzdm-TNqPh1pPsvZrwg)
[23:12:31,899][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:12:31,899][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:12:31,900][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:12:31,900][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:12:31,902][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:12:31,902][INFO ][test                     ][Test worker] nodes are started
[23:12:31,902][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: D31mq6zZSZaGVsdmK8F6XA)
[23:12:31,902][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[23:12:31,903][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating Index [[test/wB-tRSsgR1a8FnQ3x9kCuA]], shards [5]/[1] - reason [create index]
[23:12:31,903][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:12:32,427][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] using dynamic[true]
[23:12:32,429][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[23:12:32,431][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:12:32,431][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test/wB-tRSsgR1a8FnQ3x9kCuA] closing index service (reason [cleaning up after validating index on master])
[23:12:32,431][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:12:32,431][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:12:32,431][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:12:32,431][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test/wB-tRSsgR1a8FnQ3x9kCuA] closed... (reason [cleaning up after validating index on master])
[23:12:32,431][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [api]]
[23:12:32,432][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:12:32,432][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:12:32,432][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [[test/wB-tRSsgR1a8FnQ3x9kCuA]] creating index
[23:12:32,432][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating Index [[test/wB-tRSsgR1a8FnQ3x9kCuA]], shards [5]/[1] - reason [create index]
[23:12:32,432][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:12:32,972][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] using dynamic[true]
[23:12:32,972][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [[test/wB-tRSsgR1a8FnQ3x9kCuA]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;points&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[23:12:32,974][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][1] creating shard
[23:12:32,975][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/wB-tRSsgR1a8FnQ3x9kCuA/1, shard=[test][1]}]
[23:12:32,975][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:12:32,976][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:12:32,976][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]
[23:12:32,977][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:12:32,977][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][2] creating shard
[23:12:32,977][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] starting recovery from store ...
[23:12:32,977][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/wB-tRSsgR1a8FnQ3x9kCuA/2, shard=[test][2]}]
[23:12:32,977][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:12:32,978][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:12:32,978][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]
[23:12:32,978][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[SOT4M-R][generic][T#3]] wipe translog location - creating new translog
[23:12:32,979][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:12:32,979][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][3] creating shard
[23:12:32,979][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#1]] starting recovery from store ...
[23:12:32,979][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[SOT4M-R][generic][T#3]] no translog ID present in the current generation - creating one
[23:12:32,980][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/wB-tRSsgR1a8FnQ3x9kCuA/3, shard=[test][3]}]
[23:12:32,980][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:12:32,980][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:12:32,981][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]
[23:12:32,982][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:12:32,982][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[SOT4M-R][generic][T#1]] wipe translog location - creating new translog
[23:12:32,982][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][0] creating shard
[23:12:32,982][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#2]] starting recovery from store ...
[23:12:32,983][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/wB-tRSsgR1a8FnQ3x9kCuA/0, shard=[test][0]}]
[23:12:32,983][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:12:32,984][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[SOT4M-R][generic][T#1]] no translog ID present in the current generation - creating one
[23:12:32,984][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:12:32,984][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:12:32,984][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] recovery completed from [shard_store], took [9ms]
[23:12:32,984][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]
[23:12:32,984][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery]]
[23:12:32,985][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery]]
[23:12:32,985][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[SOT4M-R][generic][T#2]] wipe translog location - creating new translog
[23:12:32,985][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:12:32,985][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#4]] starting recovery from store ...
[23:12:32,986][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[SOT4M-R][generic][T#2]] no translog ID present in the current generation - creating one
[23:12:32,986][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:12:32,986][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:12:32,986][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]]
[23:12:32,987][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]]
[23:12:32,987][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [1s] done applying updated cluster_state (version: 3, uuid: J5exQi_ySRGW3tZFefHmZA)
[23:12:32,987][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]]]: execute
[23:12:32,987][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[SOT4M-R][generic][T#4]] wipe translog location - creating new translog
[23:12:32,987][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[SOT4M-RpSrCUm-o3uXWFvA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=S3SiDMrFTgaHEmelLMnWjQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:12:32.430Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery]])
[23:12:32,987][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[SOT4M-RpSrCUm-o3uXWFvA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=EvKwjm9pTZiTt_9yxI_Eug], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:12:32.430Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]])
[23:12:32,988][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:12:32,988][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#2]] recovery completed from [shard_store], took [8ms]
[23:12:32,988][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery]]
[23:12:32,988][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[SOT4M-R][generic][T#4]] no translog ID present in the current generation - creating one
[23:12:32,988][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery]]
[23:12:32,989][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]]]
[23:12:32,989][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:12:32,989][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:12:32,990][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][4] creating shard
[23:12:32,990][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:12:32,990][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#4]] recovery completed from [shard_store], took [8ms]
[23:12:32,991][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery]]
[23:12:32,991][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/wB-tRSsgR1a8FnQ3x9kCuA/4, shard=[test][4]}]
[23:12:32,991][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery]]
[23:12:32,991][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:12:32,991][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:12:32,991][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]
[23:12:32,992][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:12:32,992][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] starting recovery from store ...
[23:12:32,993][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:12:32,993][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:12:32,993][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:12:32,993][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:12:32,993][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:12:32,993][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:12:32,994][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[SOT4M-R][generic][T#3]] wipe translog location - creating new translog
[23:12:32,994][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [S3SiDMrFTgaHEmelLMnWjQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [EvKwjm9pTZiTt_9yxI_Eug], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 4, uuid: xmhR2S8YQtCyZJF4LQkHyw)
[23:12:32,994][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[SOT4M-R][generic][T#3]] no translog ID present in the current generation - creating one
[23:12:32,994][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:12:32,995][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[SOT4M-RpSrCUm-o3uXWFvA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=3CzM9mq2Q3ixi1RDZvwjKg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:12:32.430Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery]])
[23:12:32,995][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[SOT4M-RpSrCUm-o3uXWFvA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=_V6BmdGmS92HhQ6tTFwV9A], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:12:32.430Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery]])
[23:12:32,996][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:12:32,996][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:12:32,996][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:12:32,997][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:12:32,997][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:12:32,998][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:12:32,998][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:12:32,998][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [SOT4M-RpSrCUm-o3uXWFvA] for shard entry [shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]]
[23:12:32,998][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]]
[23:12:33,000][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [3CzM9mq2Q3ixi1RDZvwjKg], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [_V6BmdGmS92HhQ6tTFwV9A], primary term [0], message [master {SOT4M-R}{SOT4M-RpSrCUm-o3uXWFvA}{K4T9RnxeSsOFkCgHk3FbdA}{local}{local[4]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 5, uuid: iBhwWtyxTnuwzQ2KC3xD_Q)
[23:12:33,000][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]]]: execute
[23:12:33,000][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[SOT4M-RpSrCUm-o3uXWFvA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=-p0iUvYnRrmm80DDuFKu2A], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:12:32.430Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]])
[23:12:33,002][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]]]
[23:12:33,002][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:12:33,002][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:12:33,003][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:12:33,005][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[SOT4M-R][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [-p0iUvYnRrmm80DDuFKu2A], primary term [0], message [after new shard recovery]]]: took [4ms] done applying updated cluster_state (version: 6, uuid: Goy6aFhgQrmS6RNNQsQYVQ)
[23:13:01,899][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[SOT4M-R][management][T#2]] low disk watermark [85%] exceeded on [SOT4M-RpSrCUm-o3uXWFvA][SOT4M-R][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 134gb[14.3%], replicas will not be assigned to this node
[23:13:03,044][INFO ][test                     ][Test worker] stopping nodes
[23:13:03,044][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:13:03,045][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[23:13:03,045][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/wB-tRSsgR1a8FnQ3x9kCuA] closing index service (reason [shutdown])
[23:13:03,045][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:13:03,045][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:03,046][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:03,051][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:03,051][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:03,051][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:03,052][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:03,052][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:03,052][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:13:03,052][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:13:03,052][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:03,052][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:03,052][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:03,052][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:03,053][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:03,053][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:03,054][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:03,054][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:13:03,054][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:13:03,054][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:03,054][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:03,054][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:03,054][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:03,054][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:03,055][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:03,055][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:03,055][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:13:03,055][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:13:03,055][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:03,055][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:03,060][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:03,060][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:03,061][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:03,061][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:03,061][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:03,061][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:13:03,061][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:13:03,061][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:03,061][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:03,062][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:03,062][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:03,062][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:03,062][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:03,062][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:03,062][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:13:03,063][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:13:03,063][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:13:03,063][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:13:03,063][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/wB-tRSsgR1a8FnQ3x9kCuA] closed... (reason [shutdown])
[23:13:03,063][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:13:03,063][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:13:03,064][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:13:03,070][INFO ][test                     ][Test worker] data files wiped
[23:13:05,071][INFO ][test                     ][Test worker] settings cluster name
[23:13:05,071][INFO ][test                     ][Test worker] starting nodes
[23:13:05,071][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:13:05,072][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:13:05,075][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:13:05,075][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:13:05,075][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:13:05,075][INFO ][org.elasticsearch.node.Node][Test worker] node name [eKZLgAw] derived from node ID [eKZLgAwOTse4Crke82KNqw]; set [node.name] to override
[23:13:05,075][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:13:05,075][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:13:05,075][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:13:05,075][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:13:05,075][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:13:05,076][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:13:05,077][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:13:05,078][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:13:05,081][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:13:05,081][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:13:05,082][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:13:05,082][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:13:05,082][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:13:05,082][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:13:05,082][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:13:05,082][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:13:05,082][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:13:05,084][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:13:05,084][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:13:05,084][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:13:05,085][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:13:05,085][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:13:05,086][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:13:05,086][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:13:05,086][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:13:05,086][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:13:05,112][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:13:05,120][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:13:05,255][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:13:05,259][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:13:05,260][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:13:05,260][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:13:05,260][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[5]}, bound_addresses {local[5]}
[23:13:05,262][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:13:05,262][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:13:05,263][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:13:08,268][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[eKZLgAw][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]}], id[35], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:13:08,269][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[eKZLgAw][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:13:08,270][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:13:08,272][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:13:08,272][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] new_master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:13:08,272][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:13:08,272][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:13:08,273][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:13:08,273][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [2ms] done applying updated cluster_state (version: 1, uuid: asg9Y-GeSymcCMDkYDG7SQ)
[23:13:08,274][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:13:08,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:13:08,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:13:08,275][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:13:08,277][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:13:08,277][INFO ][test                     ][Test worker] nodes are started
[23:13:08,277][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: ou9d4b6dQJOq_9n75SoZnA)
[23:13:08,278][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: execute
[23:13:08,278][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating Index [[test/7cpE_UHuR3-PAmoXX0z3mw]], shards [5]/[1] - reason [create index]
[23:13:08,278][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:13:08,807][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] using dynamic[true]
[23:13:08,810][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test] creating index, cause [api], templates [], shards [5]/[1], mappings [type1]
[23:13:08,811][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:13:08,811][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test/7cpE_UHuR3-PAmoXX0z3mw] closing index service (reason [cleaning up after validating index on master])
[23:13:08,811][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:13:08,811][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:13:08,811][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:13:08,812][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test/7cpE_UHuR3-PAmoXX0z3mw] closed... (reason [cleaning up after validating index on master])
[23:13:08,812][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [api]]
[23:13:08,812][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:13:08,812][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:13:08,812][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [[test/7cpE_UHuR3-PAmoXX0z3mw]] creating index
[23:13:08,812][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating Index [[test/7cpE_UHuR3-PAmoXX0z3mw]], shards [5]/[1] - reason [create index]
[23:13:08,812][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:13:09,338][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] using dynamic[true]
[23:13:09,338][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [[test/7cpE_UHuR3-PAmoXX0z3mw]] adding mapping [type1], source [{&quot;type1&quot;:{&quot;properties&quot;:{&quot;points&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;store&quot;:true,&quot;fields&quot;:{&quot;sort&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;naturalsort&quot;,&quot;fielddata&quot;:true}}}}}}]
[23:13:09,341][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][1] creating shard
[23:13:09,341][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/7cpE_UHuR3-PAmoXX0z3mw/1, shard=[test][1]}]
[23:13:09,341][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:13:09,343][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:13:09,343][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]
[23:13:09,345][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:13:09,345][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][2] creating shard
[23:13:09,345][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] starting recovery from store ...
[23:13:09,345][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/7cpE_UHuR3-PAmoXX0z3mw/2, shard=[test][2]}]
[23:13:09,345][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:13:09,346][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:13:09,346][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]
[23:13:09,347][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[eKZLgAw][generic][T#3]] wipe translog location - creating new translog
[23:13:09,347][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:13:09,347][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][3] creating shard
[23:13:09,347][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#1]] starting recovery from store ...
[23:13:09,348][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/7cpE_UHuR3-PAmoXX0z3mw/3, shard=[test][3]}]
[23:13:09,348][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:13:09,348][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[eKZLgAw][generic][T#3]] no translog ID present in the current generation - creating one
[23:13:09,349][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:13:09,349][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]
[23:13:09,349][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[eKZLgAw][generic][T#1]] wipe translog location - creating new translog
[23:13:09,350][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:13:09,350][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][0] creating shard
[23:13:09,350][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[eKZLgAw][generic][T#1]] no translog ID present in the current generation - creating one
[23:13:09,351][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#2]] starting recovery from store ...
[23:13:09,351][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/7cpE_UHuR3-PAmoXX0z3mw/0, shard=[test][0]}]
[23:13:09,351][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:13:09,352][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:13:09,353][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] recovery completed from [shard_store], took [11ms]
[23:13:09,353][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery]]
[23:13:09,353][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:13:09,353][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery]]
[23:13:09,353][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[eKZLgAw][generic][T#2]] wipe translog location - creating new translog
[23:13:09,353][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]
[23:13:09,354][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[eKZLgAw][generic][T#2]] no translog ID present in the current generation - creating one
[23:13:09,354][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:13:09,355][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#4]] starting recovery from store ...
[23:13:09,355][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:13:09,355][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#1]] recovery completed from [shard_store], took [10ms]
[23:13:09,355][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]]
[23:13:09,355][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]]
[23:13:09,357][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [create-index [test], cause [api]]: took [1s] done applying updated cluster_state (version: 3, uuid: tHinOskaRXik5E6KxJbMYw)
[23:13:09,357][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[eKZLgAw][generic][T#4]] wipe translog location - creating new translog
[23:13:09,357][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]]]: execute
[23:13:09,358][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[eKZLgAwOTse4Crke82KNqw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=uWKcCtdERFyFHwYxu3WVeg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:13:08.810Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery]])
[23:13:09,358][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[eKZLgAwOTse4Crke82KNqw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=QSNxMEIwSDG_f1-Jg9PfbA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:13:08.810Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]])
[23:13:09,358][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:13:09,358][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#2]] recovery completed from [shard_store], took [10ms]
[23:13:09,358][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery]]
[23:13:09,359][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery]]
[23:13:09,359][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[eKZLgAw][generic][T#4]] no translog ID present in the current generation - creating one
[23:13:09,360][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]]]
[23:13:09,360][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:13:09,360][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:13:09,362][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][4] creating shard
[23:13:09,362][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:13:09,362][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#4]] recovery completed from [shard_store], took [11ms]
[23:13:09,362][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery]]
[23:13:09,362][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery]]
[23:13:09,363][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/7cpE_UHuR3-PAmoXX0z3mw/4, shard=[test][4]}]
[23:13:09,363][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:13:09,364][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:13:09,364][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]
[23:13:09,365][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:13:09,365][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] starting recovery from store ...
[23:13:09,365][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:13:09,366][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:13:09,366][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:13:09,366][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:13:09,366][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:13:09,366][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:13:09,367][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[eKZLgAw][generic][T#3]] wipe translog location - creating new translog
[23:13:09,368][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [uWKcCtdERFyFHwYxu3WVeg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [QSNxMEIwSDG_f1-Jg9PfbA], primary term [0], message [after new shard recovery]]]: took [10ms] done applying updated cluster_state (version: 4, uuid: C8G3LmtARBmOPC6cRyH7Lg)
[23:13:09,368][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:13:09,368][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[eKZLgAw][generic][T#3]] no translog ID present in the current generation - creating one
[23:13:09,369][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[eKZLgAwOTse4Crke82KNqw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=vI2bpfMYSm2pVbCICxK09w], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:13:08.810Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery]])
[23:13:09,369][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[eKZLgAwOTse4Crke82KNqw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=WXsVWv1ES3S0_5ocjkw4Qw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:13:08.810Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery]])
[23:13:09,370][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:13:09,370][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:13:09,370][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:13:09,371][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:13:09,372][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:13:09,372][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][generic][T#3]] recovery completed from [shard_store], took [8ms]
[23:13:09,372][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [eKZLgAwOTse4Crke82KNqw] for shard entry [shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]]
[23:13:09,372][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:13:09,372][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]]
[23:13:09,373][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [vI2bpfMYSm2pVbCICxK09w], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [WXsVWv1ES3S0_5ocjkw4Qw], primary term [0], message [master {eKZLgAw}{eKZLgAwOTse4Crke82KNqw}{4uhyx16bQ-2W31t-8Mmz6A}{local}{local[5]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: ruCG1Y4rR3iND54LfxO32A)
[23:13:09,373][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]]]: execute
[23:13:09,373][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[eKZLgAwOTse4Crke82KNqw], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=awfsnGPkSOyzv8GuwABuZw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:13:08.810Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]])
[23:13:09,374][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]]]
[23:13:09,374][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:13:09,374][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:13:09,375][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:13:09,376][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[eKZLgAw][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [awfsnGPkSOyzv8GuwABuZw], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: KRq2n8tORTGMGL8oi9YxnA)
[23:13:38,274][INFO ][org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor][elasticsearch[eKZLgAw][management][T#2]] low disk watermark [85%] exceeded on [eKZLgAwOTse4Crke82KNqw][eKZLgAw][/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0] free: 134gb[14.3%], replicas will not be assigned to this node
[23:13:39,431][INFO ][test                     ][Test worker] stopping nodes
[23:13:39,431][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:13:39,432][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[23:13:39,432][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/7cpE_UHuR3-PAmoXX0z3mw] closing index service (reason [shutdown])
[23:13:39,432][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:13:39,432][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:39,432][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:39,439][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:39,439][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:39,439][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:39,440][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:39,440][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:39,440][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:13:39,440][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:13:39,440][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:39,440][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:39,445][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:39,445][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:39,445][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:39,446][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:39,446][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:39,446][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:13:39,446][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:13:39,446][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:39,446][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:39,446][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:39,446][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:39,446][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:39,447][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:39,447][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:39,447][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:13:39,447][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:13:39,447][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:39,447][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:39,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:39,456][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:39,456][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:39,457][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:39,457][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:39,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:13:39,457][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:13:39,458][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:13:39,458][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:13:39,458][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:13:39,458][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:13:39,458][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:13:39,459][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:13:39,459][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:13:39,459][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:13:39,459][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:13:39,459][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:13:39,459][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:13:39,459][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/7cpE_UHuR3-PAmoXX0z3mw] closed... (reason [shutdown])
[23:13:39,460][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:13:39,460][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:13:39,461][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:13:39,464][INFO ][test                     ][Test worker] data files wiped
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 05.02.2017 23:15:52</p>
</div>
</div>
</body>
</html>
