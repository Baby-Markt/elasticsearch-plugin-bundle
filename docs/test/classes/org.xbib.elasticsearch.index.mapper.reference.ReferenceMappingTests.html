<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>Class org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/org.xbib.elasticsearch.index.mapper.reference.html">org.xbib.elasticsearch.index.mapper.reference</a> &gt; ReferenceMappingTests</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">4</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">38.068s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testRefFromID</td>
<td>9.131s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefInDoc</td>
<td>9.241s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testRefMappings</td>
<td>9.403s</td>
<td class="success">passed</td>
</tr>
<tr>
<td class="success">testSearch</td>
<td>10.293s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>[23:15:04,389][INFO ][test                     ][Test worker] settings cluster name
[23:15:04,389][INFO ][test                     ][Test worker] starting nodes
[23:15:04,390][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:15:04,390][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:15:04,393][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:15:04,393][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.2gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:15:04,393][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:15:04,393][INFO ][org.elasticsearch.node.Node][Test worker] node name [jxqVFT6] derived from node ID [jxqVFT6UTuGSMEXylQZzuA]; set [node.name] to override
[23:15:04,393][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:15:04,393][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:15:04,394][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:15:04,394][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:15:04,394][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:15:04,394][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:15:04,394][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:15:04,394][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:15:04,394][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:15:04,394][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:15:04,395][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:15:04,396][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:15:04,398][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:15:04,398][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:15:04,398][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:15:04,399][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:15:04,399][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:15:04,399][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:15:04,399][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:15:04,399][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:15:04,399][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:15:04,400][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:15:04,400][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:15:04,400][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:15:04,401][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:15:04,401][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:15:04,401][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:15:04,401][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:15:04,401][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:04,401][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:04,427][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:15:04,432][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:15:04,620][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:15:04,625][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:15:04,626][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:15:04,626][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:15:04,626][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[11]}, bound_addresses {local[11]}
[23:15:04,627][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:15:04,627][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:15:04,628][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:15:07,631][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[jxqVFT6][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]}], id[77], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:15:07,632][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[jxqVFT6][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:15:07,632][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:15:07,632][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:15:07,632][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] new_master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:15:07,632][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:15:07,633][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:15:07,633][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:15:07,633][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: gBE9My2NRliS1nGZMPWClQ)
[23:15:07,634][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:15:07,634][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:15:07,635][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:15:07,635][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:15:07,637][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:15:07,637][INFO ][test                     ][Test worker] nodes are started
[23:15:07,637][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: NJmjP7EiQ7mWgqoLGO8W9g)
[23:15:07,637][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[23:15:07,638][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[23:15:07,638][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating Index [[test/yly7DKtFScyXnF4yV9d89Q]], shards [5]/[1] - reason [create index]
[23:15:07,638][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:08,186][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:08,186][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:08,187][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:15:08,188][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test/yly7DKtFScyXnF4yV9d89Q] closing index service (reason [cleaning up after validating index on master])
[23:15:08,188][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:08,188][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:08,188][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:08,188][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test/yly7DKtFScyXnF4yV9d89Q] closed... (reason [cleaning up after validating index on master])
[23:15:08,188][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[23:15:08,188][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:15:08,188][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:15:08,188][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [[test/yly7DKtFScyXnF4yV9d89Q]] creating index
[23:15:08,189][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating Index [[test/yly7DKtFScyXnF4yV9d89Q]], shards [5]/[1] - reason [create index]
[23:15:08,189][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:08,703][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:08,704][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][1] creating shard
[23:15:08,704][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/yly7DKtFScyXnF4yV9d89Q/1, shard=[test][1]}]
[23:15:08,704][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:15:08,705][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:08,705][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:08,707][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:08,707][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][2] creating shard
[23:15:08,707][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] starting recovery from store ...
[23:15:08,708][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/yly7DKtFScyXnF4yV9d89Q/2, shard=[test][2]}]
[23:15:08,708][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:15:08,709][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:08,709][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:08,709][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#3]] wipe translog location - creating new translog
[23:15:08,710][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:08,710][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][3] creating shard
[23:15:08,710][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] starting recovery from store ...
[23:15:08,710][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:08,711][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/yly7DKtFScyXnF4yV9d89Q/3, shard=[test][3]}]
[23:15:08,711][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:15:08,711][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:08,712][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:08,712][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#1]] wipe translog location - creating new translog
[23:15:08,713][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:08,713][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][0] creating shard
[23:15:08,713][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] starting recovery from store ...
[23:15:08,713][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/yly7DKtFScyXnF4yV9d89Q/0, shard=[test][0]}]
[23:15:08,714][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:15:08,714][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:08,714][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:08,714][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] recovery completed from [shard_store], took [10ms]
[23:15:08,715][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:08,715][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery]]
[23:15:08,715][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:08,715][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery]]
[23:15:08,715][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#2]] wipe translog location - creating new translog
[23:15:08,716][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:08,716][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:15:08,716][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:08,716][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]]
[23:15:08,716][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:08,717][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]]
[23:15:08,717][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] starting recovery from store ...
[23:15:08,718][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 3, uuid: AY96xdaXRxe-UmsStpE1qw)
[23:15:08,718][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]]]: execute
[23:15:08,718][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#4]] wipe translog location - creating new translog
[23:15:08,718][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=8XMPgwA4SH6uWpUljJ9Kww], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:08.186Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery]])
[23:15:08,718][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=YLCxm2cGQoS0df5ySu8P8Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:08.186Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]])
[23:15:08,719][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:08,719][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] recovery completed from [shard_store], took [8ms]
[23:15:08,719][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery]]
[23:15:08,719][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:08,719][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery]]
[23:15:08,720][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]]]
[23:15:08,720][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:15:08,720][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:15:08,721][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][4] creating shard
[23:15:08,721][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:08,721][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] recovery completed from [shard_store], took [7ms]
[23:15:08,721][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery]]
[23:15:08,721][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery]]
[23:15:08,721][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/yly7DKtFScyXnF4yV9d89Q/4, shard=[test][4]}]
[23:15:08,721][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:15:08,722][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:08,722][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:08,723][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:08,723][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] starting recovery from store ...
[23:15:08,724][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:08,724][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:08,725][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:08,725][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:08,725][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:08,725][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:08,726][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#3]] wipe translog location - creating new translog
[23:15:08,727][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [8XMPgwA4SH6uWpUljJ9Kww], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [YLCxm2cGQoS0df5ySu8P8Q], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 4, uuid: uzFrXNIbQCKTRakfKe9I4A)
[23:15:08,727][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:08,727][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=cOaBulUjRAqj3YWHYcQTDg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:08.186Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery]])
[23:15:08,727][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:08,727][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=XfiwQvQUS2C1AoNwvfX2NA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:08.186Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery]])
[23:15:08,729][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:08,729][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:15:08,729][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:15:08,729][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:08,730][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:08,730][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] recovery completed from [shard_store], took [8ms]
[23:15:08,730][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:08,730][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]]
[23:15:08,730][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]]
[23:15:08,731][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [cOaBulUjRAqj3YWHYcQTDg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [XfiwQvQUS2C1AoNwvfX2NA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 5, uuid: pyenq1MXS4e_CbG1nqiyBA)
[23:15:08,731][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]]]: execute
[23:15:08,731][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=DJy96ZjXSsmlUXuRADPoGw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:08.186Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]])
[23:15:08,732][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]]]
[23:15:08,732][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:15:08,732][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:15:08,733][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:08,735][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [DJy96ZjXSsmlUXuRADPoGw], primary term [0], message [after new shard recovery]]]: took [4ms] done applying updated cluster_state (version: 6, uuid: Qt4n32WERuaVbpBXRu6-PQ)
[23:15:08,740][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[23:15:09,255][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:09,259][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [test/yly7DKtFScyXnF4yV9d89Q] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:09,260][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[23:15:09,260][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:15:09,260][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:15:09,260][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [[test/yly7DKtFScyXnF4yV9d89Q]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:09,262][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [522ms] done applying updated cluster_state (version: 7, uuid: JwUhryPYRPKW19KXLS6hyg)
[23:15:09,283][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[23:15:09,283][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[23:15:09,283][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating Index [[authorities/M2hYRM5qSe-OVXqXQIxQ_A]], shards [5]/[1] - reason [create index]
[23:15:09,284][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:09,796][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:09,797][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:09,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[23:15:09,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities/M2hYRM5qSe-OVXqXQIxQ_A] closing index service (reason [cleaning up after validating index on master])
[23:15:09,798][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:09,798][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:09,798][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:09,798][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities/M2hYRM5qSe-OVXqXQIxQ_A] closed... (reason [cleaning up after validating index on master])
[23:15:09,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[23:15:09,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [8]
[23:15:09,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 8
[23:15:09,799][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [[authorities/M2hYRM5qSe-OVXqXQIxQ_A]] creating index
[23:15:09,799][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating Index [[authorities/M2hYRM5qSe-OVXqXQIxQ_A]], shards [5]/[1] - reason [create index]
[23:15:09,799][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:10,400][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:10,401][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][1] creating shard
[23:15:10,401][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/M2hYRM5qSe-OVXqXQIxQ_A/1, shard=[authorities][1]}]
[23:15:10,401][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[23:15:10,402][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:10,402][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:10,403][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:10,403][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][2] creating shard
[23:15:10,403][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] starting recovery from store ...
[23:15:10,404][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/M2hYRM5qSe-OVXqXQIxQ_A/2, shard=[authorities][2]}]
[23:15:10,404][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[23:15:10,404][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:10,404][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:10,404][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#1]] wipe translog location - creating new translog
[23:15:10,405][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:10,405][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][3] creating shard
[23:15:10,405][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] starting recovery from store ...
[23:15:10,405][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/M2hYRM5qSe-OVXqXQIxQ_A/3, shard=[authorities][3]}]
[23:15:10,405][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[23:15:10,405][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:10,406][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:10,406][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:10,406][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#2]] wipe translog location - creating new translog
[23:15:10,407][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:10,407][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][0] creating shard
[23:15:10,407][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] starting recovery from store ...
[23:15:10,407][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:10,407][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/M2hYRM5qSe-OVXqXQIxQ_A/0, shard=[authorities][0]}]
[23:15:10,407][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[23:15:10,408][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:10,409][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:10,409][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] recovery completed from [shard_store], took [7ms]
[23:15:10,409][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery]]
[23:15:10,409][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:10,409][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery]]
[23:15:10,410][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#4]] wipe translog location - creating new translog
[23:15:10,411][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:10,411][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] starting recovery from store ...
[23:15:10,411][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:10,412][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:10,412][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#2]] recovery completed from [shard_store], took [8ms]
[23:15:10,412][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#2]] [authorities][2] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]]
[23:15:10,412][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#2]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]]
[23:15:10,412][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [1.1s] done applying updated cluster_state (version: 8, uuid: uSumMM5-ShyZoljnSARdvQ)
[23:15:10,413][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]]]: execute
[23:15:10,413][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=8qqrVMuxQR-1DwSGQ0M63Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:09.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery]])
[23:15:10,413][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#3]] wipe translog location - creating new translog
[23:15:10,413][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=q5RNV9tnSyK4ECGf3EdBWQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:09.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]])
[23:15:10,414][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:10,415][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:10,415][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#4]] recovery completed from [shard_store], took [9ms]
[23:15:10,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery]]
[23:15:10,415][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery]]
[23:15:10,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]]]
[23:15:10,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [9]
[23:15:10,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 9
[23:15:10,417][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:10,417][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][4] creating shard
[23:15:10,417][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#3]] recovery completed from [shard_store], took [9ms]
[23:15:10,417][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery]]
[23:15:10,417][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery]]
[23:15:10,417][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/M2hYRM5qSe-OVXqXQIxQ_A/4, shard=[authorities][4]}]
[23:15:10,417][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[23:15:10,417][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:10,417][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:10,418][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:10,418][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] starting recovery from store ...
[23:15:10,419][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:10,419][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:10,419][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,419][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,419][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,419][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,420][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[jxqVFT6][generic][T#1]] wipe translog location - creating new translog
[23:15:10,421][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [8qqrVMuxQR-1DwSGQ0M63Q], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [q5RNV9tnSyK4ECGf3EdBWQ], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 9, uuid: P-T-XwlqQYildZhxDJoMiw)
[23:15:10,421][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:10,421][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=j3DdjPuQRfS4O31nCMTldg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:09.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery]])
[23:15:10,421][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ZaQQBQw5Rg2PqkNywFMRaA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:09.797Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery]])
[23:15:10,421][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[jxqVFT6][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:10,423][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:10,423][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [10]
[23:15:10,423][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 10
[23:15:10,423][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:10,423][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][generic][T#1]] recovery completed from [shard_store], took [6ms]
[23:15:10,424][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery]]
[23:15:10,424][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [jxqVFT6UTuGSMEXylQZzuA] for shard entry [shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,424][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery]]
[23:15:10,424][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:10,424][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:10,425][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:10,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [j3DdjPuQRfS4O31nCMTldg], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [ZaQQBQw5Rg2PqkNywFMRaA], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [6ms] done applying updated cluster_state (version: 10, uuid: CftASZZSSSGUw-kY_3vP4w)
[23:15:10,427][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:10,427][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[jxqVFT6UTuGSMEXylQZzuA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=H1rNY-kxQDCPdSwAQMM3jQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:09.797Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery]])
[23:15:10,429][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:10,429][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [11]
[23:15:10,429][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 11
[23:15:10,430][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:10,431][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [H1rNY-kxQDCPdSwAQMM3jQ], primary term [0], message [master {jxqVFT6}{jxqVFT6UTuGSMEXylQZzuA}{Vu-ahLhITMSv5LMYMINx1Q}{local}{local[11]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [4ms] done applying updated cluster_state (version: 11, uuid: U36oT4X1QViHz_DhN_f91A)
[23:15:10,435][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[23:15:11,118][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:11,120][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [authorities/M2hYRM5qSe-OVXqXQIxQ_A] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:11,121][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[23:15:11,121][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] publishing cluster state version [12]
[23:15:11,121][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] set local cluster state to version 12
[23:15:11,121][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] [[authorities/M2hYRM5qSe-OVXqXQIxQ_A]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:11,124][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[jxqVFT6][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [688ms] done applying updated cluster_state (version: 12, uuid: xU1QzjGxTaGPrtHoT5Po0A)
[23:15:11,746][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _source = null
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = someType
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _type = null
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _uid = someType#1
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _version = -1
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings someField = 1234
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = a
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = b
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings ref = c
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _all = 1234
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _source
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _type
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _uid
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _version
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = someField
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = ref
[23:15:11,748][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefMappings _field_names = _all
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _source = null
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = someType
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _type = null
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _uid = someType#1
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _version = -1
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings someField = 1234
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = a
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = b
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings ref = c
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _all = 1234
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _source
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _type
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _uid
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _version
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = someField
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = ref
[23:15:11,750][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] reparse testRefMappings _field_names = _all
[23:15:11,750][INFO ][test                     ][Test worker] stopping nodes
[23:15:11,750][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:15:11,751][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/M2hYRM5qSe-OVXqXQIxQ_A] closing index service (reason [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test] closing ... (reason [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,751][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/yly7DKtFScyXnF4yV9d89Q] closing index service (reason [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,751][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:15:11,751][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:11,752][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:11,752][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,752][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,752][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:11,752][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:11,752][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:11,753][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:11,753][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:11,753][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:11,753][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:11,754][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[23:15:11,754][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:11,754][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[23:15:11,754][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:15:11,754][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:15:11,754][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,754][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:11,754][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:11,754][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:11,754][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:11,755][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:11,755][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[23:15:11,755][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[23:15:11,755][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:11,755][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:15:11,755][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,755][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:11,755][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:11,755][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:11,755][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:11,755][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:11,756][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:11,756][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:11,756][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[23:15:11,756][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[23:15:11,756][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:11,756][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,756][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:11,756][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,756][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:15:11,756][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:15:11,757][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:11,757][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:11,757][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:11,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:11,758][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:11,758][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:15:11,758][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:15:11,758][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,758][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,767][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:11,767][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:11,768][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:11,771][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:11,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:11,772][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:11,772][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:11,772][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:11,772][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:15:11,773][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:11,773][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:15:11,773][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:11,774][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:11,774][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:11,774][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [test/yly7DKtFScyXnF4yV9d89Q] closed... (reason [shutdown])
[23:15:11,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[23:15:11,774][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[23:15:11,775][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:11,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:11,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:11,775][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:11,776][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:11,778][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:11,778][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:11,778][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[23:15:11,778][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:11,778][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[23:15:11,778][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:11,779][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/M2hYRM5qSe-OVXqXQIxQ_A] closed... (reason [shutdown])
[23:15:11,779][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:15:11,779][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:15:11,781][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:15:11,791][INFO ][test                     ][Test worker] data files wiped
[23:15:13,792][INFO ][test                     ][Test worker] settings cluster name
[23:15:13,793][INFO ][test                     ][Test worker] starting nodes
[23:15:13,793][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:15:13,795][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:15:13,799][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:15:13,799][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.3gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:15:13,799][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:15:13,799][INFO ][org.elasticsearch.node.Node][Test worker] node name [Yp67SSS] derived from node ID [Yp67SSSyRLSQtiquLjF6mA]; set [node.name] to override
[23:15:13,799][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:15:13,799][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:15:13,799][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:15:13,799][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:15:13,799][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:15:13,800][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:15:13,801][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:15:13,801][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:15:13,801][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:15:13,801][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:15:13,801][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:15:13,803][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:15:13,803][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:15:13,803][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:15:13,803][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:15:13,803][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:15:13,803][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:15:13,804][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:15:13,804][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:15:13,804][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:15:13,805][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:15:13,805][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:15:13,805][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:15:13,805][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:15:13,806][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:15:13,806][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:15:13,806][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:15:13,806][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:13,806][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:13,832][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:15:13,836][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:15:14,020][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:15:14,024][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:15:14,025][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:15:14,026][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:15:14,026][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[12]}, bound_addresses {local[12]}
[23:15:14,027][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:15:14,028][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:15:14,028][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:15:17,032][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[Yp67SSS][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]}], id[84], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:15:17,033][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[Yp67SSS][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:15:17,033][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:15:17,034][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:15:17,034][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] new_master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:15:17,034][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:15:17,034][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:15:17,035][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:15:17,035][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: cmuZTBnARWye5SS6QPZwyQ)
[23:15:17,036][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:15:17,036][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:15:17,037][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:15:17,037][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:15:17,039][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:15:17,039][INFO ][test                     ][Test worker] nodes are started
[23:15:17,039][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [3ms] done applying updated cluster_state (version: 2, uuid: LZaZ_wLhQWSnFZV5YJGbfA)
[23:15:17,040][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[23:15:17,040][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[23:15:17,041][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating Index [[test/njyjuMGFSXSpGGU4ijkgKQ]], shards [5]/[1] - reason [create index]
[23:15:17,042][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:17,592][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:17,593][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:17,595][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:15:17,596][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test/njyjuMGFSXSpGGU4ijkgKQ] closing index service (reason [cleaning up after validating index on master])
[23:15:17,596][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:17,596][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:17,596][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:17,596][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test/njyjuMGFSXSpGGU4ijkgKQ] closed... (reason [cleaning up after validating index on master])
[23:15:17,596][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[23:15:17,596][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:15:17,596][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:15:17,597][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [[test/njyjuMGFSXSpGGU4ijkgKQ]] creating index
[23:15:17,597][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating Index [[test/njyjuMGFSXSpGGU4ijkgKQ]], shards [5]/[1] - reason [create index]
[23:15:17,597][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:18,120][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:18,120][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][1] creating shard
[23:15:18,120][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/njyjuMGFSXSpGGU4ijkgKQ/1, shard=[test][1]}]
[23:15:18,120][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:15:18,121][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:18,121][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:18,122][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:18,122][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][2] creating shard
[23:15:18,122][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] starting recovery from store ...
[23:15:18,123][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/njyjuMGFSXSpGGU4ijkgKQ/2, shard=[test][2]}]
[23:15:18,123][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:15:18,123][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:18,123][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:18,124][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#3]] wipe translog location - creating new translog
[23:15:18,124][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:18,124][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][3] creating shard
[23:15:18,124][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] starting recovery from store ...
[23:15:18,125][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/njyjuMGFSXSpGGU4ijkgKQ/3, shard=[test][3]}]
[23:15:18,125][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:15:18,126][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:18,126][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:18,127][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#1]] wipe translog location - creating new translog
[23:15:18,127][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:18,128][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:18,128][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:18,128][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][0] creating shard
[23:15:18,128][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] starting recovery from store ...
[23:15:18,129][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/njyjuMGFSXSpGGU4ijkgKQ/0, shard=[test][0]}]
[23:15:18,129][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:15:18,129][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:18,130][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:18,130][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#2]] wipe translog location - creating new translog
[23:15:18,130][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:18,130][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] recovery completed from [shard_store], took [10ms]
[23:15:18,130][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery]]
[23:15:18,131][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery]]
[23:15:18,131][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:18,131][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] starting recovery from store ...
[23:15:18,131][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:18,131][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:18,132][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:15:18,132][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]]
[23:15:18,132][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]]
[23:15:18,133][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#4]] wipe translog location - creating new translog
[23:15:18,133][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 3, uuid: WlLfMZOcTnucsD01206EHw)
[23:15:18,133][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]]]: execute
[23:15:18,133][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ljt2oSwaQU61d9ljBnK1DQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:17.593Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery]])
[23:15:18,134][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ZMJL9TxFRhCL9Yg6EwFaXQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:17.593Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]])
[23:15:18,134][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:18,134][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] recovery completed from [shard_store], took [9ms]
[23:15:18,134][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery]]
[23:15:18,134][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:18,134][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery]]
[23:15:18,135][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]]]
[23:15:18,135][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:15:18,135][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:15:18,136][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][4] creating shard
[23:15:18,136][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:18,136][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] recovery completed from [shard_store], took [7ms]
[23:15:18,136][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery]]
[23:15:18,136][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery]]
[23:15:18,136][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/njyjuMGFSXSpGGU4ijkgKQ/4, shard=[test][4]}]
[23:15:18,136][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:15:18,137][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:18,137][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:18,138][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:18,138][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] starting recovery from store ...
[23:15:18,138][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:18,138][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:18,138][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:18,138][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:18,138][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:18,138][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:18,139][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#3]] wipe translog location - creating new translog
[23:15:18,139][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [ljt2oSwaQU61d9ljBnK1DQ], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ZMJL9TxFRhCL9Yg6EwFaXQ], primary term [0], message [after new shard recovery]]]: took [6ms] done applying updated cluster_state (version: 4, uuid: uUNJqOmASB2YOOhhbbnU2g)
[23:15:18,140][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:18,140][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=VQPw4NopT4qLIVaXPSTOyg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:17.593Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery]])
[23:15:18,140][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=wQ3K0x6bTTicobCelWkO9g], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:17.593Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery]])
[23:15:18,140][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:18,140][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:18,141][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:15:18,141][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:15:18,143][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:18,143][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:18,143][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:15:18,144][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:18,144][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]]
[23:15:18,144][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]]
[23:15:18,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [VQPw4NopT4qLIVaXPSTOyg], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [wQ3K0x6bTTicobCelWkO9g], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 5, uuid: hQzhClYAR-qim2YKo9K3BA)
[23:15:18,146][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]]]: execute
[23:15:18,146][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=5_RhgW4ERRKUecrgYoTfng], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:17.593Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]])
[23:15:18,147][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]]]
[23:15:18,147][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:15:18,147][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:15:18,148][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:18,150][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [5_RhgW4ERRKUecrgYoTfng], primary term [0], message [after new shard recovery]]]: took [3ms] done applying updated cluster_state (version: 6, uuid: sN1E1HirSzOV-6OxZ-UGrQ)
[23:15:18,152][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[23:15:18,694][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:18,698][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [test/njyjuMGFSXSpGGU4ijkgKQ] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:18,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[23:15:18,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:15:18,698][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:15:18,699][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [[test/njyjuMGFSXSpGGU4ijkgKQ]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:18,701][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [549ms] done applying updated cluster_state (version: 7, uuid: XVT-WnOjQ5ynnTK79pdkcw)
[23:15:18,729][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[23:15:18,729][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[23:15:18,729][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating Index [[authorities/NLKcHTmERTehxxHlaUmV1g]], shards [5]/[1] - reason [create index]
[23:15:18,730][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:19,238][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:19,239][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:19,240][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[23:15:19,240][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities/NLKcHTmERTehxxHlaUmV1g] closing index service (reason [cleaning up after validating index on master])
[23:15:19,240][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:19,240][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:19,240][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:19,240][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities/NLKcHTmERTehxxHlaUmV1g] closed... (reason [cleaning up after validating index on master])
[23:15:19,240][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[23:15:19,240][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [8]
[23:15:19,241][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 8
[23:15:19,241][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [[authorities/NLKcHTmERTehxxHlaUmV1g]] creating index
[23:15:19,241][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating Index [[authorities/NLKcHTmERTehxxHlaUmV1g]], shards [5]/[1] - reason [create index]
[23:15:19,241][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:19,756][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:19,756][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][1] creating shard
[23:15:19,756][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/NLKcHTmERTehxxHlaUmV1g/1, shard=[authorities][1]}]
[23:15:19,756][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[23:15:19,757][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:19,757][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:19,759][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:19,759][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][2] creating shard
[23:15:19,759][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] starting recovery from store ...
[23:15:19,760][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/NLKcHTmERTehxxHlaUmV1g/2, shard=[authorities][2]}]
[23:15:19,760][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[23:15:19,760][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:19,760][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:19,761][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#1]] wipe translog location - creating new translog
[23:15:19,762][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:19,762][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][3] creating shard
[23:15:19,762][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] starting recovery from store ...
[23:15:19,762][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/NLKcHTmERTehxxHlaUmV1g/3, shard=[authorities][3]}]
[23:15:19,762][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[23:15:19,762][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:19,762][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:19,763][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:19,763][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#2]] wipe translog location - creating new translog
[23:15:19,763][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:19,763][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][0] creating shard
[23:15:19,763][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] starting recovery from store ...
[23:15:19,764][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/NLKcHTmERTehxxHlaUmV1g/0, shard=[authorities][0]}]
[23:15:19,764][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[23:15:19,764][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:19,764][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:19,764][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:19,764][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:19,764][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] recovery completed from [shard_store], took [8ms]
[23:15:19,764][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery]]
[23:15:19,764][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery]]
[23:15:19,765][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#4]] wipe translog location - creating new translog
[23:15:19,765][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:19,765][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] starting recovery from store ...
[23:15:19,766][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:19,766][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:19,766][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#2]] recovery completed from [shard_store], took [6ms]
[23:15:19,766][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#2]] [authorities][2] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]]
[23:15:19,766][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#2]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]]
[23:15:19,766][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 8, uuid: SdxJY-wpTKeEORP7Ku12tA)
[23:15:19,766][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]]]: execute
[23:15:19,766][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#3]] wipe translog location - creating new translog
[23:15:19,767][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=0qs39C0iSeKSRBsBJRI5KA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:19.239Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery]])
[23:15:19,767][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=8XpaM1UVQf-mSg-cSG_k7Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:19.239Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]])
[23:15:19,767][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:19,767][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:19,768][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#4]] recovery completed from [shard_store], took [5ms]
[23:15:19,768][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery]]
[23:15:19,768][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery]]
[23:15:19,768][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]]]
[23:15:19,768][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [9]
[23:15:19,768][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 9
[23:15:19,769][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][4] creating shard
[23:15:19,769][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:19,769][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:15:19,769][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery]]
[23:15:19,769][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery]]
[23:15:19,770][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/NLKcHTmERTehxxHlaUmV1g/4, shard=[authorities][4]}]
[23:15:19,770][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[23:15:19,770][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:19,770][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:19,771][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:19,771][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] starting recovery from store ...
[23:15:19,772][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:19,773][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:19,773][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,773][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,773][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,773][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,775][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[Yp67SSS][generic][T#1]] wipe translog location - creating new translog
[23:15:19,776][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[Yp67SSS][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:19,777][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [0qs39C0iSeKSRBsBJRI5KA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [8XpaM1UVQf-mSg-cSG_k7Q], primary term [0], message [after new shard recovery]]]: took [11ms] done applying updated cluster_state (version: 9, uuid: awBVMEOHRheiKlSSzCI7BA)
[23:15:19,778][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:19,778][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=-aE3Eem8R0qEZ72ljejmsw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:19.239Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery]])
[23:15:19,778][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=YEY7HX2IQ5quNXPW8bMAcw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:19.239Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery]])
[23:15:19,781][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:19,781][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][generic][T#1]] recovery completed from [shard_store], took [11ms]
[23:15:19,781][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery]]
[23:15:19,781][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery]]
[23:15:19,782][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:19,782][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [10]
[23:15:19,782][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 10
[23:15:19,782][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [Yp67SSSyRLSQtiquLjF6mA] for shard entry [shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,782][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:19,783][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:19,783][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:19,785][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [-aE3Eem8R0qEZ72ljejmsw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [YEY7HX2IQ5quNXPW8bMAcw], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [6ms] done applying updated cluster_state (version: 10, uuid: lskD2e7iQUOavzh3ef-kcA)
[23:15:19,785][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:19,785][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[Yp67SSSyRLSQtiquLjF6mA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=aFuTwN5pQk26P8KZTrBL4Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:19.239Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery]])
[23:15:19,786][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:19,786][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [11]
[23:15:19,786][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 11
[23:15:19,787][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:19,788][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [aFuTwN5pQk26P8KZTrBL4Q], primary term [0], message [master {Yp67SSS}{Yp67SSSyRLSQtiquLjF6mA}{RAlt6cIYRKyXCGHle7p83A}{local}{local[12]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 11, uuid: EORgdOvoS56saf8di7RA0g)
[23:15:19,790][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[23:15:20,318][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:20,321][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [authorities/NLKcHTmERTehxxHlaUmV1g] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:20,321][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[23:15:20,321][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] publishing cluster state version [12]
[23:15:20,321][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] set local cluster state to version 12
[23:15:20,322][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] [[authorities/NLKcHTmERTehxxHlaUmV1g]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:20,324][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[Yp67SSS][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [533ms] done applying updated cluster_state (version: 12, uuid: SRguVplDSdykaFgUbUfj6w)
[23:15:20,888][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[23:15:20,890][INFO ][test                     ][Test worker] stopping nodes
[23:15:20,890][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:15:20,891][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities] closing ... (reason [shutdown])
[23:15:20,891][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/NLKcHTmERTehxxHlaUmV1g] closing index service (reason [shutdown])
[23:15:20,892][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:15:20,892][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test] closing ... (reason [shutdown])
[23:15:20,892][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,892][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,892][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/njyjuMGFSXSpGGU4ijkgKQ] closing index service (reason [shutdown])
[23:15:20,892][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:20,893][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[23:15:20,893][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:20,893][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,893][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,893][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:20,893][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:20,893][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:20,894][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:20,895][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:20,896][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:20,896][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:20,896][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:15:20,896][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[23:15:20,896][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:15:20,896][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[23:15:20,896][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,896][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:20,896][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:20,896][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:20,897][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:20,897][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:20,897][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:20,897][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:15:20,897][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:15:20,897][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:20,897][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:20,897][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,897][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[23:15:20,897][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,897][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[23:15:20,897][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:20,897][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:20,898][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,898][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:20,898][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:20,898][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:20,898][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:15:20,898][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:15:20,898][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,898][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:20,899][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:20,899][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[23:15:20,899][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[23:15:20,899][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,899][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,899][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:20,899][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:20,899][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:20,900][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:20,900][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:20,900][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[23:15:20,900][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[23:15:20,900][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,900][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,906][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:20,906][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:20,907][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:20,907][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:20,907][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:20,908][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:20,908][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:20,908][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:20,908][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[23:15:20,908][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:20,909][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[23:15:20,909][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:20,909][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:20,910][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/njyjuMGFSXSpGGU4ijkgKQ] closed... (reason [shutdown])
[23:15:20,910][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:20,910][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:15:20,910][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:15:20,910][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:20,910][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:20,910][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:20,910][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:20,910][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:20,912][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:20,912][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:20,912][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:15:20,912][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:20,912][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:15:20,912][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:20,912][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/NLKcHTmERTehxxHlaUmV1g] closed... (reason [shutdown])
[23:15:20,912][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:15:20,912][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:15:20,913][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:15:20,922][INFO ][test                     ][Test worker] data files wiped
[23:15:22,923][INFO ][test                     ][Test worker] settings cluster name
[23:15:22,923][INFO ][test                     ][Test worker] starting nodes
[23:15:22,924][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:15:22,924][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:15:22,929][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:15:22,929][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.3gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:15:22,929][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:15:22,930][INFO ][org.elasticsearch.node.Node][Test worker] node name [RHQ81Gq] derived from node ID [RHQ81GqNQEevEKw61tVK9w]; set [node.name] to override
[23:15:22,930][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:15:22,930][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:15:22,930][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:15:22,930][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:15:22,930][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:15:22,931][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:15:22,932][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:15:22,932][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:15:22,932][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:15:22,932][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:15:22,934][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:15:22,934][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:15:22,934][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:15:22,934][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:15:22,934][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:15:22,934][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:15:22,934][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:15:22,934][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:15:22,935][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:15:22,936][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:15:22,936][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:15:22,936][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:15:22,937][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:15:22,937][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:15:22,937][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:15:22,937][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:15:22,937][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:22,937][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:22,963][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:15:22,967][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:15:23,152][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:15:23,156][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:15:23,157][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:15:23,157][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:15:23,157][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[13]}, bound_addresses {local[13]}
[23:15:23,158][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:15:23,158][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:15:23,159][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:15:26,163][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[RHQ81Gq][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]}], id[91], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:15:26,164][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[RHQ81Gq][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:15:26,164][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:15:26,164][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:15:26,164][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] new_master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:15:26,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:15:26,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:15:26,165][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:15:26,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: CnLLtt8vT32s-YzV-8dXKw)
[23:15:26,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:15:26,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:15:26,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:15:26,166][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:15:26,168][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:15:26,168][INFO ][test                     ][Test worker] nodes are started
[23:15:26,168][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [2ms] done applying updated cluster_state (version: 2, uuid: c66eUYk2Tg6PuJ0pbseFMw)
[23:15:26,168][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[23:15:26,169][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[23:15:26,169][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating Index [[test/Yskzje3VRtGCtOPsREpVcA]], shards [5]/[1] - reason [create index]
[23:15:26,170][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:26,693][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:26,693][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:26,694][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:15:26,695][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test/Yskzje3VRtGCtOPsREpVcA] closing index service (reason [cleaning up after validating index on master])
[23:15:26,695][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:26,695][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:26,695][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:26,695][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test/Yskzje3VRtGCtOPsREpVcA] closed... (reason [cleaning up after validating index on master])
[23:15:26,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[23:15:26,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:15:26,695][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:15:26,695][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [[test/Yskzje3VRtGCtOPsREpVcA]] creating index
[23:15:26,695][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating Index [[test/Yskzje3VRtGCtOPsREpVcA]], shards [5]/[1] - reason [create index]
[23:15:26,696][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:27,244][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:27,244][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][1] creating shard
[23:15:27,245][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Yskzje3VRtGCtOPsREpVcA/1, shard=[test][1]}]
[23:15:27,245][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:15:27,246][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:27,246][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:27,247][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:27,247][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][2] creating shard
[23:15:27,247][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] starting recovery from store ...
[23:15:27,247][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Yskzje3VRtGCtOPsREpVcA/2, shard=[test][2]}]
[23:15:27,247][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:15:27,248][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:27,248][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:27,248][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#3]] wipe translog location - creating new translog
[23:15:27,249][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:27,249][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][3] creating shard
[23:15:27,249][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] starting recovery from store ...
[23:15:27,249][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Yskzje3VRtGCtOPsREpVcA/3, shard=[test][3]}]
[23:15:27,249][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:15:27,249][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:27,250][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:27,250][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:27,250][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#1]] wipe translog location - creating new translog
[23:15:27,250][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:27,250][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][0] creating shard
[23:15:27,250][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] starting recovery from store ...
[23:15:27,250][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Yskzje3VRtGCtOPsREpVcA/0, shard=[test][0]}]
[23:15:27,251][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:15:27,251][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:27,251][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:27,251][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:27,252][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#2]] wipe translog location - creating new translog
[23:15:27,252][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:27,252][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] recovery completed from [shard_store], took [7ms]
[23:15:27,252][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:27,252][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery]]
[23:15:27,252][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery]]
[23:15:27,252][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] starting recovery from store ...
[23:15:27,253][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:27,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:27,253][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] recovery completed from [shard_store], took [5ms]
[23:15:27,253][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]]
[23:15:27,253][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]]
[23:15:27,253][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 3, uuid: QABZf2hESI6tU1r5sWlwcA)
[23:15:27,254][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]]]: execute
[23:15:27,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=SX7Y9ak8RIqx3At-vpmq2Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:26.694Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery]])
[23:15:27,254][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#4]] wipe translog location - creating new translog
[23:15:27,254][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=LgG5xjN4Thm8S7dMzER7xw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:26.694Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]])
[23:15:27,255][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:27,256][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]]]
[23:15:27,256][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:15:27,256][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:27,256][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] recovery completed from [shard_store], took [7ms]
[23:15:27,256][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery]]
[23:15:27,256][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:15:27,256][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery]]
[23:15:27,257][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][4] creating shard
[23:15:27,257][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:27,258][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] recovery completed from [shard_store], took [7ms]
[23:15:27,258][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery]]
[23:15:27,258][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/Yskzje3VRtGCtOPsREpVcA/4, shard=[test][4]}]
[23:15:27,258][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery]]
[23:15:27,258][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:15:27,259][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:27,259][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:27,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:27,260][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] starting recovery from store ...
[23:15:27,261][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:27,261][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:27,261][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,261][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,262][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,262][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,262][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#3]] wipe translog location - creating new translog
[23:15:27,263][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [SX7Y9ak8RIqx3At-vpmq2Q], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [LgG5xjN4Thm8S7dMzER7xw], primary term [0], message [after new shard recovery]]]: took [9ms] done applying updated cluster_state (version: 4, uuid: kpv852YeSqWHV5pjzjRn_w)
[23:15:27,263][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:27,263][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=OD64U5tsStu_oW3pYRo72g], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:26.694Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery]])
[23:15:27,263][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ZoXfzcINRlu35UBTaeP1Lg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:26.694Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery]])
[23:15:27,263][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:27,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:27,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:15:27,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:15:27,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:27,265][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] recovery completed from [shard_store], took [7ms]
[23:15:27,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery]]
[23:15:27,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][4] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery]]
[23:15:27,265][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][4] received shard started for [shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:27,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:27,266][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:27,267][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [OD64U5tsStu_oW3pYRo72g], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [ZoXfzcINRlu35UBTaeP1Lg], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 5, uuid: yFAmCwn3TBaaES7sJJDHkg)
[23:15:27,267][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:27,267][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=_HLwfQG5TnijBbOLSnZrxQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:26.694Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery]])
[23:15:27,268][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:27,268][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:15:27,268][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:15:27,268][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:27,269][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [after new shard recovery], shard id [[test][4]], allocation id [_HLwfQG5TnijBbOLSnZrxQ], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [1ms] done applying updated cluster_state (version: 6, uuid: LKnpTFL2RiybLdPobxagXA)
[23:15:27,270][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[23:15:27,796][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:27,798][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [test/Yskzje3VRtGCtOPsREpVcA] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:27,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[23:15:27,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:15:27,799][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:15:27,799][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [[test/Yskzje3VRtGCtOPsREpVcA]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:27,802][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [531ms] done applying updated cluster_state (version: 7, uuid: tUojtHtcSBifR8RxaVQqcA)
[23:15:27,840][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[23:15:27,840][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[23:15:27,841][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating Index [[authorities/5Rzx4Ay6R7uZaDGgcTr-Dw]], shards [5]/[1] - reason [create index]
[23:15:27,842][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:28,416][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:28,416][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:28,417][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[23:15:28,417][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities/5Rzx4Ay6R7uZaDGgcTr-Dw] closing index service (reason [cleaning up after validating index on master])
[23:15:28,417][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:28,417][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:28,418][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:28,418][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities/5Rzx4Ay6R7uZaDGgcTr-Dw] closed... (reason [cleaning up after validating index on master])
[23:15:28,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[23:15:28,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [8]
[23:15:28,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 8
[23:15:28,418][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [[authorities/5Rzx4Ay6R7uZaDGgcTr-Dw]] creating index
[23:15:28,418][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating Index [[authorities/5Rzx4Ay6R7uZaDGgcTr-Dw]], shards [5]/[1] - reason [create index]
[23:15:28,418][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:28,956][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:28,957][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][1] creating shard
[23:15:28,957][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/5Rzx4Ay6R7uZaDGgcTr-Dw/1, shard=[authorities][1]}]
[23:15:28,957][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[23:15:28,959][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:28,959][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:28,961][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:28,961][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][2] creating shard
[23:15:28,961][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] starting recovery from store ...
[23:15:28,962][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/5Rzx4Ay6R7uZaDGgcTr-Dw/2, shard=[authorities][2]}]
[23:15:28,962][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[23:15:28,963][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:28,963][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:28,964][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#1]] wipe translog location - creating new translog
[23:15:28,964][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:28,965][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][3] creating shard
[23:15:28,965][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] starting recovery from store ...
[23:15:28,965][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/5Rzx4Ay6R7uZaDGgcTr-Dw/3, shard=[authorities][3]}]
[23:15:28,965][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[23:15:28,965][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:28,965][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:28,965][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:28,966][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#2]] wipe translog location - creating new translog
[23:15:28,966][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:28,966][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][0] creating shard
[23:15:28,966][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] starting recovery from store ...
[23:15:28,966][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/5Rzx4Ay6R7uZaDGgcTr-Dw/0, shard=[authorities][0]}]
[23:15:28,966][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[23:15:28,966][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:28,967][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:28,967][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:28,967][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:28,967][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] recovery completed from [shard_store], took [10ms]
[23:15:28,967][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery]]
[23:15:28,967][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery]]
[23:15:28,967][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#4]] wipe translog location - creating new translog
[23:15:28,968][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:28,968][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] starting recovery from store ...
[23:15:28,968][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:28,969][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:28,969][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#2]] recovery completed from [shard_store], took [7ms]
[23:15:28,969][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#2]] [authorities][2] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]]
[23:15:28,969][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#2]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]]
[23:15:28,969][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#3]] wipe translog location - creating new translog
[23:15:28,969][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [1.1s] done applying updated cluster_state (version: 8, uuid: -sNv0NZ3RX273Lhqmpp5xw)
[23:15:28,969][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]]]: execute
[23:15:28,969][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=VU0ooQPnTKa8mjS0StV-zA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:28.416Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery]])
[23:15:28,969][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=DlTfGGFYRMGOtXzqFFh7pA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:28.416Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]])
[23:15:28,970][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:28,970][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:28,970][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#4]] recovery completed from [shard_store], took [5ms]
[23:15:28,970][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery]]
[23:15:28,970][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery]]
[23:15:28,971][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]]]
[23:15:28,971][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [9]
[23:15:28,971][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 9
[23:15:28,972][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:28,972][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][4] creating shard
[23:15:28,972][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#3]] recovery completed from [shard_store], took [5ms]
[23:15:28,972][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery]]
[23:15:28,972][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery]]
[23:15:28,972][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/5Rzx4Ay6R7uZaDGgcTr-Dw/4, shard=[authorities][4]}]
[23:15:28,972][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[23:15:28,973][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:28,973][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:28,974][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:28,974][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] starting recovery from store ...
[23:15:28,974][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:28,975][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:28,975][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:28,975][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:28,975][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:28,975][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:28,976][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[RHQ81Gq][generic][T#1]] wipe translog location - creating new translog
[23:15:28,977][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [VU0ooQPnTKa8mjS0StV-zA], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [DlTfGGFYRMGOtXzqFFh7pA], primary term [0], message [after new shard recovery]]]: took [7ms] done applying updated cluster_state (version: 9, uuid: 011W4WZ4QaW7iH97T9rllQ)
[23:15:28,977][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:28,977][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[RHQ81Gq][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:28,977][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=OyJNB5kaSJ-di2eiTkyh5Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:28.416Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery]])
[23:15:28,978][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=iQXm5KnWRXKRB8Nan229ng], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:28.416Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery]])
[23:15:28,979][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:28,979][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [10]
[23:15:28,979][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 10
[23:15:28,980][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:28,980][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:28,980][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][generic][T#1]] recovery completed from [shard_store], took [7ms]
[23:15:28,980][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:28,980][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [RHQ81GqNQEevEKw61tVK9w] for shard entry [shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]]
[23:15:28,980][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]]
[23:15:28,981][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [OyJNB5kaSJ-di2eiTkyh5Q], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [iQXm5KnWRXKRB8Nan229ng], primary term [0], message [master {RHQ81Gq}{RHQ81GqNQEevEKw61tVK9w}{iiIrqa5XT46wxyB2LVF_GQ}{local}{local[13]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 10, uuid: a7BDgi5iS-OGAE8itunCRg)
[23:15:28,981][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]]]: execute
[23:15:28,981][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[RHQ81GqNQEevEKw61tVK9w], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=NoXWc0foR_GOJtZ5K5SCag], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:28.416Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]])
[23:15:28,983][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]]]
[23:15:28,983][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [11]
[23:15:28,983][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 11
[23:15:28,983][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:28,984][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [NoXWc0foR_GOJtZ5K5SCag], primary term [0], message [after new shard recovery]]]: took [3ms] done applying updated cluster_state (version: 11, uuid: iyfT8rZ-TmqA9lAdfz41xA)
[23:15:28,986][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[23:15:29,553][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:29,554][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [authorities/5Rzx4Ay6R7uZaDGgcTr-Dw] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:29,555][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[23:15:29,555][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] publishing cluster state version [12]
[23:15:29,555][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] set local cluster state to version 12
[23:15:29,555][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] [[authorities/5Rzx4Ay6R7uZaDGgcTr-Dw]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:29,558][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[RHQ81Gq][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [571ms] done applying updated cluster_state (version: 12, uuid: Tom_mPieRXS715zoxq5HVA)
[23:15:30,132][DEBUG][org.elasticsearch.index.mapper.MapperService][Test worker] using dynamic[true]
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _source = null
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = docs
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _type = null
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _uid = docs#1
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _version = -1
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title = A title
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc title.keyword = null
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = A creator
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = A contributor
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc authorID = 1
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc dc.creator = John Doe
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc bib.contributor = John Doe
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A title
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A creator
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = A contributor
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _all = 1
[23:15:30,134][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _source
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _type
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _uid
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _version
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = title.keyword
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = authorID
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = dc.creator
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = bib.contributor
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[23:15:30,135][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] testRefInDoc _field_names = _all
[23:15:30,135][INFO ][test                     ][Test worker] stopping nodes
[23:15:30,135][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:15:30,136][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test] closing ... (reason [shutdown])
[23:15:30,136][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/Yskzje3VRtGCtOPsREpVcA] closing index service (reason [shutdown])
[23:15:30,136][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[23:15:30,136][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities] closing ... (reason [shutdown])
[23:15:30,137][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,137][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/5Rzx4Ay6R7uZaDGgcTr-Dw] closing index service (reason [shutdown])
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:30,137][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:30,137][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,137][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:30,137][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:30,138][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:30,138][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:30,138][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:30,139][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:30,139][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[23:15:30,139][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:30,139][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[23:15:30,139][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:15:30,139][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:15:30,139][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:30,139][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:30,139][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:30,139][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:30,139][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:30,140][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:30,140][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:30,140][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[23:15:30,140][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[23:15:30,140][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:30,140][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:30,140][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,140][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:15:30,140][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,141][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:15:30,141][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:30,141][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:30,141][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,141][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,141][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:30,141][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:30,141][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:30,141][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:30,142][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:30,142][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:30,142][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:30,142][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[23:15:30,142][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[23:15:30,142][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:30,142][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:15:30,142][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,142][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:15:30,143][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,143][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:30,143][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:30,143][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,143][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,143][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:30,144][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:30,145][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:30,145][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[23:15:30,145][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[23:15:30,145][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,145][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,151][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:30,151][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:30,151][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:30,152][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:30,152][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:30,152][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:30,152][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[23:15:30,152][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:30,152][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:30,152][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[23:15:30,152][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:30,152][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:30,153][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [test/Yskzje3VRtGCtOPsREpVcA] closed... (reason [shutdown])
[23:15:30,153][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:30,153][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:30,153][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:15:30,153][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:15:30,153][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:30,153][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:30,153][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:30,153][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:30,153][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:30,154][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:30,154][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:30,154][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:15:30,154][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:30,154][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:15:30,154][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:30,154][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [authorities/5Rzx4Ay6R7uZaDGgcTr-Dw] closed... (reason [shutdown])
[23:15:30,154][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:15:30,154][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:15:30,156][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:15:30,163][INFO ][test                     ][Test worker] data files wiped
[23:15:32,164][INFO ][test                     ][Test worker] settings cluster name
[23:15:32,164][INFO ][test                     ][Test worker] starting nodes
[23:15:32,164][INFO ][test                     ][Test worker] settings={cluster.name=test-helper-cluster--joerg-1, http.enabled=false, path.home=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle, transport.type=local}
[23:15:32,165][INFO ][org.elasticsearch.node.Node][Test worker] initializing ...
[23:15:32,169][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] using node location [[NodePath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, spins=null}]], local_lock_id [0]
[23:15:32,169][DEBUG][org.elasticsearch.env.NodeEnvironment][Test worker] node data locations details:
 -&gt; /Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0, free_space [134.3gb], usable_space [134gb], total_space [931gb], spins? [unknown], mount [/ (/dev/disk0s2)], type [hfs]
[23:15:32,169][INFO ][org.elasticsearch.env.NodeEnvironment][Test worker] heap size [3.5gb], compressed ordinary object pointers [true]
[23:15:32,169][INFO ][org.elasticsearch.node.Node][Test worker] node name [G5gTcED] derived from node ID [G5gTcEDXTe6vUhEct-imCA]; set [node.name] to override
[23:15:32,169][INFO ][org.elasticsearch.node.Node][Test worker] version[5.1.1], pid[37396], build[5395e21/2016-12-06T12:36:15.409Z], OS[Mac OS X/10.9.5/x86_64], JVM[Azul Systems, Inc./OpenJDK 64-Bit Server VM/1.8.0_112/25.112-b16]
[23:15:32,169][DEBUG][org.elasticsearch.node.Node][Test worker] using config [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/config], data [[/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data]], logs [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/logs], plugins [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins]
[23:15:32,169][DEBUG][org.elasticsearch.plugins.PluginsService][Test worker] [/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/plugins] directory does not exist.
[23:15:32,169][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] no modules loaded
[23:15:32,170][INFO ][org.elasticsearch.plugins.PluginsService][Test worker] loaded plugin [org.xbib.elasticsearch.plugin.bundle.BundlePlugin]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [force_merge], size [1], queue size [unbounded]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_started], core [1], max [16], keep alive [5m]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [listener], size [4], queue size [unbounded]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [index], size [8], queue size [200]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [refresh], core [1], max [4], keep alive [5m]
[23:15:32,170][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [generic], core [4], max [128], keep alive [30s]
[23:15:32,171][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [warmer], core [1], max [4], keep alive [5m]
[23:15:32,171][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [search], size [13], queue size [1k]
[23:15:32,171][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [flush], core [1], max [4], keep alive [5m]
[23:15:32,171][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [fetch_shard_store], core [1], max [16], keep alive [5m]
[23:15:32,171][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [management], core [1], max [5], keep alive [5m]
[23:15:32,172][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [get], size [8], queue size [1k]
[23:15:32,172][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [bulk], size [8], queue size [50]
[23:15:32,172][DEBUG][org.elasticsearch.threadpool.ThreadPool][Test worker] created thread pool: name [snapshot], core [1], max [4], keep alive [5m]
[23:15:32,173][DEBUG][org.elasticsearch.script.ScriptService][Test worker] using script cache with max_size [100], expire [0s]
[23:15:32,175][DEBUG][org.elasticsearch.common.network.IfConfig][Test worker] configuration:

lo0
        inet 127.0.0.1 netmask:255.0.0.0 scope:host
        inet6 fe80::1 prefixlen:64 scope:link
        inet6 ::1 prefixlen:128 scope:host
        UP MULTICAST LOOPBACK mtu:16384 index:1

en0
        inet 192.168.178.23 netmask:255.255.255.0 broadcast:192.168.178.255 scope:site
        inet6 2001:4dd0:310b:1:89c4:7756:10e4:fce7 prefixlen:64
        inet6 2001:4dd0:310b:1:7a31:c1ff:fed6:f350 prefixlen:64
        inet6 fe80::7a31:c1ff:fed6:f350 prefixlen:64 scope:link
        hardware 78:31:C1:D6:F3:50
        UP MULTICAST mtu:1500 index:4

[23:15:32,176][DEBUG][org.elasticsearch.monitor.jvm.JvmGcMonitorService][Test worker] enabled [true], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}], overhead [50, 25, 10]
[23:15:32,177][DEBUG][org.elasticsearch.monitor.os.OsService][Test worker] using refresh_interval [1s]
[23:15:32,177][DEBUG][org.elasticsearch.monitor.process.ProcessService][Test worker] using refresh_interval [1s]
[23:15:32,177][DEBUG][org.elasticsearch.monitor.jvm.JvmService][Test worker] using refresh_interval [1s]
[23:15:32,178][DEBUG][org.elasticsearch.monitor.fs.FsService][Test worker] using refresh_interval [1s]
[23:15:32,178][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider][Test worker] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[23:15:32,178][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider][Test worker] using [cluster_concurrent_rebalance] with [2]
[23:15:32,179][DEBUG][org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider][Test worker] using node_concurrent_outgoing_recoveries [2], node_concurrent_incoming_recoveries [2], node_initial_primaries_recoveries [4]
[23:15:32,180][DEBUG][org.elasticsearch.index.store.IndexStoreConfig][Test worker] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [0b]
[23:15:32,180][DEBUG][org.elasticsearch.indices.IndicesQueryCache][Test worker] using [node] query cache with size [364mb] max filter count [10000]
[23:15:32,180][DEBUG][org.elasticsearch.indices.IndexingMemoryController][Test worker] using indexing buffer size [364mb] with indices.memory.shard_inactive_time [5m], indices.memory.interval [5s]
[23:15:32,181][DEBUG][org.elasticsearch.transport.local.LocalTransport][Test worker] creating [8] workers, queue_size [-1]
[23:15:32,181][DEBUG][org.elasticsearch.discovery.zen.UnicastZenPing][Test worker] using initial hosts [0.0.0.0], with concurrent_connects [10], resolve_timeout [5s]
[23:15:32,181][DEBUG][org.elasticsearch.discovery.zen.ElectMasterService][Test worker] using minimum_master_nodes [-1]
[23:15:32,181][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][Test worker] using ping_timeout [3s], join.timeout [1m], master_election.ignore_non_master [false]
[23:15:32,181][DEBUG][org.elasticsearch.discovery.zen.MasterFaultDetection][Test worker] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:32,181][DEBUG][org.elasticsearch.discovery.zen.NodesFaultDetection][Test worker] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[23:15:32,206][DEBUG][org.elasticsearch.indices.recovery.RecoverySettings][Test worker] using max_bytes_per_sec[40mb]
[23:15:32,212][DEBUG][org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator][Test worker] using initial_shards [quorum]
[23:15:32,403][DEBUG][org.xbib.elasticsearch.common.langdetect.LangdetectService][Test worker] language detection service installed for [ar, bg, bn, cs, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, ko, lt, lv, mk, ml, nl, no, pa, pl, pt, ro, ru, sq, sv, ta, te, th, tl, tr, uk, ur, vi, zh-cn, zh-tw]
[23:15:32,406][DEBUG][org.elasticsearch.gateway.GatewayMetaState][Test worker] took 0s to load state
[23:15:32,407][INFO ][org.elasticsearch.node.Node][Test worker] initialized
[23:15:32,407][INFO ][org.elasticsearch.node.Node][Test worker] starting ...
[23:15:32,408][INFO ][org.elasticsearch.transport.TransportService][Test worker] publish_address {local[14]}, bound_addresses {local[14]}
[23:15:32,408][DEBUG][org.elasticsearch.node.Node][Test worker] waiting to join the cluster. timeout [30s]
[23:15:32,409][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [initial_join]: execute
[23:15:32,409][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [initial_join]: took [0s] no change in cluster_state
[23:15:35,415][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[G5gTcED][generic][T#1]] filtered ping responses: (ignore_non_masters [false])
	--&gt; ping_response{node [{G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]}], id[98], master [null],cluster_state_version [-1], cluster_name[test-helper-cluster--joerg-1]}
[23:15:35,416][DEBUG][org.elasticsearch.discovery.zen.ZenDiscovery][elasticsearch[G5gTcED][generic][T#1]] elected as master, waiting for incoming joins ([0] needed)
[23:15:35,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: execute
[23:15:35,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [1], source [zen-disco-elected-as-master ([0] nodes joined)]
[23:15:35,416][INFO ][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] new_master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]}, reason: zen-disco-elected-as-master ([0] nodes joined)
[23:15:35,416][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [1]
[23:15:35,417][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 1
[23:15:35,417][INFO ][org.elasticsearch.node.Node][Test worker] started
[23:15:35,417][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [zen-disco-elected-as-master ([0] nodes joined)]: took [1ms] done applying updated cluster_state (version: 1, uuid: Sjn2KhWTQbGi5sT_pcbKaw)
[23:15:35,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: execute
[23:15:35,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [2], source [local-gateway-elected-state]
[23:15:35,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [2]
[23:15:35,418][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 2
[23:15:35,420][INFO ][org.elasticsearch.gateway.GatewayService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] recovered [0] indices into cluster_state
[23:15:35,420][INFO ][test                     ][Test worker] nodes are started
[23:15:35,420][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [local-gateway-elected-state]: took [1ms] done applying updated cluster_state (version: 2, uuid: QAdqy1uDRP2AruE7iP77fA)
[23:15:35,420][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'test' index
[23:15:35,420][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: execute
[23:15:35,421][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[test/plGH3VpxT96u-_mymAmCiA]], shards [5]/[1] - reason [create index]
[23:15:35,421][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:35,940][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:35,941][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:35,942][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test] closing ... (reason [cleaning up after validating index on master])
[23:15:35,942][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test/plGH3VpxT96u-_mymAmCiA] closing index service (reason [cleaning up after validating index on master])
[23:15:35,942][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:35,942][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:35,942][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:35,942][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test/plGH3VpxT96u-_mymAmCiA] closed... (reason [cleaning up after validating index on master])
[23:15:35,942][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [3], source [create-index [test], cause [auto(index api)]]
[23:15:35,942][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [3]
[23:15:35,942][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 3
[23:15:35,943][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[test/plGH3VpxT96u-_mymAmCiA]] creating index
[23:15:35,943][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[test/plGH3VpxT96u-_mymAmCiA]], shards [5]/[1] - reason [create index]
[23:15:35,944][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:36,480][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:36,480][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][1] creating shard
[23:15:36,480][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/plGH3VpxT96u-_mymAmCiA/1, shard=[test][1]}]
[23:15:36,480][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [test][1]
[23:15:36,481][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:36,481][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:36,482][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:36,482][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][2] creating shard
[23:15:36,482][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] starting recovery from store ...
[23:15:36,482][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/plGH3VpxT96u-_mymAmCiA/2, shard=[test][2]}]
[23:15:36,483][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [test][2]
[23:15:36,483][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:36,483][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:36,484][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#3]] wipe translog location - creating new translog
[23:15:36,484][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:36,484][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][3] creating shard
[23:15:36,484][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] starting recovery from store ...
[23:15:36,484][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/plGH3VpxT96u-_mymAmCiA/3, shard=[test][3]}]
[23:15:36,484][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [test][3]
[23:15:36,485][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:36,485][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:36,485][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:36,485][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#1]] wipe translog location - creating new translog
[23:15:36,485][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:36,485][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][0] creating shard
[23:15:36,485][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] starting recovery from store ...
[23:15:36,486][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/plGH3VpxT96u-_mymAmCiA/0, shard=[test][0]}]
[23:15:36,486][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [test][0]
[23:15:36,486][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:36,486][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:36,486][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:36,487][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#2]] wipe translog location - creating new translog
[23:15:36,487][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:36,487][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] starting recovery from store ...
[23:15:36,488][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:36,488][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:36,488][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] recovery completed from [shard_store], took [7ms]
[23:15:36,488][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [test][1] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery]]
[23:15:36,488][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [test][1] received shard started for [shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery]]
[23:15:36,489][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:36,489][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] recovery completed from [shard_store], took [6ms]
[23:15:36,489][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [test][2] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]]
[23:15:36,489][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [test][2] received shard started for [shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]]
[23:15:36,489][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [test], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 3, uuid: rASz4HlvTouRED8QNsmBqg)
[23:15:36,489][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#4]] wipe translog location - creating new translog
[23:15:36,490][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]]]: execute
[23:15:36,490][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][1] starting shard [test][1], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=y50oEDAMQLqn_ZWaDOfDcg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:35.941Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery]])
[23:15:36,490][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][2] starting shard [test][2], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=ObfPCzI_TYuveJTZFEbeeA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:35.941Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]])
[23:15:36,491][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:36,491][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:36,491][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] recovery completed from [shard_store], took [7ms]
[23:15:36,491][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [test][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery]]
[23:15:36,491][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [test][3] received shard started for [shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery]]
[23:15:36,492][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [4], source [shard-started[shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]]]
[23:15:36,492][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [4]
[23:15:36,492][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 4
[23:15:36,493][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][4] creating shard
[23:15:36,494][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:36,494][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] recovery completed from [shard_store], took [8ms]
[23:15:36,494][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [test][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery]]
[23:15:36,494][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [test][0] received shard started for [shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery]]
[23:15:36,494][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/plGH3VpxT96u-_mymAmCiA/4, shard=[test][4]}]
[23:15:36,494][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [test][4]
[23:15:36,495][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:36,495][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:36,496][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:36,496][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] starting recovery from store ...
[23:15:36,496][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:36,497][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:36,497][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:36,497][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][3] received shard started for [shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:36,497][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:36,497][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][0] received shard started for [shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:36,497][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#3]] wipe translog location - creating new translog
[23:15:36,498][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][1]], allocation id [y50oEDAMQLqn_ZWaDOfDcg], primary term [0], message [after new shard recovery], shard id [[test][2]], allocation id [ObfPCzI_TYuveJTZFEbeeA], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 4, uuid: 9GMoP9GGTHu2TReZkBsbag)
[23:15:36,498][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:36,498][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][3] starting shard [test][3], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=QIgGrZWuTGe2M6R3lY28kQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:35.941Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery]])
[23:15:36,498][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:36,498][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][0] starting shard [test][0], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=P6aPb3KSRBaIsuAjqju_ig], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:35.941Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery]])
[23:15:36,499][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [5], source [shard-started[shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:36,499][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [5]
[23:15:36,499][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 5
[23:15:36,500][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:36,500][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:36,500][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:15:36,500][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [test][4] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]]
[23:15:36,500][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [test][4] received shard started for [shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]]
[23:15:36,501][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:36,502][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [after new shard recovery], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [after new shard recovery], shard id [[test][3]], allocation id [QIgGrZWuTGe2M6R3lY28kQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[test][0]], allocation id [P6aPb3KSRBaIsuAjqju_ig], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [3ms] done applying updated cluster_state (version: 5, uuid: D1oDhqE7S6yl_nMl3Z-saQ)
[23:15:36,502][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]]]: execute
[23:15:36,502][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test][4] starting shard [test][4], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=lWbr5LrXTI2yAW2mVBdi_Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:35.941Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]])
[23:15:36,502][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [6], source [shard-started[shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]]]
[23:15:36,502][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [6]
[23:15:36,502][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 6
[23:15:36,503][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:36,504][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[test][4]], allocation id [lWbr5LrXTI2yAW2mVBdi_Q], primary term [0], message [after new shard recovery]]]: took [2ms] done applying updated cluster_state (version: 6, uuid: w3CiOPnSSh2BhjZ4No8jvA)
[23:15:36,505][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[23:15:37,051][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:37,053][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [test/plGH3VpxT96u-_mymAmCiA] create_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:37,053][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [7], source [put-mapping[test]]
[23:15:37,053][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [7]
[23:15:37,053][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 7
[23:15:37,054][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[test/plGH3VpxT96u-_mymAmCiA]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;myfield&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:37,056][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [550ms] done applying updated cluster_state (version: 7, uuid: -jBxSILTTMq4J-VjX_OYMQ)
[23:15:37,084][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete 'authorities' index
[23:15:37,085][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: execute
[23:15:37,085][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[authorities/mJ2qaoNPQny4h3x0178lzQ]], shards [5]/[1] - reason [create index]
[23:15:37,085][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:37,613][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:37,613][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings []
[23:15:37,614][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities] closing ... (reason [cleaning up after validating index on master])
[23:15:37,614][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities/mJ2qaoNPQny4h3x0178lzQ] closing index service (reason [cleaning up after validating index on master])
[23:15:37,614][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:37,614][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:37,614][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:37,615][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities/mJ2qaoNPQny4h3x0178lzQ] closed... (reason [cleaning up after validating index on master])
[23:15:37,615][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [8], source [create-index [authorities], cause [auto(index api)]]
[23:15:37,615][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [8]
[23:15:37,615][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 8
[23:15:37,615][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[authorities/mJ2qaoNPQny4h3x0178lzQ]] creating index
[23:15:37,615][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[authorities/mJ2qaoNPQny4h3x0178lzQ]], shards [5]/[1] - reason [create index]
[23:15:37,615][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:38,138][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:38,138][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][1] creating shard
[23:15:38,139][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/mJ2qaoNPQny4h3x0178lzQ/1, shard=[authorities][1]}]
[23:15:38,139][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [authorities][1]
[23:15:38,139][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:38,140][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:38,141][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:38,141][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][2] creating shard
[23:15:38,141][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] starting recovery from store ...
[23:15:38,141][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/mJ2qaoNPQny4h3x0178lzQ/2, shard=[authorities][2]}]
[23:15:38,141][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [authorities][2]
[23:15:38,142][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:38,142][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:38,142][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#1]] wipe translog location - creating new translog
[23:15:38,144][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:38,144][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][3] creating shard
[23:15:38,144][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] starting recovery from store ...
[23:15:38,144][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/mJ2qaoNPQny4h3x0178lzQ/3, shard=[authorities][3]}]
[23:15:38,144][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [authorities][3]
[23:15:38,144][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:38,145][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:38,145][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:38,146][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#2]] wipe translog location - creating new translog
[23:15:38,146][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:38,146][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][0] creating shard
[23:15:38,147][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] starting recovery from store ...
[23:15:38,147][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/mJ2qaoNPQny4h3x0178lzQ/0, shard=[authorities][0]}]
[23:15:38,147][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:38,147][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [authorities][0]
[23:15:38,148][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:38,148][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:38,148][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:15:38,148][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [authorities][1] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]]
[23:15:38,148][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:38,148][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [authorities][1] received shard started for [shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]]
[23:15:38,148][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#4]] wipe translog location - creating new translog
[23:15:38,149][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:38,149][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] starting recovery from store ...
[23:15:38,149][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:38,150][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:38,150][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [authorities], cause [auto(index api)]]: took [1s] done applying updated cluster_state (version: 8, uuid: lXs96BdWS-uXYlIguSVMeA)
[23:15:38,150][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] recovery completed from [shard_store], took [9ms]
[23:15:38,150][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#3]] wipe translog location - creating new translog
[23:15:38,150][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [authorities][2] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery]]
[23:15:38,150][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]]]: execute
[23:15:38,150][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery]]
[23:15:38,150][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][1] starting shard [authorities][1], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=wIgMwgBcQ2mI93q9EdfJmg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:37.613Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]])
[23:15:38,151][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:38,151][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:38,151][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] recovery completed from [shard_store], took [7ms]
[23:15:38,151][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [authorities][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery]]
[23:15:38,151][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery]]
[23:15:38,152][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [9], source [shard-started[shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]]]
[23:15:38,152][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [9]
[23:15:38,152][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 9
[23:15:38,153][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:38,153][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][4] creating shard
[23:15:38,153][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:15:38,154][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [authorities][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery]]
[23:15:38,154][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery]]
[23:15:38,154][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/mJ2qaoNPQny4h3x0178lzQ/4, shard=[authorities][4]}]
[23:15:38,154][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [authorities][4]
[23:15:38,155][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:38,155][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:38,156][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:38,156][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] starting recovery from store ...
[23:15:38,157][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][2] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][2] received shard started for [shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][3] received shard started for [shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,157][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][0] received shard started for [shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,158][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#1]] wipe translog location - creating new translog
[23:15:38,158][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][1]], allocation id [wIgMwgBcQ2mI93q9EdfJmg], primary term [0], message [after new shard recovery]]]: took [8ms] done applying updated cluster_state (version: 9, uuid: EfladxXdQAuANO8AzbEz6Q)
[23:15:38,159][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:38,159][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][2] starting shard [authorities][2], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=y01PXI4ZSeucaxbjIOmgZg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:37.613Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery]])
[23:15:38,159][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][3] starting shard [authorities][3], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=8oggqw9NTE-ETQV6KdIeZA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:37.613Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery]])
[23:15:38,160][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:38,160][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][0] starting shard [authorities][0], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=7PqFQ82zQeKKe18epDWfDg], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:37.613Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery]])
[23:15:38,161][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [10], source [shard-started[shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:38,161][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [10]
[23:15:38,161][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 10
[23:15:38,163][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:38,163][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] recovery completed from [shard_store], took [9ms]
[23:15:38,163][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery]]
[23:15:38,163][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][4] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,163][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery]]
[23:15:38,163][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][4] received shard started for [shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:38,163][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:38,163][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:38,164][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:38,164][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [after new shard recovery], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [after new shard recovery], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [after new shard recovery], shard id [[authorities][2]], allocation id [y01PXI4ZSeucaxbjIOmgZg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][3]], allocation id [8oggqw9NTE-ETQV6KdIeZA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[authorities][0]], allocation id [7PqFQ82zQeKKe18epDWfDg], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 10, uuid: 7iFxLZ77QuuymwJPlAyyag)
[23:15:38,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:38,165][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities][4] starting shard [authorities][4], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=4ztkM8HYQd-ykO0M8wz4xA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:37.613Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery]])
[23:15:38,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [11], source [shard-started[shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:38,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [11]
[23:15:38,165][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 11
[23:15:38,166][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:38,167][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [after new shard recovery], shard id [[authorities][4]], allocation id [4ztkM8HYQd-ykO0M8wz4xA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 11, uuid: THLmcacdTcSQ8IerDDUQJQ)
[23:15:38,168][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: execute
[23:15:38,674][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:38,678][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [authorities/mJ2qaoNPQny4h3x0178lzQ] create_mapping [persons] with source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:38,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [12], source [put-mapping[persons]]
[23:15:38,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [12]
[23:15:38,679][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 12
[23:15:38,679][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[authorities/mJ2qaoNPQny4h3x0178lzQ]] adding mapping [persons], source [{&quot;persons&quot;:{&quot;properties&quot;:{&quot;author&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:38,682][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[persons]]: took [513ms] done applying updated cluster_state (version: 12, uuid: 9qLI814vSG6gnx03CrjYVw)
[23:15:38,714][WARN ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unable to delete index 'books'
[23:15:38,714][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: execute
[23:15:38,715][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[books/fNA5lUnhRbC7p54_gnvtzA]], shards [5]/[1] - reason [create index]
[23:15:38,715][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:39,260][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:39,262][INFO ][org.elasticsearch.cluster.metadata.MetaDataCreateIndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books] creating index, cause [api], templates [], shards [5]/[1], mappings [test]
[23:15:39,263][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books] closing ... (reason [cleaning up after validating index on master])
[23:15:39,263][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books/fNA5lUnhRbC7p54_gnvtzA] closing index service (reason [cleaning up after validating index on master])
[23:15:39,263][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:39,263][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] full cache clear, reason [close]
[23:15:39,263][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] clearing all bitsets because [close]
[23:15:39,264][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books/fNA5lUnhRbC7p54_gnvtzA] closed... (reason [cleaning up after validating index on master])
[23:15:39,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [13], source [create-index [books], cause [api]]
[23:15:39,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [13]
[23:15:39,264][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 13
[23:15:39,264][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[books/fNA5lUnhRbC7p54_gnvtzA]] creating index
[23:15:39,264][DEBUG][org.elasticsearch.indices.IndicesService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating Index [[books/fNA5lUnhRbC7p54_gnvtzA]], shards [5]/[1] - reason [create index]
[23:15:39,264][DEBUG][org.elasticsearch.index.store.IndexStore][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using index.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [null]
[23:15:39,796][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:39,796][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[books/fNA5lUnhRbC7p54_gnvtzA]] adding mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}}}}}]
[23:15:39,798][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][1] creating shard
[23:15:39,798][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][1] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/fNA5lUnhRbC7p54_gnvtzA/1, shard=[books][1]}]
[23:15:39,798][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [books][1]
[23:15:39,799][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:39,799][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:39,800][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:39,800][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][2] creating shard
[23:15:39,800][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] starting recovery from store ...
[23:15:39,800][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][2] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/fNA5lUnhRbC7p54_gnvtzA/2, shard=[books][2]}]
[23:15:39,801][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [books][2]
[23:15:39,801][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:39,801][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:39,802][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#2]] wipe translog location - creating new translog
[23:15:39,803][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:39,803][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][3] creating shard
[23:15:39,803][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] starting recovery from store ...
[23:15:39,803][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][3] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/fNA5lUnhRbC7p54_gnvtzA/3, shard=[books][3]}]
[23:15:39,803][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [books][3]
[23:15:39,803][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:39,804][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:39,804][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:39,804][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#4]] wipe translog location - creating new translog
[23:15:39,804][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:39,804][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][0] creating shard
[23:15:39,804][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] starting recovery from store ...
[23:15:39,805][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#4]] no translog ID present in the current generation - creating one
[23:15:39,805][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][0] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/fNA5lUnhRbC7p54_gnvtzA/0, shard=[books][0]}]
[23:15:39,805][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [books][0]
[23:15:39,805][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#3]] wipe translog location - creating new translog
[23:15:39,805][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:39,805][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:39,806][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:39,806][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] recovery completed from [shard_store], took [7ms]
[23:15:39,806][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [books][1] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery]]
[23:15:39,806][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [books][1] received shard started for [shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery]]
[23:15:39,806][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#3]] no translog ID present in the current generation - creating one
[23:15:39,806][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:39,806][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] starting recovery from store ...
[23:15:39,807][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:39,807][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#4]] recovery completed from [shard_store], took [6ms]
[23:15:39,807][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [books][2] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]]
[23:15:39,807][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#4]] [books][2] received shard started for [shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]]
[23:15:39,807][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#1]] wipe translog location - creating new translog
[23:15:39,807][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [create-index [books], cause [api]]: took [1s] done applying updated cluster_state (version: 13, uuid: BIEuQZ3cSI-jSrmw6NAIDw)
[23:15:39,808][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]]]: execute
[23:15:39,808][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][1] starting shard [books][1], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=wPphwayfQtWqlwWRjhSCCw], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:39.262Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery]])
[23:15:39,808][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][2] starting shard [books][2], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=hSRRv1-6ROaWsQ14WxDPug], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:39.262Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]])
[23:15:39,808][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#1]] no translog ID present in the current generation - creating one
[23:15:39,809][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:39,809][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#3]] recovery completed from [shard_store], took [6ms]
[23:15:39,809][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [books][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery]]
[23:15:39,810][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#3]] [books][3] received shard started for [shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery]]
[23:15:39,811][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [14], source [shard-started[shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]]]
[23:15:39,811][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [14]
[23:15:39,811][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 14
[23:15:39,812][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:39,812][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#1]] recovery completed from [shard_store], took [7ms]
[23:15:39,812][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [books][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery]]
[23:15:39,812][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery]]
[23:15:39,812][DEBUG][org.elasticsearch.indices.cluster.IndicesClusterStateService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][4] creating shard
[23:15:39,812][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][4] creating using a new path [ShardPath{path=/Users/joerg/Projects/github/jprante/elasticsearch-plugin-bundle/data/nodes/0/indices/fNA5lUnhRbC7p54_gnvtzA/4, shard=[books][4]}]
[23:15:39,812][DEBUG][org.elasticsearch.index.IndexService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] creating shard_id [books][4]
[23:15:39,813][DEBUG][org.elasticsearch.index.store.Store][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] store stats are refreshed with refresh_interval [10s]
[23:15:39,813][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]
[23:15:39,814][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [CREATED]-&gt;[RECOVERING], reason [from store]
[23:15:39,815][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] starting recovery from store ...
[23:15:39,815][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:39,815][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:39,815][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][3] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,815][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][3] received shard started for [shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,815][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][0] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,815][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][0] received shard started for [shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,816][DEBUG][org.elasticsearch.index.translog.Translog][elasticsearch[G5gTcED][generic][T#2]] wipe translog location - creating new translog
[23:15:39,817][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][1]], allocation id [wPphwayfQtWqlwWRjhSCCw], primary term [0], message [after new shard recovery], shard id [[books][2]], allocation id [hSRRv1-6ROaWsQ14WxDPug], primary term [0], message [after new shard recovery]]]: took [9ms] done applying updated cluster_state (version: 14, uuid: e86whw6IQ0OorHKT3dIshA)
[23:15:39,817][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:39,817][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][3] starting shard [books][3], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=XzZoR9YITs6VGNpJ3TP37Q], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:39.262Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery]])
[23:15:39,817][DEBUG][org.elasticsearch.index.engine.Engine][elasticsearch[G5gTcED][generic][T#2]] no translog ID present in the current generation - creating one
[23:15:39,817][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][0] starting shard [books][0], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=0ZtbRz9BR_eMi5SG5fPboQ], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:39.262Z], delayed=false, allocation_status[no_attempt]] (shard started task: [shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery]])
[23:15:39,818][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [15], source [shard-started[shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:39,818][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [15]
[23:15:39,818][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 15
[23:15:39,819][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from shard_store]
[23:15:39,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][generic][T#2]] recovery completed from [shard_store], took [7ms]
[23:15:39,820][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [books][4] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery]]
[23:15:39,820][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][4] sending [internal:cluster/shard/started] to [G5gTcEDXTe6vUhEct-imCA] for shard entry [shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,820][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][generic][T#2]] [books][4] received shard started for [shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery]]
[23:15:39,820][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][4] received shard started for [shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]
[23:15:39,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:39,820][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:39,822][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [after new shard recovery], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [after new shard recovery], shard id [[books][3]], allocation id [XzZoR9YITs6VGNpJ3TP37Q], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started], shard id [[books][0]], allocation id [0ZtbRz9BR_eMi5SG5fPboQ], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [5ms] done applying updated cluster_state (version: 15, uuid: RUwhEsHwRnSavpJgd0Y4cA)
[23:15:39,822][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: execute
[23:15:39,823][DEBUG][org.elasticsearch.cluster.action.shard.ShardStateAction][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books][4] starting shard [books][4], node[G5gTcEDXTe6vUhEct-imCA], [P], recovery_source[new shard recovery], s[INITIALIZING], a[id=TT85TAtTQIS80RVhBiOKfA], unassigned_info[[reason=INDEX_CREATED], at[2017-02-05T22:15:39.262Z], delayed=false, allocation_status[deciders_throttled]] (shard started task: [shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery]])
[23:15:39,823][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [16], source [shard-started[shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]
[23:15:39,823][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [16]
[23:15:39,824][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 16
[23:15:39,824][DEBUG][org.elasticsearch.index.shard.IndexShard][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state is [STARTED]]
[23:15:39,825][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [shard-started[shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [after new shard recovery], shard id [[books][4]], allocation id [TT85TAtTQIS80RVhBiOKfA], primary term [0], message [master {G5gTcED}{G5gTcEDXTe6vUhEct-imCA}{atso3DE7TMuvH7gWow4dJw}{local}{local[14]} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]]]: took [2ms] done applying updated cluster_state (version: 16, uuid: 6PR8lqSDQIiyFJ5YCgVITg)
[23:15:39,828][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[test]]: execute
[23:15:40,377][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] using dynamic[true]
[23:15:40,380][DEBUG][org.elasticsearch.cluster.metadata.MetaDataMappingService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [books/fNA5lUnhRbC7p54_gnvtzA] update_mapping [test] with source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:40,381][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] cluster state updated, version [17], source [put-mapping[test]]
[23:15:40,381][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] publishing cluster state version [17]
[23:15:40,381][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] set local cluster state to version 17
[23:15:40,381][DEBUG][org.elasticsearch.index.mapper.MapperService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] [[books/fNA5lUnhRbC7p54_gnvtzA]] updating mapping [test], source [{&quot;test&quot;:{&quot;properties&quot;:{&quot;authorID&quot;:{&quot;type&quot;:&quot;ref&quot;,&quot;type&quot;:&quot;ref&quot;,&quot;ref_index&quot;:&quot;authorities&quot;,&quot;ref_type&quot;:&quot;persons&quot;,&quot;ref_fields&quot;:[&quot;author&quot;],&quot;copy_to&quot;:[&quot;dc.creator&quot;,&quot;bib.contributor&quot;]},&quot;bib&quot;:{&quot;properties&quot;:{&quot;contributor&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;dc&quot;:{&quot;properties&quot;:{&quot;creator&quot;:{&quot;type&quot;:&quot;text&quot;}}},&quot;title&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;fields&quot;:{&quot;keyword&quot;:{&quot;type&quot;:&quot;keyword&quot;,&quot;ignore_above&quot;:256}}}}}}]
[23:15:40,385][DEBUG][org.elasticsearch.cluster.service.ClusterService][elasticsearch[G5gTcED][clusterService#updateTask][T#1]] processing [put-mapping[test]]: took [556ms] done applying updated cluster_state (version: 17, uuid: URTK9nYMTbqzfEDOcDi_RA)
[23:15:40,414][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] mappings={properties={authorID={type=ref, ref_index=authorities, ref_type=persons, ref_fields=[author], copy_to=[dc.creator, bib.contributor]}, bib={properties={contributor={type=text}}}, dc={properties={creator={type=text}}}, title={type=text, fields={keyword={type=keyword, ignore_above=256}}}}}
[23:15:40,416][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] unref hits = 1
[23:15:40,416][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[23:15:40,419][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] ref hits = 1
[23:15:40,419][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[23:15:40,421][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 unref hits = 1
[23:15:40,421][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[23:15:40,423][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] field 2 ref hits = 1
[23:15:40,423][INFO ][org.xbib.elasticsearch.index.mapper.reference.ReferenceMappingTests][Test worker] {bib={contributor=A contributor}, title=A title, authorID=1, dc={creator=A creator}}
[23:15:40,423][INFO ][test                     ][Test worker] stopping nodes
[23:15:40,424][INFO ][org.elasticsearch.node.Node][Test worker] stopping ...
[23:15:40,424][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities] closing ... (reason [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/mJ2qaoNPQny4h3x0178lzQ] closing index service (reason [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closing... (reason: [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test] closing ... (reason [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,425][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,425][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test/plGH3VpxT96u-_mymAmCiA] closing index service (reason [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:40,425][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books] closing ... (reason [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:40,425][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closing... (reason: [shutdown])
[23:15:40,425][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books/fNA5lUnhRbC7p54_gnvtzA] closing index service (reason [shutdown])
[23:15:40,426][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,426][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closing... (reason: [shutdown])
[23:15:40,426][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:40,426][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,426][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[23:15:40,426][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[23:15:40,427][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,428][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,428][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[23:15:40,428][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:40,428][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:40,429][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:40,429][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:40,429][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [0] closed (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closing... (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:40,430][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [0] closed (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closing... (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [0] closed (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closing... (reason: [shutdown])
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:40,430][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,430][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,430][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[23:15:40,430][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[23:15:40,431][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:40,431][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:40,431][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[23:15:40,431][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:40,432][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:40,432][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:40,432][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [1] closed (reason: [shutdown])
[23:15:40,432][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closing... (reason: [shutdown])
[23:15:40,432][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[23:15:40,433][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,433][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[23:15:40,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,433][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [1] closed (reason: [shutdown])
[23:15:40,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:40,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:40,433][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:40,433][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closing... (reason: [shutdown])
[23:15:40,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:40,433][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [1] closed (reason: [shutdown])
[23:15:40,433][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closing... (reason: [shutdown])
[23:15:40,433][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,433][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:40,433][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,433][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:40,434][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[23:15:40,434][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:40,434][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:40,434][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [2] closed (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closing... (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[23:15:40,435][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,435][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [2] closed (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closing... (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [2] closed (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closing... (reason: [shutdown])
[23:15:40,435][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,435][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[23:15:40,435][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[23:15:40,436][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[23:15:40,436][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[23:15:40,436][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[23:15:40,436][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [3] closed (reason: [shutdown])
[23:15:40,436][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closing... (reason: [shutdown])
[23:15:40,436][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#3]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,436][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,441][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close now acquiring writeLock
[23:15:40,441][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:40,441][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:40,441][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] close acquired writeLock
[23:15:40,442][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:40,442][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#3]] translog closed
[23:15:40,443][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:40,443][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:40,443][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:40,443][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#3]] engine closed [api]
[23:15:40,443][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:40,443][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#3]] store reference count on close: 0
[23:15:40,443][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#3]] [4] closed (reason: [shutdown])
[23:15:40,443][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:40,443][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[23:15:40,443][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [3] closed (reason: [shutdown])
[23:15:40,443][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closing... (reason: [shutdown])
[23:15:40,443][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#2]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,443][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#3]] full cache clear, reason [close]
[23:15:40,443][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,444][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#3]] clearing all bitsets because [close]
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close now acquiring writeLock
[23:15:40,444][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] close acquired writeLock
[23:15:40,444][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [3] closed (reason: [shutdown])
[23:15:40,444][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closing... (reason: [shutdown])
[23:15:40,444][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#2]] translog closed
[23:15:40,444][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#3]] [test/plGH3VpxT96u-_mymAmCiA] closed... (reason [shutdown])
[23:15:40,444][DEBUG][org.elasticsearch.index.shard.IndexShard][indices_shutdown[T#1]] state: [STARTED]-&gt;[CLOSED], reason [shutdown]
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] flushing shard on close - this might take some time to sync files to disk
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close now acquiring writeLock
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] close acquired writeLock
[23:15:40,444][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#2]] engine closed [api]
[23:15:40,444][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#2]] store reference count on close: 0
[23:15:40,444][DEBUG][org.elasticsearch.index.translog.Translog][indices_shutdown[T#1]] translog closed
[23:15:40,444][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#2]] [4] closed (reason: [shutdown])
[23:15:40,444][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:40,445][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#2]] full cache clear, reason [close]
[23:15:40,445][DEBUG][org.elasticsearch.index.engine.Engine][indices_shutdown[T#1]] engine closed [api]
[23:15:40,445][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#2]] clearing all bitsets because [close]
[23:15:40,445][DEBUG][org.elasticsearch.index.store.Store][indices_shutdown[T#1]] store reference count on close: 0
[23:15:40,445][DEBUG][org.elasticsearch.index.IndexService][indices_shutdown[T#1]] [4] closed (reason: [shutdown])
[23:15:40,445][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:40,445][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#2]] [authorities/mJ2qaoNPQny4h3x0178lzQ] closed... (reason [shutdown])
[23:15:40,445][DEBUG][org.elasticsearch.index.cache.query.IndexQueryCache][indices_shutdown[T#1]] full cache clear, reason [close]
[23:15:40,445][DEBUG][org.elasticsearch.index.cache.bitset.BitsetFilterCache][indices_shutdown[T#1]] clearing all bitsets because [close]
[23:15:40,445][DEBUG][org.elasticsearch.indices.IndicesService][indices_shutdown[T#1]] [books/fNA5lUnhRbC7p54_gnvtzA] closed... (reason [shutdown])
[23:15:40,445][INFO ][org.elasticsearch.node.Node][Test worker] stopped
[23:15:40,446][INFO ][org.elasticsearch.node.Node][Test worker] closing ...
[23:15:40,447][INFO ][org.elasticsearch.node.Node][Test worker] closed
[23:15:40,457][INFO ][test                     ][Test worker] data files wiped
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 3.2.1</a> at 05.02.2017 23:15:52</p>
</div>
</div>
</body>
</html>
